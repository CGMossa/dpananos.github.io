---
title: Difference in Difference Estimates Can Be Biased When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments
date: "2023-09-03"
code-fold: true
echo: true
fig-cap-location: top
categories: [AB Testing, Statistics, Causal Inference]
number-sections: false
draft: true
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(ggExtra)

theme_set(theme_minimal(base_size = 16))
```


## Introduction

I've run into people randomizing units to treatment and control and then looking to see if there are pre-treatment differences.  If there are, I've heard -- at Zapier and [cross validated](https://stats.stackexchange.com/questions/625007/ab-testing-control-was-performing-0-5-better-than-experiment-set-before-the-in/625523#625523) -- that a difference in difference (DiD) should be performed.  After all, there are baseline differences!  We need to adjust for those.

To be clear -- using DiD to analyze randomized experiments is fine.  The resulting estimate of the ATE should be unbiased assuming the experiment was run without a hitch. You don't need to do difference in difference because prior to treatment the two groups will have the same distribution of potential outcomes.  Their pre-treatment differences are 0 *in expectation*.  Any detection of a difference -- again, assuming the experiment was run well -- is sampling variability.

Running DiD because we found baseline differences is a form of deciding on analysis based on the observed data, and we all know that is a statistical faux pas.  But how bad could it be?  Are our estimates of the treatment effect biased?  What do we lose when we let the data decide if we should run a DiD or a t-test?

## Simulation

To find out, let's simulate a very simple example.  Let's assume that I run an experiment and measure units before and after.  The observations on each unit are uncorrelated and have standard normal distribution in the absence of the treatment.  If $A$ is a binary indicator for treatment (1 for treatment, 0 else) then the data are

$$ y_{pre} \sim \mbox{Normal}\left(0, \sigma^2\right) \>, $$
$$ y_{post} \sim \mbox{Normal}\left(\tau \cdot A, \sigma^2 \right) \>. $$


I'll run 20, 000 simulations of a simple randomized experiment.  Each time, I'll sample $N$ units in each arm, enough to detect a treatment effect from a t-test with 80% power.  I'll then run a t-test via OLS and a DiD.  I'll record the pre-treatment difference in each group and if it was statistically significant at the 5% level.  For these simulations, I'll set $\tau=1$ and $\sigma=1$ which means I need $N=17$ users per arm.

We'll plot some treatment effect estimates and see what is happening when we choose to do DiD when the data suggest we do. Now importantly, I'm making very strong assumptions about the experiment being run.  In particular, I'm making assumptions that all went well, there is no funny business with timing or randomization, etc.  In terms of a medical trial, I got 34 people to all stand in a line, randomly gave each placebo or drug, watched them all take it at the same time, and recorded outcomes.  The purpose of these simulation and blog post is to investigate statistical properties and not to wring about whatabouts.

In the code cell below is the code to run these simulations

```{r}
simulate_data <- function(N_per_arm=17, tau=1, sigma=1){
  
  
  A <- rbinom(2*N_per_arm, 1, 0.5)
  y_pre <- rnorm(2*N_per_arm, 0, sigma)
  y_post <- rnorm(2*N_per_arm, tau*A, sigma)

  
  pre <- tibble(y=y_pre, trt=A, period=0)
  post <- tibble(y=y_post, trt=A, period=1)
  
  bind_rows(pre, post)
      
}

do_analysis <- function(i){
  d <- simulate_data()
  
  #DiD 
  did <- lm(y ~ trt*period, data=d)
  # t-test, only on post data
  tt <- lm(y ~ trt, data=filter(d, period==1))
  
  tt_ate <- coef(tt)['trt']
  did_ate <- coef(did)['trt:period']
  
  pre_test <- t.test(y~trt, data = filter(d, period==0))
  
  pre_period_diff <- diff(pre_test$estimate)
  detected <- if_else(pre_test$p.value<0.05, 'Pre-Period Difference', 'No Pre-Period Difference')
  
  tibble(
    tt_ate, 
    did_ate, 
    pre_period_diff, 
    detected
  )
}


results <- map_dfr(1:20000, do_analysis, .id = 'sim')
```


## Results



Shown below are the ATEs from each analysis.  Nothing too surprising here, the ATEs are unbiased (the histograms are centered at $\tau=1$).  There might be some differences in variance, but I don't care about that right now.



```{r}
plot <- results %>% 
        ggplot(aes(tt_ate, did_ate)) + 
        geom_point(alpha = 0.5) + 
        labs(
          x= 'T-test ATE',
          y='DiD ATE'
        )

ggMarginal(plot, type='histogram')
```


Let's now plot the ATEs for each method against the pre-period differences.  Because all observations are assumed independent, I'm going to expect that the ATEs for the t-test are uncorrelated with the pre-period difference.  However, because the DiD uses pre-period information, I'm going to expect a correlation (I just don't know how big). 


```{r}
plot <- results %>% 
  pivot_longer(tt_ate:did_ate, names_to = 'analysis', values_to = 'ate') %>% 
  mutate(
    analysis = if_else(analysis=='tt_ate', 'T-test', 'DiD')
  ) %>% 
  ggplot(aes(pre_period_diff, ate)) + 
  geom_point(alpha=0.5) + 
  facet_grid( ~ analysis) + 
  labs(x='Pre period difference',
       y = 'ATE')


plot 
```

Great, this makes sense.  The ATE is for the t-test is uncorrelated with the pre-period difference, as expected.  The ATE DiD is correlated with the pre-period difference, and that's likely due to regression to the mean.  Now, let's stratify by cases when the pre-period difference is (erroneously) thought to be non-zero.

```{r}
plot + facet_grid(detected ~ analysis)
```

It is unsurprising that the tails of each of these blogs is cut off.  After all, the pre-period difference needs to be extreme enough to reject the null.  Let's first talk about that bottom right cell -- the t test when there is a detected pre-period difference.  Because there is no correlation between pre-period difference and the ATE, the ATEs are still unbiased.  That's great.  What about DiD?

Note that the correlation means that those blobs don't have the same mean.  In fact, if you run K-means on those blobs, you can very easily seperate them and estimate the ATE and its very far from 1!  That's bias!  How big those biases (plural, because it depends on the size of the pre-treatment difference) depends on the strength of the correlation between the ATE anf the pre-period difference.  In this particular example, the left most cluster has an average ATE of about 1.8 while the right most cluster has an average ATE of about 0.24.  That's a big amount of bias in either direction.

```{r, include=FALSE}
blob_tails <- results %>% 
              pivot_longer(tt_ate:did_ate, names_to = 'analysis', values_to = 'ate') %>% 
              mutate(
                analysis = if_else(analysis=='tt_ate', 'T-test', 'DiD')
              ) %>% 
              filter((detected=='Pre-Period Difference')&(analysis=='DiD')) %>% 
              select(pre_period_diff, ate)

X <- as.matrix(blob_tails)
means <- kmeans(scale(X),centers = 2)

blob_tails$cluster <- means$cluster

blob_tails %>% 
  group_by(cluster) %>% 
  summarise(ate = mean(ate))
```

## Conclusion

Both DiD and the T-test are ways to obtain unbiased estimates of the ATE for randomized experiments.  DiD isn't needed since the distribution of potential outcomes in the pre-period is the same, so prior to intervention the null hypothesis would be true.  Additionally, the ATE in DiD is correlated with the pre-period difference in means.  This is likely due to regression to the mean.  This correlation means that when you test for pre-period differences and then choose to do DiD based on the results of that analysis, you will likely end up with a biased estimate of the ATE when the analysis tells you there is a pre-period difference.

The way to avoid these biases is to choose a method of analysis -- DiD or t-test, like I said it doesn't matter -- prior to seeing the data.  Checks for data quality are welcomed, but do not use statistics to tell you what statistics to compute.  I've clearly shown that can go awry.