{
  "hash": "2bb4652c8826565bd505e708e09c9961",
  "result": {
    "markdown": "---\ntitle: Evaluating Bayesian 'Power'\ndate: \"2023-07-12\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: false\n---\n\nAt Zapier, we use Eppo for our AB testing.  I could write a whole blog post on why I love the product so much, but suffice to say it does sensible things with plenty of sensible options and is run by more than sensible people.  \n\nEppo has proved to be very stakeholder friendly as well, and has caught on with one team in particular who is running some high visibility experiments.  Recently, this team wanted to run an AB test on a lower traffic page.  Due to the low traffic, there was a concern that running an AB test for a single week would not yield sufficient power.  The proposed solution was to \"run a Bayesian test\" to \"speed up the experiment\".\n\nThis post is intended to challenge that thinging a bit.  Admittedly, I *also* thought that if we were to use a Bayesian model then we could...learn faster, I'm not sure.   I spent most of my Ph.D lauding the fact we could condition on what little information we had thanks to strong priors without actually thoroughly evaluating what happened when we did.  I actually built a bespoke Bayesian model, with strong priors informed by previous data and if anything it made it *harder* to detect an effect.\n\nSo in this post, we're going to break down that thinking in the context of a conversion rate optimization AB test.  \n\n## What Could It Mean To \"Run The Experiment Faster\"\n\n##  Mathematical Details\n\nIn randomized experiments of this flavor, our main estimand of interest [^1] is\n\n[^1]: For better or worse\n\n$$ \\mbox{lift} = \\dfrac{E[Y(1)] - E[Y(0)]}{E[Y(0)]} = \\dfrac{E[Y(1)]}{E[Y(0)]} - 1 $$\n\nFor now, note that this looks a lot like relative risk.  In fact, it looks so much like relative risk that I'm going to model that instead because I won't have to use the Delta method too much\n\n$$ RR =\\dfrac{E[Y(1)]}{E[Y(0)]} \\>. $$\n\nIn any case, we'll randomize $n_t$ users to treatment, and $n_c$ users to control.  From there, $y_t$ from treatment will \"convert\" (i.e. get some outcome, this is where the C in CRO comes from) and $y_c$ will convert from control.  We'll model these data as\n\n$$ y_t \\sim \\mbox{Binomail}(\\theta_t) $$\n$$ y_c \\sim \\mbox{Binomail}(\\theta_c) $$\n\nand so $RR = \\theta_t / \\theta_c$.  With this setup, we can now analyze the data using a frequentist method and a Bayesian method.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\n\nclass NormalModel:\n  \n  def __init__(self, mu_prior = 0, sd_prior = 0.1):\n    \n    self.mu_prior = mu_prior\n    self.sd_prior = sd_prior\n    self.var_prior = sd_prior**2\n    \n    self.mu_posterior = None\n    self.sd_posterior = None\n    self.var_posterior = None\n    \n  def update(self, log_rr, var_log_rr):\n    \n    wts = 1.0 / np.array([self.var_prior, var_log_rr])\n    means = np.array([self.mu_prior, log_rr])\n    \n    \n    self.mu_posterior = np.average(a=means, weights=wts)\n    self.var_posterior = 1/(wts.sum())\n    self.sd_posterior = np.sqrt(self.var_posterior)\n    \n    \nrr = 1.1\nn = 10000\nx = np.random.binomial(n = n, p = 0.1)\ny = np.random.binomial(n = n, p = 0.1*rr)\n\nlog_rr = np.log(y) - np.log(x)\nvar_log_rr = 1/x - 1/n + 1/y -1/n\n\n\nmodel = NormalModel()    \nmodel.update(log_rr=log_rr, var_log_rr=var_log_rr)\n\nmodel.mu_posterior\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.06490260338894872\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}