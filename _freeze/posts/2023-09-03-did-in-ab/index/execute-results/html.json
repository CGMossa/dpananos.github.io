{
  "hash": "a601052aa4e6397571c9b985b4d30fef",
  "result": {
    "markdown": "---\ntitle: Difference in Difference Estimates are Biased When Randomizing And Testing For Pre-Treatment Differences\ndate: \"2023-09-03\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: [AB Testing, Statistics, Causal Inference]\nnumber-sections: false\ndraft: true\n---\n\n\n\n\n\n## Introduction\n\nI've run into people randomizing units to treatment and control and then looking to see if there are pre-treatment differences.  If there are, I've heard -- at Zapier and [cross validated](https://stats.stackexchange.com/questions/625007/ab-testing-control-was-performing-0-5-better-than-experiment-set-before-the-in/625523#625523) -- that a difference in difference (DiD) should be performed.  After all, there are baseline differences!  We need to adjust for those.\n\nTo be clear -- using DiD to analyze randomized experiments is fine.  The resulting estimate of the ATE should be unbiased assuming the experiment was run without a hitch. You don't need to do difference in difference because prior to treatment the two groups will have the same distribution of potential outcomes.  Their pre-treatment differences are 0 *in expectation*.  Any detection of a difference -- again, assuming the experiment was run well -- is sampling variability.\n\nRunning DiD because we found baseline differences is a form of deciding on analysis based on the observed data, and we all know that is a statistical faux pas.  But how bad could it be?  Are our estimates of the treatment effect biased?  What do we lose when we let the data decide if we should run a DiD or a t-test?\n\n## Simulation\n\nTo find out, let's simulate a very simple example.  Let's assume that I run an experiment and measure units before and after.  The observations on each unit are uncorrelated and have standard normal distribution in the absence of the treatment.  If $A$ is a binary indicator for treatment (1 for treatment, 0 else) then the data are\n\n$$ y_{pre} \\sim \\mbox{Normal}\\left(0, \\sigma^2\\right) \\>, $$\n$$ y_{post} \\sim \\mbox{Normal}\\left(\\tau \\cdot A, \\sigma^2 \\right) \\>. $$\n\n\nI'll run 20, 000 simulations of a simple randomized experiment.  Each time, I'll sample $N$ units in each arm, enough to detect a treatment effect from a t-test with 80% power.  I'll then run a t-test via OLS and a DiD.  I'll record the pre-treatment difference in each group and if it was statistically significant at the 5% level.  For these simulations, I'll set $\\tau=1$ and $\\sigma=1$ which means I need $N=17$ users per arm.\n\nWe'll plot some treatment effect estimates and see what is happening when we choose to do DiD when the data suggest we do.\n\nIn the code cell below is the code to run these simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_data <- function(N_per_arm=17, tau=1, sigma=1){\n  \n  \n  A <- rbinom(2*N_per_arm, 1, 0.5)\n  y_pre <- rnorm(2*N_per_arm, 0, sigma)\n  y_post <- rnorm(2*N_per_arm, tau*A, sigma)\n\n  \n  pre <- tibble(y=y_pre, trt=A, period=0)\n  post <- tibble(y=y_post, trt=A, period=1)\n  \n  bind_rows(pre, post)\n      \n}\n\ndo_analysis <- function(i){\n  d <- simulate_data()\n  \n  #DiD \n  did <- lm(y ~ trt*period, data=d)\n  # t-test, only on post data\n  tt <- lm(y ~ trt, data=filter(d, period==1))\n  \n  tt_ate <- coef(tt)['trt']\n  did_ate <- coef(did)['trt:period']\n  pre_test <- t.test(y~trt, data = filter(d, period==0))\n  pre_period_diff <- diff(pre_test$estimate)\n  detected <- if_else(pre_test$p.value<0.05, 'Pre-Period Difference', 'No Pre-Period Difference')\n  \n  tibble(\n    tt_ate, \n    did_ate, \n    pre_period_diff, \n    detected\n  )\n}\n\n\nresults <- map_dfr(1:20000, do_analysis, .id = 'sim')\n```\n:::\n\n\n\n## Results\n\n\n\nShown below are the ATEs from each analysis.  Nothing too surprising here, the ATEs are unbiased (the histograms are centered at $\\tau=1$).  There might be some differences in variance, but I don't care about that right now.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot <- results %>% \n        ggplot(aes(tt_ate, did_ate)) + \n        geom_point(alpha = 0.5) + \n        labs(\n          x= 'T-test ATE',\n          y='DiD ATE'\n        )\n\nggMarginal(plot, type='histogram')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nLet's now plot the ATEs for each method against the pre-period differences.  Because all observations are assumed independent, I'm going to expect that the ATEs for the t-test are uncorrelated with the pre-period difference.  However, because the DiD uses pre-period information, I'm going to expect a correlation (I just don't know how big). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot <- results %>% \n  pivot_longer(tt_ate:did_ate, names_to = 'analysis', values_to = 'ate') %>% \n  mutate(\n    analysis = if_else(analysis=='tt_ate', 'T-test', 'DiD')\n  ) %>% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(alpha=0.5) + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE')\n\n\nplot \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nGreat, this makes sense.  The ATE is for the t-test is uncorrelated with the pre-period difference, as expected.  The ATE DiD is correlated with the pre-period difference, and that's likely due to regression to the mean.  Now, let's stratify by cases when the pre-period difference is (erroneously) thought to be non-zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot + facet_grid(detected ~ analysis)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt is unsurprising that the tails of each of these blogs is cut off.  After all, the pre-period difference needs to be extreme enough to reject the null.  Let's first talk about that bottom right cell -- the t test when there is a detected pre-period difference.  Because there is no correlation between pre-period difference and the ATE, the ATEs are still unbiased.  That's great.  What about DiD?\n\nNote that the correlation means that those blobs don't have the same mean.  In fact, if you run K-means on those blobs, you can very easily seperate them and estimate the ATE and its very far from 1!  That's bias!\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}