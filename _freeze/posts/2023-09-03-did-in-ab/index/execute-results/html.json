{
  "hash": "68bca2534d5669c7fcd7678d7f6ed8e2",
  "result": {
    "markdown": "---\ntitle: Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments\ndate: \"2023-09-03\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: [AB Testing, Statistics, Causal Inference]\nnumber-sections: false\ndraft: false\n---\n\n\n\n\nEdits:\n\n* @devonbancroft1 mentions that in AB testing, users are highly correlated between periods. It would be useful to examine how that correlation changes outcomes.  I've edited the simulation to allow for drawing correlated pre and post data.\n\n* @fiftycente1 mentions that bias is about expectation.  That's true, I've edited my post to explicitly distinguish between bias and accuracy.\n\n\n## Introduction\n\nI've run into people randomizing units to treatment and control and then looking to see if there are pre-treatment differences with a hypothesis test.  If there are, I've heard -- at Zapier and [cross validated](https://stats.stackexchange.com/questions/625007/ab-testing-control-was-performing-0-5-better-than-experiment-set-before-the-in/625523#625523) -- that a difference in difference (DiD) should be performed.  After all, there are baseline differences!  We need to adjust for those.\n\nTo be clear -- using DiD to analyze randomized experiments is fine.  The resulting estimate of the ATE should be unbiased assuming the experiment was run without a hitch. You don't need to do difference in difference because prior to treatment the two groups will have the same distribution of potential outcomes.  Their pre-treatment differences are 0 *in expectation*.  Any detection of a difference -- again, assuming the experiment was run well -- is sampling variability.\n\nRunning DiD because we found baseline differences is a form of deciding on analysis based on the observed data, and we all know that is a statistical faux pas.  But how bad could it be?  Are our estimates of the treatment effect biased?  Are they precise?  What do we lose when we let the data decide if we should run a DiD or a t-test?\n\n## Simulation\n\nTo find out, let's simulate a very simple example.  Let's assume that I run an experiment and measure units before and after.  The observations on each unit are iid and have standard normal distribution in the absence of the treatment.  If $A$ is a binary indicator for treatment (1 for treatment, 0 else) then the data are\n\n$$ y_{pre} \\sim \\mbox{Normal}\\left(0, \\sigma^2\\right) \\>, $$\n$$ y_{post} \\sim \\mbox{Normal}\\left(\\tau \\cdot A, \\sigma^2 \\right) \\>. $$\n\n\nWe will allow for the pre and post observations to be correlated with correlation $\\rho$.\n\nI'll run 5, 000 simulations of a simple randomized experiment.  Each time, I'll sample $N$ units in each arm, enough to detect a treatment effect from a t-test with 80% power.  I'll then run a t-test via OLS and a DiD.  I'll record the pre-treatment difference in each group and if it was statistically significant at the 5% level.  I'm also going to carry around the estimates from an ANCOVA since this seems to be the recommended approach for when we have pre-treatment information that is correlated with the outcome.  For these simulations, I'll set $\\tau=1$ and $\\sigma=1$ which means I need $N=17$ users per arm to achieve 80% power.\n\nWe'll plot some treatment effect estimates and see what is happening when we choose to do DiD when the data suggest we do. Now importantly, I'm making very strong assumptions about the experiment being run.  In particular, I'm making assumptions that all went well, there is no funny business with timing or randomization, etc.  In terms of a medical trial, I got 34 people to all stand in a line, randomly gave each placebo or drug, watched them all take it at the same time, and recorded outcomes.  The purpose of these simulation and blog post is to investigate statistical properties and not to wring about whatabouts.\n\nIn the code cell below is the code to run these simulations\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulate_data <- function(N_per_arm=17, tau=1, sigma=1, rho=0, ...){\n  \n  \n  Sigma <- diag(rep(sigma, 2)) %*% matrix(c(1, rho, rho, 1), nrow=2) %*% diag(rep(sigma, 2))\n  \n  A <- rep(0:1, N_per_arm)\n  Y <- cbind(0, tau*A) + MASS::mvrnorm(2*N_per_arm, mu = c(0, 0), Sigma = Sigma)\n  \n  y_pre <- Y[, 1]\n  y_post <- Y[, 2]\n  \n  pre <- tibble(i = seq_along(A), y=y_pre, trt=A, period=0)\n  post <- tibble(i = seq_along(A), y=y_post, trt=A, period=1)\n  \n  bind_rows(pre, post)\n      \n}\n\ndo_analysis <- function(...){\n  \n  d <- simulate_data(...)\n  \n  dwide <- d %>% \n           mutate(period = if_else(period==1, 'post','pre'))  %>% \n          pivot_wider(id_cols = c(i, trt), names_from = period, values_from = y)\n  \n  #DiD \n  did <- lm(y ~ trt*period, data=d)\n  # t-test, only on post data\n  tt <- lm(y ~ trt, data=filter(d, period==1))\n  ## Ancova\n  ancova <- lm(post ~ pre + trt, data=dwide)\n  \n  tt_ate <- coef(tt)['trt']\n  did_ate <- coef(did)['trt:period']\n  ancova_ate <- coef(ancova)['trt']\n  \n  pre_test <- t.test(y~trt, data = filter(d, period==0))\n  \n  pre_period_diff <- diff(pre_test$estimate)\n  detected <- if_else(pre_test$p.value<0.05, 'Pre-Period Difference', 'No Pre-Period Difference')\n  \n  tibble(\n    tt_ate, \n    did_ate, \n    ancova_ate,\n    pre_period_diff, \n    detected\n  )\n}\n\n\nresults <- map_dfr(1:5000, ~do_analysis(), .id = 'sim')\n```\n:::\n\n\n\n## Case 1:  Uncorrelated Pre and Post Observations\n\n\n\nShown below are the ATEs from each analysis when the pre and post are uncorrelated.  Nothing too surprising here, the ATEs are unbiased (the histograms are centered at $\\tau=1$).  It seems that the t-test has lower sampling variance/higher precision, which means lower MSE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot <- results %>% \n        ggplot(aes(tt_ate, did_ate)) + \n        geom_point(alpha = 0.5) + \n        labs(\n          x= 'T-test ATE',\n          y='DiD ATE'\n        ) + \n  theme(aspect.ratio = 1)\n\nggMarginal(plot, type='histogram')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nLet's now plot the ATEs for each method against the pre-period differences.  Because all observations are assumed independent, I'm going to expect that the ATEs for the t-test are uncorrelated with the pre-period difference.  However, because the DiD uses pre-period information, I'm going to expect a correlation (I just don't know how big). \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot <- results %>% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %>% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %>% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(alpha=0.5) + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE') +\n  theme(aspect.ratio = 1)\n\n\nplot \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nGreat, this makes sense.  The ATE is for the t-test is uncorrelated with the pre-period difference, as expected.  The ATE DiD is correlated with the pre-period difference, and that's likely due to regression to the mean.  Now, let's stratify by cases when the pre-period difference is (erroneously) thought to be non-zero.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot + facet_grid(detected ~ analysis)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nIt isn't unsurprising that the tails of each of these blobs is cut off.  After all, the pre-period difference needs to be extreme enough to reject the null.  Let's first talk about that bottom right cell -- the t test when there is a detected pre-period difference.  Because there is no correlation between pre-period difference and the ATE, the ATEs are still unbiased.  That's great.  What about DiD (bottom middle cell)?\n\nNote that the correlation means that those blobs don't have the same mean.  In fact, if you run K-means on those blobs, you can very easily separate them and estimate the ATE and its very far from 1! Technically, this is unbiased because we have to average over each blob.  So it isn't that the estimates are biased from DiD, but they are *inaccurate*.  Take a look at the MSE for each estimate (shown below).  Using DiD when you detect a pre-treatment difference may be unbiased, but it has very high MSE as compared to the two other methods.\n\nWhile those estimates are unbiased, the low precision/high MSE is a good argument against checking for pre-treatment differences and then deciding on analysis style. This would be like saying \"the river is a foot deep\" when in reality the river is an inch deep in most places, and a mile deep in others.  While the estimates are unbiased, any one particular estimate is fairly far from the true ATE.\n\n\n\n\n\n## Case 2: Moderate Correlation Between Pre and Post\n\nLet's now let pre and post correlation be non-zero, but still moderate in size.  We'll let $\\rho = 0.45$ which is still much smaller than most correlation coefficients I've seen in real AB tests at Zapier.\n\n\n\nMaking the same plot as we did before, we see that DiD still suffers from poor accuracy as measured by MSE, bow now the T-test is starting to demonstrate some positive correlation between pre-period difference and ATE.  Its worth mentioning again that we would not really do anything different with the information that there was a pre-period difference when using the t-test.  The estimate of the ATE would still be unbiased and have low MSE.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- map_dfr(1:5000, ~do_analysis(N_per_arm=17, tau=1, sigma=1, rho=0.45), .id = 'sim')\n\nresults %>% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %>% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %>% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(alpha=0.5, color = 'gray 45') + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE') +\n  theme(aspect.ratio = 1) + \n  facet_grid(detected ~ analysis)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Case 3: Strong Correlation Between Pre and Post\n\nIn AB testing, it isn't uncommon to see $\\rho>0.9$ between pre and post outcomes. When we run experiments and seek to have an effect on existing user's monthly payments, many users won't change at all meaning they are typically paying the same amount this month as they were last.  That results in very high $\\rho$.\n\nLet's rerun the simulation, but this time specify that pre and post have a correlation of 0.95.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults <- map_dfr(1:5000, ~do_analysis(N_per_arm=17, tau=1, sigma=1, rho=0.95), .id = 'sim')\n\n\nmyblue <- rgb(48/255, 61/255, 78/255, 0.5)\n\nplot <- results %>% \n        ggplot(aes(tt_ate, did_ate)) + \n        geom_point( color=myblue) + \n        labs(\n          x= 'T-test ATE',\n          y='DiD ATE'\n        ) + \n  theme(aspect.ratio = 1)\n\nggMarginal(plot, type='histogram')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nI'm fairly surprised to see that the results almost completely switch.  First, the DiD estimator becomes much more efficient (see the joint distribution of ATEs) which is cool.  Second, now the estimates from DiD become unbiased *and* accurate which is a nice change.  Ancova seems to do just as well as DiD in terms of precision and accuracy, which was not expected simply because I didn't think DiD would work this well under these assumptions.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresults %>% \n  pivot_longer(tt_ate:ancova_ate, names_to = 'analysis', values_to = 'ate') %>% \n  mutate(\n    analysis = case_when(\n      analysis == 'tt_ate' ~ 'T-test',\n      analysis == 'did_ate' ~ 'DiD',\n      analysis == 'ancova_ate' ~ 'Ancova'\n    )\n  ) %>% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(color=myblue) + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE') +\n  theme(aspect.ratio = 1) + \n  facet_grid(detected ~ analysis)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n## Conclusion\n\nRandomization creates two artificial groups who have the same distribution of potential outcomes.  There is no need to correct for baseline differences in randomized groups a la DiD, and there is certainly no need to check for baseline differences after randomizing groups.\n\nHowever, deciding to adjust for baseline differences with DiD can result in unbiased but inaccurate estimates of the treatment effect under some circumstances.  From this post, those conditions seemed to be when pre and post treatment outcomes were uncorrelated. That can happen when the outcomes are very noisy or when outcomes are homogeneous across units.  While unbiased, any one particular estimate from this procedure was very far away from the true ATE, as evidenced by the larger MSE.\n\nHowever, when outcomes are correlated between pre and post (like they can be in AB tests), then the story flips.  DiD becomes very efficient, nearly as efficient as Ancova, with good accuracy and unbiased estimates.\n\nGenerally it seems then that using DiD for a randomized experiment is fine. In cases where the outcomes are highly correlated, you could even check for baseline differences with out much harm.  But it isn't needed, and there are circumstances where deciding to use DiD because of detected baseline differences can hurt your estimates.\n\nIf you want unbiasedness and low variance/high precision, the results from this post (and advice elsewhere) seems to be to stick with Ancova.  ",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}