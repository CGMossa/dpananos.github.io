{
  "hash": "7e628d2fc5defddec3231c664cb3f0b3",
  "result": {
    "markdown": "---\ntitle: Interim Analysis & Group Sequential Designs Pt 2\ndate: \"2022-07-30\"\ncode-fold: true\nfig-cap-location: top\ncategories: [Statistics, AB Testing]\nnumber-sections: false\nimage: spending.png\n---\n\n\nThis is part 2 of an ongoing series on group sequential designs for AB testing.  Previous parts are shown below\n\n> [Part 1](https://dpananos.github.io/posts/2022-07-06-gsd/)\n\nLast time, we noted that we want our AB tests to be shorter so we could learn quicker.  Peeking -- testing the data before the end of the experiment -- inflates the probability we make a false positive unless we choose the critical values of the tests a little more carefully.  The reason this happens is because requiring that any of the cumulative test statistics be larger than 1.96 in magnitude defines a region in the space of cumulative means which has a probability density larger than 5\\%.  One way to fix that is just to redefine the space to be smaller by requiring the cumulative test statistics to be larger in magnitude than some other value.  I noted this puts the unnecessary requirement on us that the thresholds all be the same.  In this blog post, we will discuss other approaches to that problem and their pros and cons.\n\nIn order to have that discussion, we need to understand what \"alpha\" ($\\alpha$) is and how it can be \"spent\".  That will allow us to talk about \"alpha spending functions\".\n\n## Preliminaries\n\nIn the last post, we looked at a $K=2$ GSD with equal sized groups.  Of course, we don't need to have equal sized groups and we can have more than two stages.  Let $n_k$ be the sample size of the $k^{th}$ group. Then $N = \\sum_k n_k$ is the total sample size.  It is sometimes more convenient to refer to the *information rates* $t_k = n_k/N$.  We will do the same for consistency with other sources on GSDs.\n\n## What is $\\alpha$, and How Do We Spend It?\n\nThe probability we reject the null, $H_0$, when it is true is called $\\alpha$.  In a classical test, we would reject $H_0$ when $\\vert Z \\vert > z_{1-\\alpha/2}$, and so $P(\\vert Z \\vert > z_{1-\\alpha/2} \\vert H_0) = \\alpha$.  Now consider a $K=4$ GSD so we can work with a concrete example.  Let $Z^{(k)}$ be the test statistic after seeing the $k^{th}$ group, and let $u_k$ be the threshold so that if $\\vert Z^{(k)} \\vert > u_k$ then we would reject the null.  Then a type one error could happen when [^1] ...\n\n[^1]: Or if you like set theory, we could write each line as $\\left( \\bigcap_{k=1}^{j-1} \\vert{Z^{(k)}} \\vert \\lt u_k \\right) \\cap \\left( \\vert{Z^{(j)}} \\vert \\geq u_j \\right)$.\n\n\n$$\n\\begin{align}\n&\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\right)  \\quad \\mbox{or} \\\\\n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\right) \\quad \\mbox{or} \\\\ \n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\lt \\vert Z^{(3)} \\vert \\right) \\quad \\mbox{or} \\\\ \n& \\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\geq \\vert Z^{(3)} \\vert \\mbox{ and } u_4 \\lt \\vert Z^{(4)} \\vert \\right)\n\\end{align}\n$$ {#eq-rejections}\n\n\nSo a type one error can occur in multiple ways, but we still want the probability we make a type one error to be $\\alpha$, which means we're going to need to evaluate the probability of the expression above.  Note that each line in (@eq-rejections) are mutually exclusive, so the probability of the expression above is just the sum of the probabilities of each expression.  This gives us \n\n\n$$\n\\begin{align}\n\\alpha = &P\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\Big\\vert H_0\\right)  +  \\\\\n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\Big\\vert H_0\\right) + \\\\ \n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\lt \\vert Z^{(3)} \\vert \\Big\\vert H_0\\right) + \\\\ \n& P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\geq \\vert Z^{(2)} \\vert \\mbox{ and } u_3 \\geq \\vert Z^{(3)} \\vert \\mbox{ and } u_4 \\lt \\vert Z^{(4)} \\vert \\Big\\vert H_0\\right)\n\\end{align}\n$$\n\n\nThe test at each stage contributes towards the probability we make a type one error $\\alpha$.  You can think of $\\alpha$ as a \"budget\", and at each stage we have to \"spend\" (see where I'm going?) some of that alpha, with the added condition that we can never buy it back (meaning our $\\alpha$ spending must be increasing).  How much we decide to spend dictates what the $u_k$ are going to be.\n\nBut how do we decide how to spend our $\\alpha$?  If $\\alpha$ is our budget for type one error, we need some sort of spending plan.  Or perhaps a spending...function.\n\n\n## $\\alpha$-Spending Functions\n\nAn $\\alpha$-spending function $\\alpha^\\star(t_k)$ can be any non-decreasing function of the information rate $t_k$ such that $\\alpha^\\star(0)=0$ and $\\alpha^\\star(1) = \\alpha$.  Using this approach, we don't need to specify the number of looks (though we may plan for $K$ of them), nor the number of observations at those looks.  Only the maximum sample size needed, $N$.\n\nEach time we make an analysis, we spend some of our budgeted $\\alpha$.  In our first analysis (at $t_1 = n_1/N$), we spend\n\n\n$$ P\\left( u_1 \\lt \\vert Z^{(1)} \\vert \\Big\\vert H_0\\right) = \\alpha^\\star(t_1) \\>. $$\n\nAt the second analysis, we spend\n\n\n$$P\\left(u_1 \\geq \\vert Z^{(1)} \\vert \\mbox{ and } u_2 \\lt \\vert Z^{(2)} \\vert \\Big\\vert H_0\\right) = \\alpha^\\star(t_2) - \\alpha^\\star(t_1) \\>,$$\n\nand so on, with the $k^{th}$ analysis being the difference in the alpha spending functions at the successive information rates.  The spend is defined in this way so that the sum of the spend totals $\\alpha$ since $\\alpha = \\sum_{k=2}^K \\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})$.  The quantities $\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})$ determine what the $u_k$ should be through something called the *recursive integration formula*, which I will not be covering because wow is it every mathy and I need some time.\n\nTwo popular $\\alpha$-spending functions are the Pockock Spending function and the O'Brien Flemming spending function, shown in @fig-spending-function.  The equations don't matter, what matters is the qualitative behavior.\n\n::: {.cell}\n::: {.cell-output-display}\n![Pocock and O'Brien $\\alpha$-spending functions](index_files/figure-html/fig-spending-function-1.png){#fig-spending-function width=672}\n:::\n:::\n\n\nIn gray is the line $y = 0.05x$, which would correspond to an alpha spending function in which our spend is proportional to the difference in information rates.  Note how the Pocock function is kind of close to the diagonal (but not exactly on it), while the O'Brien Flemmming is constant up until $t_k \\approx 0.3$ and then starts to increase. The result of this qualitative behavioiur is evident when we plot our rejection regions (the $u_k$, which remember depend on the spending function).\n\n## Plotting the Rejection Regions\n\nIn my [last post](https://dpananos.github.io/posts/2022-07-06-gsd/), the rejection region was plotted on the joint distribution of the $Z^{(1)}$ and $Z^{(2)}$.  That is easy for two dimensions, doable for 3, and impossible for us to comprehend beyond that.  Luckily, there is a simpler way of visualizing these rejection regions.  We can simply plot the rejection regions $u_k$ as a function of the information rate $t_k$.  Let's plot the rejection regions for the Pocock and O'Brien Flemming spending functions now.  But, I'm not going to label them just yet.  I want you to think about which one might be which and why (knowing what we know about spending functions and the qualitative behaviour we saw above).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nextract_lims <- function(sfu){\n  \n  gs = gsDesign(\n  k=11, \n  timing = seq(0.1, 1, length.out = 11),\n  test.type=2,\n  alpha=0.025, \n  beta = 0.2,\n  sfu=sfu\n)\n  \ntibble(tk = gs$timing,\n       lower = gs$lower$bound,\n       upper = gs$upper$bound,\n       spending = if_else(sfu=='OF', \"O'Brien Flemming\", \"Pocock\")\n       )\n\n}\n\n\nsfus<- c('Pocock', 'OF')\n\nlims <- map_dfr(sfus, extract_lims) %>% \n      pivot_longer(cols = lower:upper, names_to = 'which', values_to = 'uk' )\n\nlims %>% \n  ggplot(aes(tk, uk, linetype = interaction(which, spending))) + \n  geom_line(color=my_blue, size=1) + \n  scale_linetype_manual(values = c(1, 1, 2, 2)) +\n  guides(linetype=F) + \n  theme(aspect.ratio = 1, \n        panel.grid.major = element_line()\n        )+\n  labs(x=expression(t[k]), \n       y=expression(u[k]))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggsave('spending.png', dpi = 240)\n```\n:::\n\n\n<details><summary>Click to see which is which </summary>\n<p>\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n</p>\n</details>\n\n\nOne of the spending functions results in the same $u_k$, regardless of information rate, while the other seems to put a relatively low chance of rejecting the null (low alpha spend) in the beginning but then allows for a larger chance to reject the null later (larger alpha spend).  Now, knowing what we know about the spending function qualitative behaviour, which function corresponds to which spending function?\n\nThe solid line is very clearly the O'Brien Flemming spending function.  When $t_k$ is small, then the O'Brien Flemming spending function has a small amount of spend (because $\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})$ is very small when $t_k$ is small).  But, when $t_k$ is large, then $\\alpha^\\star(t_k) - \\alpha^\\star(t_{k-1})$ is large, leading to more spend and hence smaller $u_k$.  The Pocock function is the dashed line, but I have no good rationale why constant lines should come from a spending function which is not on the diagonal.  I'd love an explanation if you have one.\n\n## Visualising Alpha Spending in Action\n\nIts one thing to talk about alpha spending (@eq-rejections and the probability statements that follow it), but it is another thing completely to see it in action.\n\nI'm going to use `{rpact}` to obtain the $u_k$ for a $K=4$ stage GSD using the O'Brien Flemming spending function.  Then, I'm going to simulate some data from a GSD and compute the spend to show you how it works.  I really hope you take the time to do the same, it can really clear up how the spending works.\n\nFirst, we need data, and a lot of it.  Some of the spend can be on the order of 1e-5, so I'm going to cheat a little.  The book I'm working from writes down the joint distribution of the $Z$ under some assumptions (namely that the standard deviation is known and the data are normal).  Let's use that joint distirbution to simulate 10, 000, 000 samples.  This should give me about 3 decimal places of accuracy.\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(0)\nn = 250\nK = 4\n\n# Construct the Cholesky Factor, row-wise\nA = matrix(rep(0, K*K), nrow = K)\nA[1, 1] = 1\nfor(i in 2:K){\n  A[i, 1:i] = sqrt(n)/sqrt(i*n)\n}\n\n# Construct the covariance \nS = A %*% t(A)\n# Draw from a multivariate normal\n# Lots of draws because the alpha spend will be small\nX = MASS::mvrnorm(10e6, rep(0, K), S)\n```\n:::\n\n\nNow, let's use `{rpart}` to get those critical values as well as how much alpha should be spent at each stage.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nr = rpact::getDesignGroupSequential(\n  kMax = K, \n  sided=2, \n  alpha=0.05, \n  beta=0.2,\n  informationRates = seq(0.25, 1, length.out=K),\n  typeOfDesign = 'OF'\n  )\n\nz_vals = r$criticalValues\naspend = r$alphaSpent\n```\n:::\n\n\n\nNow, its just a matter of taking means. The `ith` column of `X` represents a mean I might see in a group sequential design at the `ith` stage.  We know what the critical value is for each stage, so we just have to estimate the proportion of observations in each column which are beyond the associated critical value.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nX1 = abs(X[, 1])>z_vals[1]\nX2 = abs(X[, 2])>z_vals[2]\nX3 = abs(X[, 3])>z_vals[3]\nX4 = abs(X[, 4])>z_vals[4]\n```\n:::\n\n\nTo compute the alpha spend, we just compute the probability statement above\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nmy_spend = c(\n  mean(X1),\n  mean((!X1)&X2),\n  mean((!X1)&(!X2)&X3),\n  mean((!X1)&(!X2)&(!X3)&X4)\n)\n```\n:::\n\n\nNow, we just take the cumulative sum of `my_spend` to determine how much alpha we spend up to the `ith` stage\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(cumsum(my_spend), aspend, xlab = 'Simulated Spend', ylab='Spend From rpact')\nabline(0, 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOh MAN does that feel good!  We spend very nearly the projected alpha at each stage.  THAT is alpha spending in action!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}