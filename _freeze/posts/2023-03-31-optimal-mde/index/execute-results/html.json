{
  "hash": "06d2262f0687e6be6462d186e856c90f",
  "result": {
    "markdown": "---\ntitle: \"Choosing the Optimal MDE for Experimentation\"\ndate: \"2023-03-31\"\ncode-fold: false\necho: false\nfig-cap-location: top\ncategories: [AB Tests, Statistics]\nnumber-sections: false\ndraft: true\n---\n\n::: {.cell}\n\n:::\n\n\nOne of the most prevalent questions I'm asked about AB tests concerns sample size, and in particular about the minimal detectable effect (MDE).  Set your MDE too big and your experiments are under powered, but you can run a lof of them (at least as many as your team can produce in a given window of time).  Set your MDE too small and you are wasting valuable time because you need to collect large samples.  There is very clearly a trade off between large/small MDEs, and a valuable question concerns the \"Goldilocks\" MDE; not too big, not too small, just right.\n\nI've been thinking about how to better choose the MDE based on previous data, and I think I've come up with a scheme to do so.  In this post, I demonstrate how to use simulation to determine the optimal MDE for experiments.  The MDE is chosen so as to optimize the \"cumulative improvement\" to the metric, under some mild and some strong assumptions. A starting assumption is that the experimenting team can model their experiments using a hierarchical linear model, as I've shown in [this](https://dpananos.github.io/posts/2022-07-20-pooling-experiments/) post.\n\nThe post begins with additional assumptions about the team, constraints imposed thereon, and the nature of experimental effects.  I then describe the simulation procedure, demonstrating how to perform the computations in R. While imperfect, I think this is at least a half way objective method to answer \"what MDE should we use?\".\n\n## Assumptions\n\nHere some assumptions that makes life a little easier:\n\n* The team's entire job is running experiments.  Due to resourcing constraints, they can only run a finite number experiments per year.  For this case study, I'm going to assume the team can run 24 experiments a year (or 2 per month on average).  The team can run experiments back to back.\n* The team works in a frequentist framework, and they always run 2 tailed tests because there is a chance they could hurt the product, and they would want to know that.\n* The main causal contrast is relative risk.  In industry, we call this the \"lift\".\n* The outcome is a binary outcome, and the baseline rate is 8%.\n* 2,500,000 unique visitors to your website per year.\n* The team generates lift fairly reliably and these lifts sustain through time.  There is no decay of the effect, no interaction between experiments, nor is there any seasonality.  These are blatantly false, but they simplify enough for us to get traction.\n* The population level lift distribution is log normal, with parameters $\\mu=\\log(1.01)$ and  $\\sigma=0.01$ on the log scale.  This means the team increases the metric by approximately 2% on average.\n* The team is really only interested in positive effects (lift > 1) so you they implement anything with lift < 1, and if the null fails to be rejected you will stick with the status quo.\n* The same MDE is used to plan all experiments.\n\nFrom here, we have enough to proceed.\n\n## Description of the Procedure\n\nLet the number of unique users landing on the page be `n_uuid`.  This means we can run the minimum of 24 and `floor(n_uuid/(2*n_per_group))` experiments per year.  Since the MDE is used in the sample size calculation, the MDE determines the number of experiments.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n* Let's refer to the number of unique visitors as `n_uuid` and the baseline signup rate as `baseline`.\n* Given an MDE, its easy to determine the requisite sample size.  Call this `n_per_group`.\n* This means you can run `min(24,  n_uuid/n_per_group)` experiments per year.\n* Each experiment has some true underlying lift called `real_lift`.\n* Given the sample size, you can compute the power to detect `real_lift` *but* we're only interested in rejecting the null when the probability the estimated lift is greater than 1. Call this power `power`.\n* Now, all we have to do is simulate binomial numbers for each experiment with probability of success `power`.  When the simulated number is 1, that means we detected that effect, else we failed to detect the effect.\n* Because we assumed lifts sustain through time, then the cumulative improvement to the sign up rate is the product of the detected lifts.\n* The number we are optimizing for is the cumulative improvement to the signup rate (the cumulative lift).\n\nThis is fairly easy to program in R.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWhat I find most interesting here is that the \"optimal MDE\" (i.e the MDE which yields maximal cumulative improvement to signup rate) *is not the average lift the team make*.  Its actually a little larger (~5% MDE where as the team generates a 2% lift on average).  Moreover, its actually *better* to aim for large MDEs than low MDEs, as the expected cumulative improvement to signup rate tapers off much more slowly as the MDE grows large.  This is likely because large MDEs allow for more experiments, meaning more opportunity to improve as opposed to running long experiments to detect small effects.  That makes sense; large MDEs give you more kicks at the can.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}