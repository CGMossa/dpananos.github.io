{
  "hash": "c98ff36c7e504c558fd5f21bc8be7459",
  "result": {
    "markdown": "---\ntitle: Regularization of Noisy Multinomial Countrs\ndate: \"2023-09-08\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: false\n---\n\n\n\nI'm reading [*Statistical Learning with Sparsity*](https://hastie.su.domains/StatLearnSparsity/).  Its like Elements of Statistical Learning, but just for the LASSO.  In chapter 3, there is this interesting example of using the LASSO to estimate rates for noisy multinomial processes.  Here is the setup from the book\n\n> Suppose we have a sample of $N$ counts $\\{ y_k \\}_{k-=1}^N$ from an $N$-cell multinomial distribution, and let $r_k = y_k / \\sum_{\\ell=1}^N y_{\\ell}$ be the corresponding vector of proportions.  For example, in large-scale web applications, these counts might represent the number of people in each country in teh YSA that visited a particular website during a given week.  This vector could be sparse depending on the specifics, so there is desire to regularize toward a broader, more stable distribution $\\mathbf{u} = \\{ u_k \\}_{k=1}^N$ (for example, the same demographic, except measured over years)\n\nIn essence, we fit a LASSO using the $\\mathbf{u}$ as an offset.The authors go on to show how minimizing a discrete version of the KL-divergence with some error tolerance between estimated and observed can be re-framed as constrained optimization of the Poisson log likelihood.  In short, you can fit a glmnet model with `family=\"poisson\"` to do the procedure described in that quote.\n\nThe authors kind of give a half hearted attempt at an application of such a procedure, so I tried to come up with one myself.  In a [previous blog post](https://dpananos.github.io/posts/2022-08-16-pca/), we used looked at what kind of experts exist on statsexchange.  We can use the data explorer I used for that post to look at two frequencies:\n\n* Frequencies of total posts over a typical week in the year 2022, and\n* Frequencies of posts tagged \"regularization\" posts over a typical week  in the year 2022.\n\nThere are considerably more non-regularization posts than there are regularization posts, so while we should expect the frequencies to be similar there is probably some noise.  Let's use the query (below) to get our data and make a plot of those frequencies.\n\n<details><summary>Click to see SQL Query </summary>\n<p>\n\n```\nselect\ncast(A.CreationDate as date) as creation_date,\ncount(distinct A.id) as num_posts,\ncount(distinct case when lower(TargetTagName) = 'regularization' then A.id end) as reg_posts\nfrom Posts A\nleft join PostTags as B on A.Id = B.PostId\nleft join TagSynonyms as C on B.TagId = C.Id\nwhere A.PostTypeId = 1 and A.CreationDate between '2022-01-01' and '2022-12-31'\ngroup by cast(A.CreationDate as date)\norder by cast(A.CreationDate as date)\n\n```\n\n</p>\n</details>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(kableExtra)\n\ntheme_set(\n  theme_minimal(base_size = 12) %+replace%\n    theme(\n      aspect.ratio = 1/1.61,\n      legend.position = 'top'\n    )\n  )\n\n\nd <- read_csv('QueryResults.csv') %>% \n     mutate_at(\n       .vars = vars(contains('post')),\n       .funs = list(p = \\(x) x/sum(x))\n     )\n\nd$oday <- factor(d$Day, ordered=T, levels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))\n\nd %>% \n  arrange(oday) %>% \n  select(oday, num_posts, reg_posts) %>% \n  kbl(\n    col.names = c('Day of Week', 'Total Post Count', 'Regularization Post Count')\n  ) %>% \n  kable_styling(bootstrap_options = 'striped')\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Day of Week </th>\n   <th style=\"text-align:right;\"> Total Post Count </th>\n   <th style=\"text-align:right;\"> Regularization Post Count </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Sunday </td>\n   <td style=\"text-align:right;\"> 1699 </td>\n   <td style=\"text-align:right;\"> 13 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Monday </td>\n   <td style=\"text-align:right;\"> 2494 </td>\n   <td style=\"text-align:right;\"> 37 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Tuesday </td>\n   <td style=\"text-align:right;\"> 2759 </td>\n   <td style=\"text-align:right;\"> 24 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wednesday </td>\n   <td style=\"text-align:right;\"> 2805 </td>\n   <td style=\"text-align:right;\"> 26 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Thursday </td>\n   <td style=\"text-align:right;\"> 2689 </td>\n   <td style=\"text-align:right;\"> 32 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Friday </td>\n   <td style=\"text-align:right;\"> 2500 </td>\n   <td style=\"text-align:right;\"> 27 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Saturday </td>\n   <td style=\"text-align:right;\"> 1703 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbase_plot <- d %>% \n            ggplot() + \n            geom_point(aes(oday, num_posts_p, color='All Posts')) + \n            geom_point(aes(oday, reg_posts_p, color='Regularization Posts')) + \n            scale_y_continuous(labels = scales::percent) + \n            labs(x='Day', y='Post Frequency', color='')\n            \nbase_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nObviously, the smaller number of posts about regularization leads to larger noise in the multinomial estimates.  You can very easily see that in the plot; Monday is not special, its very likely noise.\n\nSo here is where we can use LASSO to reign in some of that noise.  We just need to specify how discordant we want our predicted frequencies to be from the observed frequencies, and fit a LASSO model with a penalty which achieves this desired tolerance.\n\nSo let's say my predictions from the LASSO are the vector $\\mathbf{p}$ and the observed frequencies for regularization posts are $\\mathbf{r}$.  Let's say I want the largest error between the two to be no larger than 0.05.  Shown below is some code for how to do that:\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# Set up the regression problem\nu <- d$num_posts_p\nr <- d$reg_posts_p\nX <- model.matrix(~Day-1, data=d)\n\n# Grid of penalties to search over\nlambda.grid <- 2^seq(-8, 0, 0.005)\n\n# Fir the model and compute the difference between the largest error and our tolerance\nfit_lasso <- function(x){\n  fit <- glmnet(X, r, family='poisson', offset=log(u), lambda=x)\n  p <- predict(fit, newx=X, newoffset=log(u), type='response')\n  abs(max(r-p) - 0.03)\n}\n\nerrors<-map_dbl(lambda.grid, fit_lasso)\nlambda <- lambda.grid[which.min(errors)]\nfit <- glmnet(X, r, family='poisson', offset=log(u), lambda=lambda)\nd$predicted <- predict(fit, newx=X, newoffset=log(u), type='response')[, 's0']\n\nnew_plot <- d %>% \n            ggplot() + \n            geom_point(aes(oday, num_posts_p, color='All Posts')) + \n            geom_point(aes(oday, reg_posts_p, color='Regularization Posts')) + \n            geom_point(aes(oday, predicted, color='Predicted Posts')) + \n            scale_y_continuous(labels = scales::percent) + \n            labs(x='Day', y='Post Frequency', color='')\n\nnew_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\nThis might be easier to see if we follow Tuffte and show small multiples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlam <- c(0.001, 0.01, 0.02, 0.04)\n\nall_d <- map_dfr(lam, ~{\n  fit <- glmnet(X, r, family='poisson', offset=log(u), lambda=.x)\n  d$predicted <- predict(fit, newx=X, newoffset=log(u), type='response')[, 's0']\n  d$lambda = .x\n  \n  d\n})\n\nall_d %>% \n      ggplot() + \n      geom_point(aes(oday, num_posts_p, color='All Posts')) + \n      geom_point(aes(oday, reg_posts_p, color='Regularization Posts')) + \n      geom_point(aes(oday, predicted, color='Predicted Posts')) + \n      scale_y_continuous(labels = scales::percent) + \n      labs(x='Day', y='Post Frequency', color='') + \n      facet_wrap(~lambda, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\nCool!  Using $\\mathbf{u}$ (here, the frequency of total posts on each day) acts as a sort of target towards wich to regularize. \n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}