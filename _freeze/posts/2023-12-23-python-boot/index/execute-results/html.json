{
  "hash": "a049bf193ae805c6331d8898ed505dec",
  "result": {
    "markdown": "---\ntitle: Boostrapping in Python\ndate: \"2023-12-31\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: false\n---\n\nI keep forgetting the bootstrap is a thing (I've been spoiled by R where most of the standard errors for things I want are reported readily).  I guess there are a few implementations in statsmodels and scipy, but frankly I don't like statsmodels and I don't like reading documentation as much as I like hacking to (re)learn stuff.\n\nSo here is some boilerplate for me (or you) to use next time someone brings up the bootstrap in python.  Below, we'll just write a little container class for the bootstrsap results using `dataclasses.dataclass` and then a couple of functions for immplementing the bias corrected and percentile bootstrap confidence intervals.\n\n## Bootstrap in Python\n\n\nOK, first thing we need to do is create a dataclass to store all the information like the type of bootstrap confidence interval we computed, our estimate, and confidence limits.  Data classes are a lightweight way to do this.  I'll add a `__str__` method so we can print out the result nicely.\n\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n@dataclass\nclass BootstrapEstimate:\n    name: str\n    estimate: float\n    ci_lower: float\n    ci_upper: float\n\n    def __str__(self):\n\n        return f\"{self.name} Interval: {self.estimate:.2f} ({self.ci_lower:.2f} -- {self.ci_upper:.2f})\"\n```\n:::\n\n\nNow, we'll create two functions to estimate bootstrap confidence intervals.  One will be the percentile method, and the other will be the bias corrected method.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\ndef ci_percentile(estimates, alpha:float) -> np.ndarray:\n\n    q = [100*alpha/2, 100*(1-alpha/2)]\n    return np.percentile(\n        a=estimates, \n        q=q, \n        method='median_unbiased'\n    )\n\n\ndef ci_bias_corrected(estimates, theta, alpha:float) -> np.ndarray:\n\n    p0 = np.mean(estimates <= theta)\n    z0 = norm.ppf(p0)\n    za = norm.ppf([alpha/2, 1-alpha/2])\n    g_theta_bc =  norm.cdf(2*z0 + za)\n\n    return np.percentile(\n        a=estimates, \n        q=100*g_theta_bc, \n        method='median_unbiased'\n    )\n```\n:::\n\n\nLastly, we just need a function to do the bootstrapping.  I like doing this with two functions:  one to create a generator of bootstrapped datasets and another to do all the estimation.  I also use `joblib` so I can do this in parallel with more bootstrap resamples than I'd do otherwise.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\ndef make_bootstrap_samples(data, n_boot:int, prng=None) -> Generator:\n\n    if not prng:\n        prng = np.random.RandomState()\n\n    for i in range(n_boot):\n        ix = prng.randint(0, len(data), size = len(data))\n        yield data[ix]\n\ndef bootstrap(data, statistic:Callable, n_boot:int, ci_type:str='percentile', alpha=0.05, prng=None, n_jobs=-1, verbose=0) -> BootstrapEstimate:\n\n\n    straps = make_bootstrap_samples(data=data, n_boot=n_boot, prng=prng)\n    delayed_statistic = delayed(statistic)\n    estimates_list = Parallel(n_jobs=n_jobs, verbose=verbose)(delayed_statistic(bdat) for bdat in straps)\n    estimates = np.array(estimates_list)\n\n    match ci_type:\n        case 'percentile':\n            name = \"Percentile\"\n            ci_lower, ci_upper = ci_percentile(estimates=estimates, alpha=alpha)\n        case 'BC':\n            name = 'Bias Corrected'\n            ci_lower, ci_upper = ci_bias_corrected(estimates=estimates, theta=statistic(data), alpha=alpha)\n        case _:\n            raise ValueError('Not implemented yet')\n\n    result = BootstrapEstimate(\n                                name=name, \n                                estimate=statistic(data),\n                                ci_lower=ci_lower,\n                                ci_upper=ci_upper\n                                )\n\n    return result\n```\n:::\n\n\nOK, now let's test it out.  Let's make 1000 draws from a standard gaussian and estimate the 97.5% quantile which should be around 1.96.  I'll make 10, 000 bootstrap resamples which is about 1000 samples per core on my machine.\n\n::: {.cell highlight-style='python' execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nx = np.random.normal(size=1000)\nstatistic = lambda x: np.percentile(x, 97.5)\n\na = bootstrap(data=x, statistic=statistic, n_boot=10_000, ci_type='BC')\nb = bootstrap(data=x, statistic=statistic, n_boot=10_000, ci_type='percentile')\n\nprint(a)\nprint(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBias Corrected Interval: 1.83 (1.67 -- 2.02)\nPercentile Interval: 1.83 (1.66 -- 2.00)\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}