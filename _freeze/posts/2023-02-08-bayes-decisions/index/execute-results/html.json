{
  "hash": "44e82542d59840eeb5b61a0409cc1838",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Decision Making for AB Tests on Revenue\"\neditor: visual\ndate: \"2023-02-08\"\ncode-fold: false\necho: false\nfig-cap-location: top\n\ncategories: [AB Tests, Statistics, Bayes]\nnumber-sections: false\n---\n\n\nA lot of companies operate on a subscription service.  Every month, you give them money and they allow you to use a service.  The quality of extent of that service depends on how much money you give them.  For example, Netflix allows you to pay more for higher definition content, Calendly includes some automation with their higher tiered plans, Apple provides you with more iCloud storage, and its all for a cost.  \n\nThese companies make more money if you choose the higher priced plan (duh), but they have to show you the value so that you *want* the higher priced plan.  Sometimes, this comes in the form of a free trial for a limited time; no additional cost to the consumer, but hopefully you like it enough to shell out some cash.\n\nThere is a trade off to be made here.  Try a *too* premium plan and you might get hit by sticker shock and fail to convert. Not premium enough, and the trial fails to convert users who would have found value in the more premium plans.\n\nDeciding on which plan to roll out as the default trial is a question for another post.  Assuming there is an existing decision I'm here to answer\n\n> How do we analyze experiments where we have to balance both conversion from the trial and revenue from users who find value in more premium plans?\n\nSeemingly easy, just do a t-test on log revenue or something.  Not so fast.  Consider the following:\n\n* Not every randomized user converts, meaning there are many censored revenue observations.  Filling them in with 0 prevents using the log to reign in possible long tails, and $\\log(1+x)$ has been shown to yield arbitrarily small/large ATEs.\n\n* If you're conversion rate is small enough, maybe getting enough users to power your experiment would take months and months and months.  Maybe you have the stomach for that, maybe you don't.  Its certainly my experience that the shorter the experiment can run, the faster we can improve.\n\nIn this post, I'll present a (pretty niche, honestly) Bayesian model of the process.  I'll then demonstrate how to use the model to compute expected loss in revenue.  The \"winner\" of the AB test is then the variant which produces smallest expected loss.\n\n## Revenue In Terms of Conversions\n\nLet's say you have $j=1, \\cdots, J$ plan levels. Let's assume\n\n* The probability a user converts (chooses to pay you, $C$) is $P(C)$.\n* Once the user converts, the probability the user chooses plan $j$ is $P(T=j\\mid C)$.\n* Finally, each plan produces some amount of expected revenue $E[R \\mid T=j]$.\n\nUsing the law of total expectation, the expected revenue from a given free trail plan should be\n\n$$ E[R] = \\sum_j E[R \\vert T=j] \\cdot P(T=j \\mid C) \\cdot P(C) \\>. $$\n\nIf we could jointly estimate these quantities, then we could integrate over the uncertainty in each of them and compute $E[R]$.  If we could compute $E[R]$ for two groups (say test and control), we could do Bayesain decision making.\n\n## A Possible Model\n\nLet's make some simplifying assumptions.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}