{
  "hash": "92576b2efd5c96271ad01b072334127c",
  "result": {
    "markdown": "---\ntitle: \"Gradient Descent with ODEs\"\ndate: \"2019-05-21\"\ncategories: [Python, Machine Learning, Statistics]\n---\n\nGradient descent usually isn't used to fit Ordinary Differential Equations (ODEs) to data (at least, that isn't how the Applied Mathematics departments to which I have been a part have done it).  Nevertheless, that doesn't mean that it can't be done.  For some of my recent GSoC work, I've been investigating how to compute gradients of solutions to ODEs without access to the solution's analytical form.  In this blog post, I describe how these gradients can be computed and how they can be used to fit ODEs to synchronous data with gradient descent.\n\n## Up To Speed With ODEs\n\nI realize not everyone might have studied ODEs.  Here is everything you need to know:\n\nA differential equation relates an unknown function $y \\in \\mathbb{R}^n$ to it's own derivative through a function $f: \\mathbb{R}^n \\times \\mathbb{R} \\times \\mathbb{R}^m \\rightarrow  \\mathbb{R}^n$, which also depends on time $t \\in \\mathbb{R}$ and possibly a set of parameters $\\theta \\in \\mathbb{R}^m$.  We usually write ODEs as\n\n\n$$y' = f(y,t,\\theta) \\quad y(t_0) = y_0$$\n\n\nHere, we refer to the vector $y$ as \"the system\", since the ODE above really defines a system of equations.  The problem is usually equipped with an initial state of the system $y(t_0) = y_0$ from which the system evolves forward in $t$.  Solutions to ODEs in analytic form are often *very hard* if not impossible, so most of the time we just numerically approximate the solution.  It doesn't matter how this is done because numerical integration is not the point of this post.  If you're interested, look up the class of *Runge-Kutta* methods.\n\n## Computing Gradients for ODEs\n\nIn this section, I'm going to be using derivative notation rather than $\\nabla$ for gradients.  I think it is less ambiguous.\n\nIf we want to fit an ODE model to data by minimizing some loss function $\\mathcal{L}$, then gradient descent looks like\n\n\n$$ \\theta_{n+1} = \\theta_n - \\alpha \\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} $$\n\n\nIn order to compute the gradient of the loss, we need the gradient of the solution, $y$, with respect to $\\theta$.  The gradient of the solution is the hard part here because it can not be computed (a) analytically (because analytic solutions are hard AF), or (b) through automatic differentiation without differentiating through the numerical integration of our ODE (which seems computationally wasteful).\n\nThankfully, years of research into ODEs yields a way to do this (that is not the adjoint method.  Surprise!  You thought I was going to say the adjoint method didn't you?).  Forward mode sensitivity analysis calculates gradients by extending the ODE system to include the following equations:\n\n\n$$ \\dfrac{d}{dt}\\left( \\dfrac{\\partial y}{\\partial \\theta} \\right) = \\mathcal{J}_f \\dfrac{\\partial y}{\\partial \\theta} +\\dfrac{\\partial f}{\\partial \\theta} $$\n\n\nHere, $\\mathcal{J}$ is the Jacobian of $f$ with respect to $y$.  The forward sensitivity analysis is *just another differential equation* (see how it relates the derivative of the unknown $\\partial y / \\partial \\theta$ to itself?)!  In order to compute the gradient of $y$ with respect to $\\theta$ at time $t_i$, we compute\n\n\n$$ \\dfrac{\\partial y}{\\partial \\theta} = \\int_{t_0}^{t_i} \\mathcal{J}_f \\dfrac{\\partial y}{\\partial \\theta} + \\dfrac{\\partial f}{\\partial \\theta} \\, dt $$\n\n\nI know this looks scary, but since forward mode sensitivities are just ODEs, we actually just get this from what we can consider to be a black box\n\n\n$$\\dfrac{\\partial y}{\\partial \\theta} = \\operatorname{BlackBox}(f(y,t,\\theta), t_0, y_0, \\theta)$$\n\n\nSo now that we have our gradient in hand, we can use the chain rule to write\n\n\n$$\\dfrac{\\partial \\mathcal{L}}{\\partial \\theta} =\\dfrac{\\partial \\mathcal{L}}{\\partial y} \\dfrac{\\partial y}{\\partial \\theta} $$\n\n\nWe can use automatic differentiation to compute $\\dfrac{\\partial \\mathcal{L}}{\\partial y}$.\n\nOK, so that is some math (interesting to me, maybe not so much to you).  Let's actually implement this in python.\n\n\n## Gradient Descent for the SIR Model\n\nThe SIR model is a set of differential equations which govern how a disease spreads through a homogeneously mixed closed populations.  I could write an entire thesis on this model and its various extensions (in fact, I have), so I'll let you read about those on your free time.\n\nThe system, shown below, is parameterized by a single parameter:\n\n\n$$ \\dfrac{dS}{dt} = -\\theta SI \\quad S(0) = 0.99 $$\n\n$$ \\dfrac{dI}{dt} = \\theta SI - I \\quad I(0) = 0.01 $$\n\n\nLet's define the system, the appropriate derivatives, generate some observations and fit $\\theta$ using gradient descent.  Here si what you'll need to get started:\n\n``` {.python .cell-code}\nimport autograd\nfrom autograd.builtins import tuple\nimport autograd.numpy as np\n\n#Import ode solver and rename as BlackBox for consistency with blog\nfrom scipy.integrate import odeint as BlackBox\nimport matplotlib.pyplot as plt\n```\n\n\nLet's then define the ODE system\n\n``` {.python .cell-code}\ndef f(y,t,theta):\n    '''Function describing dynamics of the system'''\n    S,I = y\n    ds = -theta*S*I\n    di = theta*S*I - I\n\n    return np.array([ds,di])\n```\n\n\nand take appropriate derivatives\n\n``` {.python .cell-code}\n#Jacobian wrt y\nJ = autograd.jacobian(f,argnum=0)\n#Gradient wrt theta\ngrad_f_theta = autograd.jacobian(f,argnum=2)\n```\n\n\nNext, we'll define the augmented system (that is, the ODE plus the sensitivities).\n\n``` {.python .cell-code}\ndef ODESYS(Y,t,theta):\n\n    #Y will be length 4.\n    #Y[0], Y[1] are the ODEs\n    #Y[2], Y[3] are the sensitivities\n\n    #ODE\n    dy_dt = f(Y[0:2],t,theta)\n    #Sensitivities\n    grad_y_theta = J(Y[:2],t,theta)@Y[-2::] + grad_f_theta(Y[:2],t,theta)\n\n    return np.concatenate([dy_dt,grad_y_theta])\n```\n\n\nWe'll optimize the $L_2$ norm of the error\n\n``` {.python .cell-code}\ndef Cost(y_obs):\n    def cost(Y):\n        '''Squared Error Loss'''\n        n = y_obs.shape[0]\n        err = np.linalg.norm(y_obs - Y, 2, axis = 1)\n\n        return np.sum(err)/n\n\n    return cost\n```\n\n\nCreate some observations from which to fit\n\n::: {.cell fig-height='5' fig-width='5' execution_count=6}\n``` {.python .cell-code}\nnp.random.seed(19920908)\n## Generate Data\n#Initial Condition\nY0 = np.array([0.99,0.01, 0.0, 0.0])\n#Space to compute solutions\nt = np.linspace(0,5,101)\n#True param value\ntheta = 5.5\n\nsol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta]))\n\n#Corupt the observations with noise\ny_obs = sol[:,:2] + np.random.normal(0,0.05,size = sol[:,:2].shape)\n\nplt.scatter(t,y_obs[:,0], marker = '.', alpha = 0.5, label = 'S')\nplt.scatter(t,y_obs[:,1], marker = '.', alpha = 0.5, label = 'I')\n\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n<matplotlib.legend.Legend at 0x7f8e9d0b3d30>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=571 height=404}\n:::\n:::\n\n\nPerform Gradient Descent\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ntheta_iter = 1.5\ncost = Cost(y_obs[:,:2])\ngrad_C = autograd.grad(cost)\n\nmaxiter = 100\nlearning_rate = 1 #Big steps\nfor i in range(maxiter):\n\n    sol = BlackBox(ODESYS,y0 = Y0, t = t, args = tuple([theta_iter]))\n\n    Y = sol[:,:2]\n\n    theta_iter -=learning_rate*(grad_C(Y)*sol[:,-2:]).sum()\n\n    if i%10==0:\n        print(\"Theta estimate: \", theta_iter)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  1.697027594337629\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  3.9189060278370365\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  4.810038385538704\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.251499985105974\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.427206219478129\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.46957706068474\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.47744643541383\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.4792194685272095\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.479636817124458\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTheta estimate:  5.47973599525063\n```\n:::\n:::\n\n\nAnd lastly, compare our fitted curves to the true curves\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nsol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta_iter]))\ntrue_sol = BlackBox(ODESYS, y0 = Y0, t = t, args = tuple([theta]))\n\n\nplt.plot(t,sol[:,0], label = 'S', color = 'C0', linewidth = 5)\nplt.plot(t,sol[:,1], label = 'I', color = 'C1', linewidth = 5)\n\nplt.scatter(t,y_obs[:,0], marker = '.', alpha = 0.5)\nplt.scatter(t,y_obs[:,1], marker = '.', alpha = 0.5)\n\n\nplt.plot(t,true_sol[:,0], label = 'Estimated ', color = 'k')\nplt.plot(t,true_sol[:,1], color = 'k')\n\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n<matplotlib.legend.Legend at 0x7f8e9de4e5e0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=571 height=404}\n:::\n:::\n\n\n## Conclusions\n\nFitting ODEs via gradient descent is possible, and not as complicated as I had initially thought.  There are still some relaxations to be explored.  Namely: what happens if we have observations at time $t_i$ for one part of the system but not the other?  How does this scale as we add more parameters to the model?  Can we speed up gradient descent some how (because it takes too long to converge as it is, hence the `maxiter` variable).  In any case, this was an interesting, yet related, divergence from my GSoC work.  I hope you learned something.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}