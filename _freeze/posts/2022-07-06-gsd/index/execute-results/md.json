{
  "hash": "cde532731e075437788477c90c6bed6b",
  "result": {
    "markdown": "---\ntitle: Interim Analysis & Group Sequential Designs Pt 1\ndate: \"2022-07-06\"\ncode-fold: true\nfig-cap-location: top\ncategories: [Statistics, AB Testing]\nnumber-sections: false\nimage: joint_dist.png\nformat: \n  gfm:\n    html-math-method: webtex\n---\n\n\n> Special thanks to [Jacob Fiksel](https://twitter.com/jfiksel1) for writing a great [blog post](file:///Users/demetripananos/Zotero/storage/HQG5JJ4N/2021-02-03-alpha_spending_explained.html) which inspired me to write my own.\n\n\n\nAt Zapier, AB testing kind of has a bad rap.  AB testing is perceived as slow -- sometimes taking up to a month to complete a single test-- with the chance that we don't get a definitive result (i.e. we fail to reject the null). One of our priorities (and hence my priority) is to find a way to speed up AB testing so we can learn faster.\n\nPeeking is one way to do that. Peeking involves testing the experimental data before the end of the experiment (\"peeking\" at the results to see if they indicate a change). As you may know from other [popular posts](https://www.evanmiller.org/how-not-to-run-an-ab-test.html) on the matter, or from sophomore stats, this can inflate the type one error.  That's a real shame, because peeking is a really attractive way to end an experiment early and save some time. Additionally, people are curious!  They want to know how things are going.  Fortunately, there are ways to satisfy the urge to peek while preserving the type one error rate.\n\nOne way to peek while preserving the type one error rate is through Group Sequential Designs (GSDs).  This series of blog posts is intended to delve into some of the theory of GSDs.  To me, theoretical understanding -- knowing why something works, or at least being able to understand how in principle I could do this myself -- is the key to learning.  I'm happy to just do this in isolation, but I bet someone else may benefit too.\n\nI'm working mainly from [this](https://link.springer.com/book/10.1007/978-3-319-32562-0) book, but I don't anticipate I will discuss the entirety of the book.  I really want to know a few key things:\n\n* What is the foundational problem for peeking?\n* How can we address that problem (i.e. How can we preserve the type one error when we peek)?\n* How else can we speed up experiments (e.g. by declaring an experiment futile)?\n* What is the theory underlying each of the above?\n\n## Goal For This Post\n\nWe know that under \"peeking conditions\" -- just testing the data as they roll in -- inflates the type one error rate.  In this post, I want to understand *why* that happens.  Like...where is the problem *exactly*?  Where will be our theoretical basis for attacking the problem of controlling the type one error rate?\n\nBut first, a little background on GSDs.\n\n## Background\n\nThe \"*G*\" in GSD means that the hypothesis test is performed on groups of observations.  Given a maximum number of groups $K$, the sample size of $k^{th}$ each group is $n_k$.\n\nThe \"*S*\" in GSD means the test is performed sequentially. If after observing the $k^{th}$ group the test statistic (computed using all the data observed up to that point) is beyond some threshold, then the null is rejected and the experiment is finished.  If not, the next group of observations is made and added to the existing data, wherein the process continues until the final group has been observed.  If after observing the final group the test statistic does not exceed the threshold, then we fail to reject the null.  The process for $K=2$ is illustrated in @fig-gsd.\n\n\n```{mermaid}\n%%| label: fig-gsd\n%%| fig-cap: A GSD for $K=2$\n%%| fig-cap-location: top\n%%| fig-align: center\nflowchart TD\n  A[Observe Group k=1] --> B[Perform Test]\n  B --> C{Data From k=1 \\n Significant?}\n  C -- Yes --> D[Reject Null]\n  C -- No --> E[Observe Group k=2]\n  E --> G{Data From k=1 and \\n k=2 Significant?}\n  G -- Yes --> D\n  G -- No --> H[Fail To Reject Null]\n```\n\n\n## Some Math on Means\n\nMeans are a fairly standard place to start for a statsitical test, so we will start there too.  Let $X_{k, i}$ be the $i^{th}$ observation in the $k^{th}$ group.  Then the mean of group $k$ is \n\n\n$$ \\bar{X}_k = \\dfrac{1}{n_k} \\sum_{i=1}^{n_{k}} X_{k, i} $$\n\n\nSince we are accumulating data, let's write the cumulative mean up to and including group $k$ as $\\bar{X}^{(k)}$, and let the cumulative standard deviation up to and including group $k$ be $\\sigma^{(k)}$.  We can actually write $\\bar{X}^{(k)}$ in terms of the group means $\\bar{X}_{k}$ using some algebra.  Its just a weighted mean of the previous $\\bar{X}_{k}$ weighted by the sample size.\n\n\n\n\n$$\n \\bar{X}^{(k)} =  \\dfrac{\\sum_{\\tilde{k} = 1}^{k^\\prime} n_{\\tilde{k}} \\bar{X}_{\\tilde{k}}}{\\sum_{\\tilde{k} = 1}^{k^\\prime} n_{\\tilde{k}}}\n$$ {#eq-eq-1}\n\n\n\n## A Simple Example\n\nRemember that our goal is to understand *why*  the type one error rate increases when we peek as data accumulates, as we might do in an AB test.  Answering *how much* is a little easier, so let's do that first.  Let's do so by analyzing a $K=2$ GSD where we assume:\n\n* That each group has the same sample size $n_1 = n_2 = n$.\n* That the data we observe are IID bernoulli trials $X_{k, i} \\sim \\operatorname{Bernoulli}(p=0.5)$ for $k=1, 2$ and $j=1, \\dots, n$.\n* That our false postie rate $\\alpha = 0.05$\n\n### *How Much* Does The Type One Inflate?\n\nLet's just simulate data under the assumptions above.  At each stage, let's test the null that $H_0: p=0.5$ against $H_A: p \\neq 0.5$ and see how frequently we reject the null. In our simulation, we will assume \"peeking\" conditions, meaning we're just going to do a test of proportions at each stage.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(tidyverse)\n\nset.seed(0)\n\n# Simulation Parameters\np<- 0.5\nn<- 250\nnsims <- as.integer((1/0.01)^2)\n\n# Run the simulation\nsims<-rerun(nsims, {\n  # K=1\n  x1 <- rbinom(1, n, p)\n  # K=2, accumulating data from each state\n  x2 <- x1 + rbinom(1, n, p)\n  \n  # Compute some various quntities we will need, like the Z score\n  K <- str_c('K=',1:2)\n  X <- c(x1, x2) / ((1:2)*n)\n  mu <- p\n  sds <- sqrt(p*(1-p)/(n*1:2))\n  Z <- (X-p)/sds\n  reject <- abs(Z)>1.96\n  \n  tibble(K, X, mu, sds, Z, reject)\n}) %>% \n  bind_rows(.id='sim')\n\nfpr<-sims %>% \n  group_by(sim) %>% \n  summarise(result=any(reject)) %>% \n  summarise(fpr = mean(result)) %>% \n  pull(fpr)\n```\n:::\n\n\nFrom our simulation, we reject the null around 8.6% of the time.  That is certainly higher than the nominal 5%, but if we recall our sophomore stats classes, isn't there a 9.8% ($1-0.95^2$) chance we reject the null?\n\nThe 8.6% isn't simulation error.  We forgot that $\\bar{X}^{(1)}$ and $\\bar{X}^{(2)}$ are *correlated*. The correlation between $\\bar{X}^{(1)}$ and $\\bar{X}^{(2)}$ makes intuitive sense.  If the sample mean for the first group is small, then the accumulated mean is also likely to be small than if we were to just take a new sample.  Let's take a more detailed look at @eq-eq-1. Note that \n\n\n$$ \\bar{X}^{(2)} = \\dfrac{n_1 \\bar{X}_1 + n_2\\bar{X}_2}{n_1 + n_2} \\>.$$\n\n\n$\\bar{X}^{(1)}$ (which is just $\\bar{X}_1$ ) appears in the expression for $\\bar{X}^{(2)}$.  In the extreme case where $n_2=1$, the stage 2 mean is going to be $n_1 \\bar{X}_1/(n_1+1) + X_{2, 1}/(n_1+1)$.  How much could a single observation change the sample mean?  It depends on the observation, but also on how big that sample is.  The stuff you learned in sophmore stats about type one error inflating like $1 - (1-\\alpha)^k$ assumes the test statistics are independent.  So where does the 8.6% come from?  To answer that, we need to understand the joint distribution of the  $\\bar{X}^{(k)}$.\n\n### *Why* The Type One Inflates\n\nThe assumptions we made above allow us to get a little analytic traction.  We know that the sampling distribution of $\\bar{X}^{(1)}$ and $\\bar{X}^{(2)}$ are asymptotic normal thanks to the CLT\n\n\n$$ \\bar{X}^{(1)} \\sim \\operatorname{Normal}\\left(p, \\dfrac{p(1-p)}{n}\\right)  $$\n\n$$ \\bar{X}^{(2)}\\sim \\operatorname{Normal}\\left( p, \\dfrac{p(1-p)}{2 n} \\right)  $$\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_blue <- rgb(45/250, 62/250, 80/250, 1)\ntheme_set(theme_classic())\n\n\nsims %>% \n  ggplot(aes(X))+\n  geom_histogram(aes(y=..density..), fill = 'light gray', color = 'black')+\n  facet_wrap(~K) + \n  geom_line(aes(y = dnorm(X,\n                          mean = mu,\n                          sd = sds[PANEL])),\n            color = my_blue, \n            size = 1)+\n  theme(\n    panel.grid.major = element_line()\n  )+\n  labs(y='Density',\n       x = expression(bar(X)^(k)))\n```\n\n::: {.cell-output-display}\n![10,000 simulations of a $K=2$ GSD. Each group has 250 observations.  Note that $\\bar{X}^{(2)}$ has smaller standard error due to the fact that 500 (250 + 250) observations are used in the computation. Sampling distributions show in blue.](index_files/figure-gfm/marginal-density-1.png)\n:::\n:::\n\n\nConsider the random vector $\\theta = \\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right)$.  Since each components has a normal marginal then the joint must be multivariate normal\n\n\n$$ \\theta \\sim \\mathcal{N}(\\mathbf{p}, \\Sigma) $$\n\n\nwith mean $\\mathbf{p} = (p,p)$ and covariance[^1] \n\n[^1]: See the @sec-cov for a calculation\n\n\n$$ \\Sigma= p(1-p)\\begin{bmatrix}\n\\dfrac{1}{n_1} &  \\dfrac{1}{n_1 + n_2} \\\\\n\\dfrac{1}{n_1 + n_2} & \\dfrac{1}{n_1 + n_2}\n\\end{bmatrix}\n$$\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma_1 <- sqrt(qchisq(0.95, 2))\nsigma_2 <- sqrt(qchisq(0.99, 2))\nsig<- p*(1-p) * matrix(c(1/n, 1/(2*n), 1/(2*n), 1/(2*n) ), nrow = 2)\ntt <- seq(0, 1, 0.01)\nx <- cos(2*pi*tt)\ny <- sin(2*pi*tt)\nR <- cbind(x,y)\ne = eigen(sig)\nV = sqrt(diag(e$values))\n\nlevel_curve_1 <- sigma_1*R %*% (e$vectors %*% V %*% t(e$vectors)) + p\ncolnames(level_curve_1) <- c(\"X1\", \"X2\")\nlevel_curve_1 <- as_tibble(level_curve_1)\nlevel_curve_2 <- sigma_2*R %*% (e$vectors %*% V %*% t(e$vectors)) + p\ncolnames(level_curve_2) <- c(\"X1\", \"X2\")\nlevel_curve_2 <- as_tibble(level_curve_2)\n\njoint <-sims %>% \n  select(sim, K, X) %>% \n  pivot_wider(names_from='K', values_from='X') %>% \n  rename(X1 = `K=1`, X2=`K=2`) %>% \n  select(-sim) %>% \n  sample_n(1000)\n\njoint %>% \n  ggplot(aes(X1, X2))+\n  geom_point(color = 'dark gray', fill='gray', alpha = 0.5, shape=21)+\n  geom_path(data=level_curve_1, aes(X1, X2), color = my_blue, linetype='dashed')+\n  geom_path(data=level_curve_2, aes(X1, X2), color = my_blue)+\n  lims(x=c(.4, .6), y=c(.4, .6))+\n  theme(\n    panel.grid.major = element_line(),\n    aspect.ratio = 1\n  )+\n  labs(x=expression(bar(X)^(1)),\n       y=expression(bar(X)^(2)))\n```\n\n::: {.cell-output-display}\n![1000 samples from the density of $\\theta$.  Dashed line indicates where region of 95\\% probability, solid line indicates region of 99\\% probability.](index_files/figure-gfm/fig-level-curves-1.png){#fig-level-curves}\n:::\n:::\n\n\nNow that we know the joint sampling distribution for our statistics of interest (namely $\\bar{X}^{(1)}$ and $\\bar{X}^{(2)}$), let's examine when we would reject the null under \"peeking\" conditions. For brevity, let's call $Z^{(k)}$ the standardized cumulative means.  Then we would reject the null under \"peeking\" conditions if $\\Big\\vert Z^{(k)} \\Big\\vert > 1.96$ for at least one $k=1, 2$.  As a probabilistic statement, we want to know\n\n\n$$ Pr\\left( \\Big\\vert Z^{(1)} \\Big\\vert > 1.96 \\cup 1.96 < \\Big\\vert Z^{(2)} \\Big\\vert \\right) \\>. $$\n\nBecause the joint is multivariate normal, we can compute this probability directly.  However, I'm just going to simulate it.\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nZ<- MASS::mvrnorm((1/0.001)^2, rep(p, 2), sig)\n\n# Something something inverse of the cholesky, I forget\nZ[, 1] <- (Z[, 1] - p)/sqrt(Z[, 1]*(1-Z[, 1])/n)\nZ[, 2] <- (Z[, 2] - p)/sqrt(Z[, 2]*(1-Z[, 2])/(2*n))\n\nz1 = abs(Z[, 1])>1.96\nz2 = abs(Z[, 2])>1.96\n\nfpr <- mean(z1|z2)\n```\n:::\n\n\nand we get something like 8.5%.  But that doesn't answer *why*, that just means I did my algebra correctly.  As always, a visualization might help.  take a look at @fig-regions.  The shaded regions show the areas where the null would be rejected.  These are the areas we would make a false positive.  The dots indicate the standardized draws from the density of $\\theta$.  Remember, this distribution *is* the null distribution for our GSD -- these are draws from $\\theta$ when $H_0$ is true.  And now here is the important part...\n\n> The shaded region depends on critical values we use for each test in the sequence.  If we naively use $Z_{1-\\alpha/2}$ as the critical value for each group as in \"peeking\" conditions, then the shaded region is too big!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsims %>% \n  select(sim, K, Z) %>% \n  pivot_wider(names_from='K', values_from='Z') %>% \n  rename(Z1 = `K=1`, Z2=`K=2`) %>% \n  select(-sim) %>% \n  sample_n(1000) %>% \n  ggplot(aes(Z1, Z2))+\n  geom_point(color = 'dark gray', fill='gray', alpha = 0.5, shape=21)+\n  scale_x_continuous(limits = c(-5, 5), expand=c(0,0))+\n  scale_y_continuous(limits = c(-5, 5), expand=c(0,0))+\n  annotate(\"rect\", xmin = -5, xmax = -1.96, ymin = -5, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = 1.96, xmax = 5, ymin = -5, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = -1.96, xmax = 1.96, ymin = 1.96, ymax = 5, alpha = .5, fill=my_blue)+\n  annotate(\"rect\", xmin = -1.96, xmax = 1.96, ymin = -1.96, ymax = -5, alpha = .5, fill=my_blue)+\n  geom_hline(aes(yintercept=-1.96), linetype='dashed')+\n  geom_hline(aes(yintercept=1.96), linetype='dashed')+\n  geom_vline(aes(xintercept=-1.96), linetype='dashed')+\n  geom_vline(aes(xintercept=1.96), linetype='dashed')+\n  theme(\n    panel.grid.major = element_line(),\n    aspect.ratio = 1\n  )+\n  labs(x=expression(Z^(1)),\n       y=expression(Z^(2)))\n\nfind_region <- function(za){\n  z1 = abs(Z[, 1])>za\n  z2 = abs(Z[, 2])>za\n\n  fpr <- mean(z1|z2)\n  \n  (fpr - 0.05)^2\n}\n\nresult<-optimize(find_region, interval=c(0, 3))\n```\n\n::: {.cell-output-display}\n![Standardized draws from the density of $\\theta$.  Shaded regions indicate where the null hypothesis would be rejected under \"peeking\" conditions.  The shaded region has approximately 8.5\\% probability mass and represents the false positive rate.  We need to select a different region so that the shaded region has probability mass closer to 5\\%.](index_files/figure-gfm/fig-regions-1.png){#fig-regions}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nThat is the *why*!  When we naively just run our test each time we peek, we are defining a region in $\\theta$ space which has too much probability. Peeking is fine, you just have to be careful in defining your rejection region in $\\theta$ space. Defining a better rejection region isn't too hard, and we can do it using a numerical search. When we do so, we find that using a critical value of 2.19 results in a type one error closer to the desired 5\\%.  However, we're implicitly restricted ourselves to having the threshold be the same for each group.  That doesn't have to be the case as we will see eventually.\n\n## Conclusion\n\nWe've done algebra, and it wasn't for nothing.  It have us insight into exactly what is going on and *why* the type one error increases under peeking.  We also know that there is a way to fix it, we just need to define the shaded region a little more carefully.  This will lead us to talk about alpha spending and various alpha spending functions.\n\n## Appendix\n\n### Covariance Calculation {#sec-cov}\n\nThe diagonals of the covariance matrix $\\Sigma$ are simply the variances of the marginal distributions.\n\n\n$$ \\Sigma_{1, 1} = \\dfrac{p(1-p)}{n_1} $$\n\n$$ \\Sigma_{2, 2} = \\dfrac{p(1-p)}{n_1 + n_2} $$\n\n\nWhat remains is the covariance, which can be obtained with some covariance rules\n\n\n$$ \\begin{align} \\operatorname{Cov}\\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right) &= \\operatorname{Cov}\\left(\\bar{X}_1, \\dfrac{n_1\\bar{X}_1 + n_2\\bar{X}_2}{n_1 + n_2}\\right)\\\\\n&=\\dfrac{n_1}{n_1 + n_2}\\operatorname{Var}(\\bar{X_1}) + \\dfrac{n_2}{n_1+n_2}\\operatorname{Cov}(\\bar{X}_1, \\bar{X}_2)\n\\end{align}$$\n\nSince the groups are independent, the sample means are also independent (but the cumlative means are not).  Meaning $\\operatorname{Cov}(\\bar{X}_1, \\bar{X}_2)=0$ so\n\n\n$$ \\operatorname{Cov}\\left(\\bar{X}^{(1)}, \\bar{X}^{(2)}\\right) = \\dfrac{p(1-p)}{n_1 + n_2} $$\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}