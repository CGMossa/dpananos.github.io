{
  "hash": "f2bc953d1909f150986eb73560b5c060",
  "result": {
    "markdown": "---\ntitle: On Marginal Effects\ndate: \"2022-01-31\"\ncode-fold: true\necho: true\nfig-cap-location: top\ncategories: []\nnumber-sections: false\ndraft: true\n---\n\n\nConsider a function, $f: \\mathbb{R}^p \\times \\mathbb{R}^p \\to \\mathbb{R}$.  This function models the conditional expectation of some data,\n\n$$ E[y \\mid X] = f(x'\\beta) \\>.$$\n\nYou can think of this as being generalized linear model, with arbitrarily complex linear predictors (e.g. splines, interactions, whatever). For now, let's assume all the $x$ are continuous.\n\nThe marginal effects function, as you may well know, is then the partial derivative of $f$ with respect to $x$, $\\nabla_x f(x'\\beta)$.  Here, I use the subscript $_x$ to denote that we've taken partials with respect to $x$ and not $beta$.\n\nIf we average the gradient over our samples, then this is an estimate of the marginal effect of $x$.\n\nLet's see that now using `jax` and `{marginaleffects}`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(marginaleffects)\nlibrary(reticulate)\nlibrary(kableExtra)\n\n\nN <- 97\nx <- rnorm(N)\ny <- rbinom(N, 1,plogis(-0.8 + 0.45*x))\ndf <- tibble(x, y)\n\nfit <- glm(y~x, data=df, family = binomial())\n\nmarginaleffects(fit) %>% \n  summary() %>% \n  kbl() %>% \n  kable_styling(bootstrap_options = c('floating', 'striped'))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> type </th>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:left;\"> contrast </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n   <th style=\"text-align:right;\"> conf.low </th>\n   <th style=\"text-align:right;\"> conf.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> response </td>\n   <td style=\"text-align:left;\"> x </td>\n   <td style=\"text-align:left;\"> mean(dY/dX) </td>\n   <td style=\"text-align:right;\"> 0.1112267 </td>\n   <td style=\"text-align:right;\"> 0.0397812 </td>\n   <td style=\"text-align:right;\"> 2.795959 </td>\n   <td style=\"text-align:right;\"> 0.0051746 </td>\n   <td style=\"text-align:right;\"> 0.0332569 </td>\n   <td style=\"text-align:right;\"> 0.1891965 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport jax.numpy as jnp\nfrom jax import grad, vmap, jacobian\n\nfrom statsmodels.formula.api import logit\nimport patsy\n\n# Import the data from R\ndf = r.df\n\nfit = logit('y~x', data=df).fit()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.556227\n         Iterations 5\n```\n:::\n\n```{.python .cell-code}\nbeta = jnp.asarray(fit.params.values)\nX = jnp.asarray(patsy.dmatrix(fit.model.data.design_info, data=df, return_type='matrix'))\n\nexpectation_func = lambda x, b: 1.0 / (1.0 + jnp.exp(-jnp.dot(x, b)))\nmarginal_fx_func = grad(expectation_func)\n\ngradients = vmap(marginal_fx_func, (0, None), 0)(X, beta)\n\ngradients.mean(0)\n\n# Compare against some pen and paper math\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray([-0.1919493 ,  0.11122083], dtype=float32)\n```\n:::\n\n```{.python .cell-code}\np = fit.predict(df)\n\n((p*(1-p))*beta[1]).mean()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.111220814\n```\n:::\n:::\n\n\n\nAwesome, the difference in the point estimates is negligible.  So that's the point estimate, what about the standard error?\n\n## The Standard Error of a Marginal Effect\n\nAccording to the delta method\n\n$$ \\operatorname{Var}({\\nabla_xf(x'\\beta)}) = J'\\> \\Sigma \\> J $$\n\nand if we wanted to compute the variance of the avergage of marginal effects\n\n$$ \\operatorname{Var}\\left(\\dfrac{1}{n} \\sum_i {\\nabla_xf(x_i'\\beta)}\\right) = \\dfrac{1}{n^2} \\sum_i J_i'\\> \\Sigma \\> J_i $$\n\n\n::: {.cell}\n\n```{.python .cell-code}\nSigma = jnp.asarray(fit.cov_params())\nJ = jacobian(marginal_fx_func)\nJs = vmap(J, (0, None), 0)(X, beta)\n\nvar = 0\nfor j in Js:\n  var += j.T @ Sigma @ j\n  \n  \n# Standard error is too large\njnp.sqrt(jnp.diag(var)/len(df)**2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nArray([0.00262202, 0.00151927], dtype=float32)\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}