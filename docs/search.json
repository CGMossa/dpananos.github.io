[
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html",
    "href": "posts/2023-09-03-did-in-ab/index.html",
    "title": "Difference in Difference Estimates Can Be Biased When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "",
    "text": "I’ve run into people randomizing units to treatment and control and then looking to see if there are pre-treatment differences. If there are, I’ve heard – at Zapier and cross validated – that a difference in difference (DiD) should be performed. After all, there are baseline differences! We need to adjust for those.\nTo be clear – using DiD to analyze randomized experiments is fine. The resulting estimate of the ATE should be unbiased assuming the experiment was run without a hitch. You don’t need to do difference in difference because prior to treatment the two groups will have the same distribution of potential outcomes. Their pre-treatment differences are 0 in expectation. Any detection of a difference – again, assuming the experiment was run well – is sampling variability.\nRunning DiD because we found baseline differences is a form of deciding on analysis based on the observed data, and we all know that is a statistical faux pas. But how bad could it be? Are our estimates of the treatment effect biased? What do we lose when we let the data decide if we should run a DiD or a t-test?"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#simulation",
    "href": "posts/2023-09-03-did-in-ab/index.html#simulation",
    "title": "Difference in Difference Estimates Can Be Biased When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Simulation",
    "text": "Simulation\nTo find out, let’s simulate a very simple example. Let’s assume that I run an experiment and measure units before and after. The observations on each unit are uncorrelated and have standard normal distribution in the absence of the treatment. If \\(A\\) is a binary indicator for treatment (1 for treatment, 0 else) then the data are\n\\[ y_{pre} \\sim \\mbox{Normal}\\left(0, \\sigma^2\\right) \\>, \\] \\[ y_{post} \\sim \\mbox{Normal}\\left(\\tau \\cdot A, \\sigma^2 \\right) \\>. \\]\nI’ll run 20, 000 simulations of a simple randomized experiment. Each time, I’ll sample \\(N\\) units in each arm, enough to detect a treatment effect from a t-test with 80% power. I’ll then run a t-test via OLS and a DiD. I’ll record the pre-treatment difference in each group and if it was statistically significant at the 5% level. For these simulations, I’ll set \\(\\tau=1\\) and \\(\\sigma=1\\) which means I need \\(N=17\\) users per arm.\nWe’ll plot some treatment effect estimates and see what is happening when we choose to do DiD when the data suggest we do. Now importantly, I’m making very strong assumptions about the experiment being run. In particular, I’m making assumptions that all went well, there is no funny business with timing or randomization, etc. In terms of a medical trial, I got 34 people to all stand in a line, randomly gave each placebo or drug, watched them all take it at the same time, and recorded outcomes. The purpose of these simulation and blog post is to investigate statistical properties and not to wring about whatabouts.\nIn the code cell below is the code to run these simulations\n\n\nCode\nsimulate_data <- function(N_per_arm=17, tau=1, sigma=1){\n  \n  \n  A <- rbinom(2*N_per_arm, 1, 0.5)\n  y_pre <- rnorm(2*N_per_arm, 0, sigma)\n  y_post <- rnorm(2*N_per_arm, tau*A, sigma)\n\n  \n  pre <- tibble(y=y_pre, trt=A, period=0)\n  post <- tibble(y=y_post, trt=A, period=1)\n  \n  bind_rows(pre, post)\n      \n}\n\ndo_analysis <- function(i){\n  d <- simulate_data()\n  \n  #DiD \n  did <- lm(y ~ trt*period, data=d)\n  # t-test, only on post data\n  tt <- lm(y ~ trt, data=filter(d, period==1))\n  \n  tt_ate <- coef(tt)['trt']\n  did_ate <- coef(did)['trt:period']\n  \n  pre_test <- t.test(y~trt, data = filter(d, period==0))\n  \n  pre_period_diff <- diff(pre_test$estimate)\n  detected <- if_else(pre_test$p.value<0.05, 'Pre-Period Difference', 'No Pre-Period Difference')\n  \n  tibble(\n    tt_ate, \n    did_ate, \n    pre_period_diff, \n    detected\n  )\n}\n\n\nresults <- map_dfr(1:20000, do_analysis, .id = 'sim')"
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#results",
    "href": "posts/2023-09-03-did-in-ab/index.html#results",
    "title": "Difference in Difference Estimates Can Be Biased When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Results",
    "text": "Results\nShown below are the ATEs from each analysis. Nothing too surprising here, the ATEs are unbiased (the histograms are centered at \\(\\tau=1\\)). There might be some differences in variance, but I don’t care about that right now.\n\n\nCode\nplot <- results %>% \n        ggplot(aes(tt_ate, did_ate)) + \n        geom_point(alpha = 0.5) + \n        labs(\n          x= 'T-test ATE',\n          y='DiD ATE'\n        )\n\nggMarginal(plot, type='histogram')\n\n\n\n\n\nLet’s now plot the ATEs for each method against the pre-period differences. Because all observations are assumed independent, I’m going to expect that the ATEs for the t-test are uncorrelated with the pre-period difference. However, because the DiD uses pre-period information, I’m going to expect a correlation (I just don’t know how big).\n\n\nCode\nplot <- results %>% \n  pivot_longer(tt_ate:did_ate, names_to = 'analysis', values_to = 'ate') %>% \n  mutate(\n    analysis = if_else(analysis=='tt_ate', 'T-test', 'DiD')\n  ) %>% \n  ggplot(aes(pre_period_diff, ate)) + \n  geom_point(alpha=0.5) + \n  facet_grid( ~ analysis) + \n  labs(x='Pre period difference',\n       y = 'ATE')\n\n\nplot \n\n\n\n\n\nGreat, this makes sense. The ATE is for the t-test is uncorrelated with the pre-period difference, as expected. The ATE DiD is correlated with the pre-period difference, and that’s likely due to regression to the mean. Now, let’s stratify by cases when the pre-period difference is (erroneously) thought to be non-zero.\n\n\nCode\nplot + facet_grid(detected ~ analysis)\n\n\n\n\n\nIt is unsurprising that the tails of each of these blogs is cut off. After all, the pre-period difference needs to be extreme enough to reject the null. Let’s first talk about that bottom right cell – the t test when there is a detected pre-period difference. Because there is no correlation between pre-period difference and the ATE, the ATEs are still unbiased. That’s great. What about DiD?\nNote that the correlation means that those blobs don’t have the same mean. In fact, if you run K-means on those blobs, you can very easily seperate them and estimate the ATE and its very far from 1! That’s bias! How big those biases (plural, because it depends on the size of the pre-treatment difference) depends on the strength of the correlation between the ATE anf the pre-period difference. In this particular example, the left most cluster has an average ATE of about 1.8 while the right most cluster has an average ATE of about 0.24. That’s a big amount of bias in either direction."
  },
  {
    "objectID": "posts/2023-09-03-did-in-ab/index.html#conclusion",
    "href": "posts/2023-09-03-did-in-ab/index.html#conclusion",
    "title": "Difference in Difference Estimates Can Be Biased When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments",
    "section": "Conclusion",
    "text": "Conclusion\nBoth DiD and the T-test are ways to obtain unbiased estimates of the ATE for randomized experiments. DiD isn’t needed since the distribution of potential outcomes in the pre-period is the same, so prior to intervention the null hypothesis would be true. Additionally, the ATE in DiD is correlated with the pre-period difference in means. This is likely due to regression to the mean. This correlation means that when you test for pre-period differences and then choose to do DiD based on the results of that analysis, you will likely end up with a biased estimate of the ATE when the analysis tells you there is a pre-period difference.\nThe way to avoid these biases is to choose a method of analysis – DiD or t-test, like I said it doesn’t matter – prior to seeing the data. Checks for data quality are welcomed, but do not use statistics to tell you what statistics to compute. I’ve clearly shown that can go awry."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Numbers, Letters, Sometimes Both",
    "section": "",
    "text": "Difference in Difference Estimates Can Be Biased When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments\n\n\n\n\n\n\n\nAB Testing\n\n\nStatistics\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDid You Do Your Homework?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nA Practical A/B Testing Brain Dump\n\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoosing the Optimal MDE for Experimentation\n\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCausal Risk Ratios in AB Tests with One Sided Non-Compliance\n\n\n\n\n\n\n\nAB Tests\n\n\nStatistics\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJourneyman Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWay Too Many Taylor Series\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrapping in SQL\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPCA on The Tags for Cross Validated\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nInterim Analysis & Group Sequential Designs Pt 2\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nForecasting Experimental Lift Using Hierarchical Bayesian Modelling\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\nBayes\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nInterim Analysis & Group Sequential Designs Pt 1\n\n\n\n\n\n\n\nStatistics\n\n\nAB Testing\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis Is A Quarto Blog\n\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nFlippin’ Fun!\n\n\n\n\n\n\n\nBayes\n\n\nStan\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHacking Sklearn To Do The Optimism Corrected Bootstrap\n\n\n\n\n\n\n\nStatistics\n\n\nMachine Learning\n\n\nPython\n\n\nScikit-Learn\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn Interpretations of Confidence Intervals\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog Link vs. Log(y)\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nGradient Descent with ODEs\n\n\n\n\n\n\n\nPython\n\n\nMachine Learning\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 21, 2019\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeat Litle Combinatorics Problem\n\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2018\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\nCoins and Factors\n\n\n\n\n\n\n\nPython\n\n\nRiddler\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2017\n\n\nDemetri Pananos\n\n\n\n\n\n\nNo matching items"
  }
]