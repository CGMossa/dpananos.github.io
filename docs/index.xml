<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Demetri Pananos Ph.D</title>
<link>https://dpananos.github.io/index.html</link>
<atom:link href="https://dpananos.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.2.269</generator>
<lastBuildDate>Mon, 11 Sep 2023 04:00:00 GMT</lastBuildDate>
<item>
  <title>Right In Ways You Don’t Care About</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-09-11-right/index.html</link>
  <description><![CDATA[ 




<p>I’ve typically <a href="https://dpananos.github.io/posts/2023-06-26-did-you-do-your-homework/">been long on the value of data scientists</a> but I think I’m starting to lose my optimism.</p>
<p>I was fucking around in git late one night and ran into merge conflicts. The first step when you get a merge conflict is to ask yourself “jesus christ, how did I keep this job?” which reminded me on <a href="https://twitter.com/vboykis/status/1681050831850811401">this tweet</a> from Vicky Boykis. Long story short, its a screen grab from a HN comment section in which some user laments how engineering gets instant errors while things like data science see errors pass silently.</p>
<p>Tune hyperparameters on the test set? Everyone else does it.</p>
<p>Run experiments “until significance”? Have a promotion.</p>
<p>Run a pre-post? If numbers are good, yay us! It numbers are bad, did you try data dreding until they look good?</p>
<p>Data Scientists are mentioned by name in David Graeber’s <em>Bullshit Jobs</em> and it is only now that I’ve begun to work (again) that I understand why. The irony is that because errors pass silently in data work, it is often difficult to assign blame or even to determine that someone should be blamed at all. You could be terrible at your job and keep it so long as you’re terrible in the right way. It makes me wonder why I put so much effort into being right in ways people don’t seem to care about. I should get an MBA or something.</p>



 ]]></description>
  <guid>https://dpananos.github.io/posts/2023-09-11-right/index.html</guid>
  <pubDate>Mon, 11 Sep 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Regularization of Noisy Multinomial Countrs</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-09-24-l1/index.html</link>
  <description><![CDATA[ 




<p>I’m reading <a href="https://hastie.su.domains/StatLearnSparsity/"><em>Statistical Learning with Sparsity</em></a>. Its like Elements of Statistical Learning, but just for the LASSO. In chapter 3, there is this interesting example of using the LASSO to estimate rates for noisy multinomial processes. Here is the setup from the book</p>
<blockquote class="blockquote">
<p>Suppose we have a sample of <img src="https://latex.codecogs.com/png.latex?N"> counts <img src="https://latex.codecogs.com/png.latex?%5C%7B%20y_k%20%5C%7D_%7Bk-=1%7D%5EN"> from an <img src="https://latex.codecogs.com/png.latex?N">-cell multinomial distribution, and let <img src="https://latex.codecogs.com/png.latex?r_k%20=%20y_k%20/%20%5Csum_%7B%5Cell=1%7D%5EN%20y_%7B%5Cell%7D"> be the corresponding vector of proportions. For example, in large-scale web applications, these counts might represent the number of people in each country in teh YSA that visited a particular website during a given week. This vector could be sparse depending on the specifics, so there is desire to regularize toward a broader, more stable distribution <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D%20=%20%5C%7B%20u_k%20%5C%7D_%7Bk=1%7D%5EN"> (for example, the same demographic, except measured over years)</p>
</blockquote>
<p>In essence, we fit a LASSO using the <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> as an offset.The authors go on to show how minimizing a discrete version of the KL-divergence with some error tolerance between estimated and observed can be re-framed as constrained optimization of the Poisson log likelihood. In short, you can fit a glmnet model with <code>family="poisson"</code> to do the procedure described in that quote.</p>
<p>The authors kind of give a half hearted attempt at an application of such a procedure, so I tried to come up with one myself. In a <a href="https://dpananos.github.io/posts/2022-08-16-pca/">previous blog post</a>, we used looked at what kind of experts exist on statsexchange. We can use the data explorer I used for that post to look at two frequencies:</p>
<ul>
<li>Frequencies of total posts over a typical week in the year 2022, and</li>
<li>Frequencies of posts tagged “regularization” posts over a typical week in the year 2022.</li>
</ul>
<p>There are considerably more non-regularization posts than there are regularization posts, so while we should expect the frequencies to be similar there is probably some noise. Let’s use the query (below) to get our data and make a plot of those frequencies.</p>
<details>
<summary>
Click to see SQL Query
</summary>
<p>
</p><pre><code>select
cast(A.CreationDate as date) as creation_date,
count(distinct A.id) as num_posts,
count(distinct case when lower(TargetTagName) = 'regularization' then A.id end) as reg_posts
from Posts A
left join PostTags as B on A.Id = B.PostId
left join TagSynonyms as C on B.TagId = C.Id
where A.PostTypeId = 1 and A.CreationDate between '2022-01-01' and '2022-12-31'
group by cast(A.CreationDate as date)
order by cast(A.CreationDate as date)
</code></pre>
<p></p>
</details>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;">library</span>(tidymodels)</span>
<span id="cb2-3"><span class="fu" style="color: #4758AB;">library</span>(glmnet)</span>
<span id="cb2-4"><span class="fu" style="color: #4758AB;">library</span>(kableExtra)</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="fu" style="color: #4758AB;">theme_set</span>(</span>
<span id="cb2-7">  <span class="fu" style="color: #4758AB;">theme_minimal</span>(<span class="at" style="color: #657422;">base_size =</span> <span class="dv" style="color: #AD0000;">12</span>) <span class="sc" style="color: #5E5E5E;">%+replace%</span></span>
<span id="cb2-8">    <span class="fu" style="color: #4758AB;">theme</span>(</span>
<span id="cb2-9">      <span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">1.61</span>,</span>
<span id="cb2-10">      <span class="at" style="color: #657422;">legend.position =</span> <span class="st" style="color: #20794D;">'top'</span></span>
<span id="cb2-11">    )</span>
<span id="cb2-12">  )</span>
<span id="cb2-13"></span>
<span id="cb2-14"></span>
<span id="cb2-15">d <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">read_csv</span>(<span class="st" style="color: #20794D;">'QueryResults.csv'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-16">     <span class="fu" style="color: #4758AB;">mutate_at</span>(</span>
<span id="cb2-17">       <span class="at" style="color: #657422;">.vars =</span> <span class="fu" style="color: #4758AB;">vars</span>(<span class="fu" style="color: #4758AB;">contains</span>(<span class="st" style="color: #20794D;">'post'</span>)),</span>
<span id="cb2-18">       <span class="at" style="color: #657422;">.funs =</span> <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">p =</span> \(x) x<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sum</span>(x))</span>
<span id="cb2-19">     )</span>
<span id="cb2-20"></span>
<span id="cb2-21">d<span class="sc" style="color: #5E5E5E;">$</span>oday <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">factor</span>(d<span class="sc" style="color: #5E5E5E;">$</span>Day, <span class="at" style="color: #657422;">ordered=</span>T, <span class="at" style="color: #657422;">levels =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">'Sunday'</span>, <span class="st" style="color: #20794D;">'Monday'</span>, <span class="st" style="color: #20794D;">'Tuesday'</span>, <span class="st" style="color: #20794D;">'Wednesday'</span>, <span class="st" style="color: #20794D;">'Thursday'</span>, <span class="st" style="color: #20794D;">'Friday'</span>, <span class="st" style="color: #20794D;">'Saturday'</span>))</span>
<span id="cb2-22"></span>
<span id="cb2-23">d <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-24">  <span class="fu" style="color: #4758AB;">arrange</span>(oday) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-25">  <span class="fu" style="color: #4758AB;">select</span>(oday, num_posts, reg_posts) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-26">  <span class="fu" style="color: #4758AB;">kbl</span>(</span>
<span id="cb2-27">    <span class="at" style="color: #657422;">col.names =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">'Day of Week'</span>, <span class="st" style="color: #20794D;">'Total Post Count'</span>, <span class="st" style="color: #20794D;">'Regularization Post Count'</span>)</span>
<span id="cb2-28">  ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-29">  <span class="fu" style="color: #4758AB;">kable_styling</span>(<span class="at" style="color: #657422;">bootstrap_options =</span> <span class="st" style="color: #20794D;">'striped'</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:left;"> Day of Week </th>
   <th style="text-align:right;"> Total Post Count </th>
   <th style="text-align:right;"> Regularization Post Count </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Sunday </td>
   <td style="text-align:right;"> 1699 </td>
   <td style="text-align:right;"> 13 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Monday </td>
   <td style="text-align:right;"> 2494 </td>
   <td style="text-align:right;"> 37 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Tuesday </td>
   <td style="text-align:right;"> 2759 </td>
   <td style="text-align:right;"> 24 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Wednesday </td>
   <td style="text-align:right;"> 2805 </td>
   <td style="text-align:right;"> 26 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Thursday </td>
   <td style="text-align:right;"> 2689 </td>
   <td style="text-align:right;"> 32 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Friday </td>
   <td style="text-align:right;"> 2500 </td>
   <td style="text-align:right;"> 27 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Saturday </td>
   <td style="text-align:right;"> 1703 </td>
   <td style="text-align:right;"> 10 </td>
  </tr>
</tbody>
</table>

</div>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">base_plot <span class="ot" style="color: #003B4F;">&lt;-</span> d <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-2">            <span class="fu" style="color: #4758AB;">ggplot</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-3">            <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, num_posts_p, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'All Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-4">            <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, reg_posts_p, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'Regularization Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-5">            <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">labels =</span> scales<span class="sc" style="color: #5E5E5E;">::</span>percent) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-6">            <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Day'</span>, <span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Post Frequency'</span>, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">''</span>)</span>
<span id="cb3-7">            </span>
<span id="cb3-8">base_plot</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-24-l1/index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Obviously, the smaller number of posts about regularization leads to larger noise in the multinomial estimates. You can very easily see that in the plot; Monday is not special, its very likely noise.</p>
<p>So here is where we can use LASSO to reign in some of that noise. We just need to specify how discordant we want our predicted frequencies to be from the observed frequencies, and fit a LASSO model with a penalty which achieves this desired tolerance.</p>
<p>So let’s say my predictions from the LASSO are the vector <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bp%7D"> and the observed frequencies for regularization posts are <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Br%7D">. Let’s say I want the largest error between the two to be no larger than 0.05. Shown below is some code for how to do that:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># Set up the regression problem</span></span>
<span id="cb4-2">u <span class="ot" style="color: #003B4F;">&lt;-</span> d<span class="sc" style="color: #5E5E5E;">$</span>num_posts_p</span>
<span id="cb4-3">r <span class="ot" style="color: #003B4F;">&lt;-</span> d<span class="sc" style="color: #5E5E5E;">$</span>reg_posts_p</span>
<span id="cb4-4">X <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">model.matrix</span>(<span class="sc" style="color: #5E5E5E;">~</span>Day<span class="dv" style="color: #AD0000;">-1</span>, <span class="at" style="color: #657422;">data=</span>d)</span>
<span id="cb4-5"></span>
<span id="cb4-6"><span class="co" style="color: #5E5E5E;"># Grid of penalties to search over</span></span>
<span id="cb4-7">lambda.grid <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">^</span><span class="fu" style="color: #4758AB;">seq</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">8</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="fl" style="color: #AD0000;">0.005</span>)</span>
<span id="cb4-8"></span>
<span id="cb4-9"><span class="co" style="color: #5E5E5E;"># Fir the model and compute the difference between the largest error and our tolerance</span></span>
<span id="cb4-10">fit_lasso <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(x){</span>
<span id="cb4-11">  fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">glmnet</span>(X, r, <span class="at" style="color: #657422;">family=</span><span class="st" style="color: #20794D;">'poisson'</span>, <span class="at" style="color: #657422;">offset=</span><span class="fu" style="color: #4758AB;">log</span>(u), <span class="at" style="color: #657422;">lambda=</span>x)</span>
<span id="cb4-12">  p <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(fit, <span class="at" style="color: #657422;">newx=</span>X, <span class="at" style="color: #657422;">newoffset=</span><span class="fu" style="color: #4758AB;">log</span>(u), <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">'response'</span>)</span>
<span id="cb4-13">  <span class="fu" style="color: #4758AB;">abs</span>(<span class="fu" style="color: #4758AB;">max</span>(r<span class="sc" style="color: #5E5E5E;">-</span>p) <span class="sc" style="color: #5E5E5E;">-</span> <span class="fl" style="color: #AD0000;">0.03</span>)</span>
<span id="cb4-14">}</span>
<span id="cb4-15"></span>
<span id="cb4-16">errors<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">map_dbl</span>(lambda.grid, fit_lasso)</span>
<span id="cb4-17">lambda <span class="ot" style="color: #003B4F;">&lt;-</span> lambda.grid[<span class="fu" style="color: #4758AB;">which.min</span>(errors)]</span>
<span id="cb4-18">fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">glmnet</span>(X, r, <span class="at" style="color: #657422;">family=</span><span class="st" style="color: #20794D;">'poisson'</span>, <span class="at" style="color: #657422;">offset=</span><span class="fu" style="color: #4758AB;">log</span>(u), <span class="at" style="color: #657422;">lambda=</span>lambda)</span>
<span id="cb4-19">d<span class="sc" style="color: #5E5E5E;">$</span>predicted <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(fit, <span class="at" style="color: #657422;">newx=</span>X, <span class="at" style="color: #657422;">newoffset=</span><span class="fu" style="color: #4758AB;">log</span>(u), <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">'response'</span>)[, <span class="st" style="color: #20794D;">'s0'</span>]</span>
<span id="cb4-20"></span>
<span id="cb4-21">new_plot <span class="ot" style="color: #003B4F;">&lt;-</span> d <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-22">            <span class="fu" style="color: #4758AB;">ggplot</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb4-23">            <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, num_posts_p, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'All Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb4-24">            <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, reg_posts_p, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'Regularization Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb4-25">            <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, predicted, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'Predicted Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb4-26">            <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">labels =</span> scales<span class="sc" style="color: #5E5E5E;">::</span>percent) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb4-27">            <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Day'</span>, <span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Post Frequency'</span>, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">''</span>)</span>
<span id="cb4-28"></span>
<span id="cb4-29">new_plot</span></code></pre></div>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-24-l1/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>This might be easier to see if we follow Tuffte and show small multiples</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">lam <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="fl" style="color: #AD0000;">0.001</span>, <span class="fl" style="color: #AD0000;">0.01</span>, <span class="fl" style="color: #AD0000;">0.02</span>, <span class="fl" style="color: #AD0000;">0.04</span>)</span>
<span id="cb5-2"></span>
<span id="cb5-3">all_d <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">map_dfr</span>(lam, <span class="sc" style="color: #5E5E5E;">~</span>{</span>
<span id="cb5-4">  fit <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">glmnet</span>(X, r, <span class="at" style="color: #657422;">family=</span><span class="st" style="color: #20794D;">'poisson'</span>, <span class="at" style="color: #657422;">offset=</span><span class="fu" style="color: #4758AB;">log</span>(u), <span class="at" style="color: #657422;">lambda=</span>.x)</span>
<span id="cb5-5">  d<span class="sc" style="color: #5E5E5E;">$</span>predicted <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">predict</span>(fit, <span class="at" style="color: #657422;">newx=</span>X, <span class="at" style="color: #657422;">newoffset=</span><span class="fu" style="color: #4758AB;">log</span>(u), <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">'response'</span>)[, <span class="st" style="color: #20794D;">'s0'</span>]</span>
<span id="cb5-6">  d<span class="sc" style="color: #5E5E5E;">$</span>lambda <span class="ot" style="color: #003B4F;">=</span> .x</span>
<span id="cb5-7">  </span>
<span id="cb5-8">  d</span>
<span id="cb5-9">})</span>
<span id="cb5-10"></span>
<span id="cb5-11">all_d <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-12">      <span class="fu" style="color: #4758AB;">ggplot</span>() <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-13">      <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, num_posts_p, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'All Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-14">      <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, reg_posts_p, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'Regularization Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-15">      <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="fu" style="color: #4758AB;">aes</span>(oday, predicted, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">'Predicted Posts'</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-16">      <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">labels =</span> scales<span class="sc" style="color: #5E5E5E;">::</span>percent) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-17">      <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Day'</span>, <span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Post Frequency'</span>, <span class="at" style="color: #657422;">color=</span><span class="st" style="color: #20794D;">''</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-18">      <span class="fu" style="color: #4758AB;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;">~</span>lambda, <span class="at" style="color: #657422;">labeller =</span> label_both)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-24-l1/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Cool! Using <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bu%7D"> (here, the frequency of total posts on each day) acts as a sort of target towards wich to regularize.</p>



 ]]></description>
  <guid>https://dpananos.github.io/posts/2023-09-24-l1/index.html</guid>
  <pubDate>Fri, 08 Sep 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Difference in Difference Estimates Can Be Inaccurate When Adjusting For Detected Pre-Treatment Differences in Randomized Experiments</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-09-03-did-in-ab/index.html</link>
  <description><![CDATA[ 




<p>Edits:</p>
<ul>
<li><p><span class="citation" data-cites="devonbancroft1">@devonbancroft1</span> mentions that in AB testing, users are highly correlated between periods. It would be useful to examine how that correlation changes outcomes. I’ve edited the simulation to allow for drawing correlated pre and post data.</p></li>
<li><p><span class="citation" data-cites="fiftycente1">@fiftycente1</span> mentions that bias is about expectation. That’s true, I’ve edited my post to explicitly distinguish between bias and accuracy.</p></li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>I’ve run into people randomizing units to treatment and control and then looking to see if there are pre-treatment differences with a hypothesis test. If there are, I’ve heard – at Zapier and <a href="https://stats.stackexchange.com/questions/625007/ab-testing-control-was-performing-0-5-better-than-experiment-set-before-the-in/625523#625523">cross validated</a> – that a difference in difference (DiD) should be performed. After all, there are baseline differences! We need to adjust for those.</p>
<p>To be clear – using DiD to analyze randomized experiments is fine. The resulting estimate of the ATE should be unbiased assuming the experiment was run without a hitch. You don’t need to do difference in difference because prior to treatment the two groups will have the same distribution of potential outcomes. Their pre-treatment differences are 0 <em>in expectation</em>. Any detection of a difference – again, assuming the experiment was run well – is sampling variability.</p>
<p>Running DiD because we found baseline differences is a form of deciding on analysis based on the observed data, and we all know that is a statistical faux pas. But how bad could it be? Are our estimates of the treatment effect biased? Are they precise? What do we lose when we let the data decide if we should run a DiD or a t-test?</p>
</section>
<section id="simulation" class="level2">
<h2 class="anchored" data-anchor-id="simulation">Simulation</h2>
<p>To find out, let’s simulate a very simple example. Let’s assume that I run an experiment and measure units before and after. The observations on each unit are iid and have standard normal distribution in the absence of the treatment. If <img src="https://latex.codecogs.com/png.latex?A"> is a binary indicator for treatment (1 for treatment, 0 else) then the data are</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y_%7Bpre%7D%20%5Csim%20%5Cmbox%7BNormal%7D%5Cleft(0,%20%5Csigma%5E2%5Cright)%20%5C%3E,%20"> <img src="https://latex.codecogs.com/png.latex?%20y_%7Bpost%7D%20%5Csim%20%5Cmbox%7BNormal%7D%5Cleft(%5Ctau%20%5Ccdot%20A,%20%5Csigma%5E2%20%5Cright)%20%5C%3E.%20"></p>
<p>We will allow for the pre and post observations to be correlated with correlation <img src="https://latex.codecogs.com/png.latex?%5Crho">.</p>
<p>I’ll run 5, 000 simulations of a simple randomized experiment. Each time, I’ll sample <img src="https://latex.codecogs.com/png.latex?N"> units in each arm, enough to detect a treatment effect from a t-test with 80% power. I’ll then run a t-test via OLS and a DiD. I’ll record the pre-treatment difference in each group and if it was statistically significant at the 5% level. I’m also going to carry around the estimates from an ANCOVA since this seems to be the recommended approach for when we have pre-treatment information that is correlated with the outcome. For these simulations, I’ll set <img src="https://latex.codecogs.com/png.latex?%5Ctau=1"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma=1"> which means I need <img src="https://latex.codecogs.com/png.latex?N=17"> users per arm to achieve 80% power.</p>
<p>We’ll plot some treatment effect estimates and see what is happening when we choose to do DiD when the data suggest we do. Now importantly, I’m making very strong assumptions about the experiment being run. In particular, I’m making assumptions that all went well, there is no funny business with timing or randomization, etc. In terms of a medical trial, I got 34 people to all stand in a line, randomly gave each placebo or drug, watched them all take it at the same time, and recorded outcomes. The purpose of these simulation and blog post is to investigate statistical properties and not to wring about whatabouts.</p>
<p>In the code cell below is the code to run these simulations</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">simulate_data <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(<span class="at" style="color: #657422;">N_per_arm=</span><span class="dv" style="color: #AD0000;">17</span>, <span class="at" style="color: #657422;">tau=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">sigma=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">rho=</span><span class="dv" style="color: #AD0000;">0</span>, ...){</span>
<span id="cb1-2">  </span>
<span id="cb1-3">  </span>
<span id="cb1-4">  Sigma <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">diag</span>(<span class="fu" style="color: #4758AB;">rep</span>(sigma, <span class="dv" style="color: #AD0000;">2</span>)) <span class="sc" style="color: #5E5E5E;">%*%</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, rho, rho, <span class="dv" style="color: #AD0000;">1</span>), <span class="at" style="color: #657422;">nrow=</span><span class="dv" style="color: #AD0000;">2</span>) <span class="sc" style="color: #5E5E5E;">%*%</span> <span class="fu" style="color: #4758AB;">diag</span>(<span class="fu" style="color: #4758AB;">rep</span>(sigma, <span class="dv" style="color: #AD0000;">2</span>))</span>
<span id="cb1-5">  </span>
<span id="cb1-6">  A <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rep</span>(<span class="dv" style="color: #AD0000;">0</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">1</span>, N_per_arm)</span>
<span id="cb1-7">  Y <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">cbind</span>(<span class="dv" style="color: #AD0000;">0</span>, tau<span class="sc" style="color: #5E5E5E;">*</span>A) <span class="sc" style="color: #5E5E5E;">+</span> MASS<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">mvrnorm</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>N_per_arm, <span class="at" style="color: #657422;">mu =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">0</span>), <span class="at" style="color: #657422;">Sigma =</span> Sigma)</span>
<span id="cb1-8">  </span>
<span id="cb1-9">  y_pre <span class="ot" style="color: #003B4F;">&lt;-</span> Y[, <span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb1-10">  y_post <span class="ot" style="color: #003B4F;">&lt;-</span> Y[, <span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb1-11">  </span>
<span id="cb1-12">  pre <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">i =</span> <span class="fu" style="color: #4758AB;">seq_along</span>(A), <span class="at" style="color: #657422;">y=</span>y_pre, <span class="at" style="color: #657422;">trt=</span>A, <span class="at" style="color: #657422;">period=</span><span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb1-13">  post <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">i =</span> <span class="fu" style="color: #4758AB;">seq_along</span>(A), <span class="at" style="color: #657422;">y=</span>y_post, <span class="at" style="color: #657422;">trt=</span>A, <span class="at" style="color: #657422;">period=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-14">  </span>
<span id="cb1-15">  <span class="fu" style="color: #4758AB;">bind_rows</span>(pre, post)</span>
<span id="cb1-16">      </span>
<span id="cb1-17">}</span>
<span id="cb1-18"></span>
<span id="cb1-19">do_analysis <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(...){</span>
<span id="cb1-20">  </span>
<span id="cb1-21">  d <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">simulate_data</span>(...)</span>
<span id="cb1-22">  </span>
<span id="cb1-23">  dwide <span class="ot" style="color: #003B4F;">&lt;-</span> d <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-24">           <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">period =</span> <span class="fu" style="color: #4758AB;">if_else</span>(period<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>, <span class="st" style="color: #20794D;">'post'</span>,<span class="st" style="color: #20794D;">'pre'</span>))  <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-25">          <span class="fu" style="color: #4758AB;">pivot_wider</span>(<span class="at" style="color: #657422;">id_cols =</span> <span class="fu" style="color: #4758AB;">c</span>(i, trt), <span class="at" style="color: #657422;">names_from =</span> period, <span class="at" style="color: #657422;">values_from =</span> y)</span>
<span id="cb1-26">  </span>
<span id="cb1-27">  <span class="co" style="color: #5E5E5E;">#DiD </span></span>
<span id="cb1-28">  did <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> trt<span class="sc" style="color: #5E5E5E;">*</span>period, <span class="at" style="color: #657422;">data=</span>d)</span>
<span id="cb1-29">  <span class="co" style="color: #5E5E5E;"># t-test, only on post data</span></span>
<span id="cb1-30">  tt <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> trt, <span class="at" style="color: #657422;">data=</span><span class="fu" style="color: #4758AB;">filter</span>(d, period<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>))</span>
<span id="cb1-31">  <span class="do" style="color: #5E5E5E;
font-style: italic;">## Ancova</span></span>
<span id="cb1-32">  ancova <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">lm</span>(post <span class="sc" style="color: #5E5E5E;">~</span> pre <span class="sc" style="color: #5E5E5E;">+</span> trt, <span class="at" style="color: #657422;">data=</span>dwide)</span>
<span id="cb1-33">  </span>
<span id="cb1-34">  tt_ate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">coef</span>(tt)[<span class="st" style="color: #20794D;">'trt'</span>]</span>
<span id="cb1-35">  did_ate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">coef</span>(did)[<span class="st" style="color: #20794D;">'trt:period'</span>]</span>
<span id="cb1-36">  ancova_ate <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">coef</span>(ancova)[<span class="st" style="color: #20794D;">'trt'</span>]</span>
<span id="cb1-37">  </span>
<span id="cb1-38">  pre_test <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">t.test</span>(y<span class="sc" style="color: #5E5E5E;">~</span>trt, <span class="at" style="color: #657422;">data =</span> <span class="fu" style="color: #4758AB;">filter</span>(d, period<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb1-39">  </span>
<span id="cb1-40">  pre_period_diff <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">diff</span>(pre_test<span class="sc" style="color: #5E5E5E;">$</span>estimate)</span>
<span id="cb1-41">  detected <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">if_else</span>(pre_test<span class="sc" style="color: #5E5E5E;">$</span>p.value<span class="sc" style="color: #5E5E5E;">&lt;</span><span class="fl" style="color: #AD0000;">0.05</span>, <span class="st" style="color: #20794D;">'Pre-Period Difference'</span>, <span class="st" style="color: #20794D;">'No Pre-Period Difference'</span>)</span>
<span id="cb1-42">  </span>
<span id="cb1-43">  <span class="fu" style="color: #4758AB;">tibble</span>(</span>
<span id="cb1-44">    tt_ate, </span>
<span id="cb1-45">    did_ate, </span>
<span id="cb1-46">    ancova_ate,</span>
<span id="cb1-47">    pre_period_diff, </span>
<span id="cb1-48">    detected</span>
<span id="cb1-49">  )</span>
<span id="cb1-50">}</span>
<span id="cb1-51"></span>
<span id="cb1-52"></span>
<span id="cb1-53">results <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">map_dfr</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>, <span class="sc" style="color: #5E5E5E;">~</span><span class="fu" style="color: #4758AB;">do_analysis</span>(), <span class="at" style="color: #657422;">.id =</span> <span class="st" style="color: #20794D;">'sim'</span>)</span></code></pre></div>
</details>
</div>
</section>
<section id="case-1-uncorrelated-pre-and-post-observations" class="level2">
<h2 class="anchored" data-anchor-id="case-1-uncorrelated-pre-and-post-observations">Case 1: Uncorrelated Pre and Post Observations</h2>
<p>Shown below are the ATEs from each analysis when the pre and post are uncorrelated. Nothing too surprising here, the ATEs are unbiased (the histograms are centered at <img src="https://latex.codecogs.com/png.latex?%5Ctau=1">). It seems that the t-test has lower sampling variance/higher precision, which means lower MSE.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">plot <span class="ot" style="color: #003B4F;">&lt;-</span> results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-2">        <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(tt_ate, did_ate)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb2-3">        <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb2-4">        <span class="fu" style="color: #4758AB;">labs</span>(</span>
<span id="cb2-5">          <span class="at" style="color: #657422;">x=</span> <span class="st" style="color: #20794D;">'T-test ATE'</span>,</span>
<span id="cb2-6">          <span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'DiD ATE'</span></span>
<span id="cb2-7">        ) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb2-8">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="fu" style="color: #4758AB;">ggMarginal</span>(plot, <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">'histogram'</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s now plot the ATEs for each method against the pre-period differences. Because all observations are assumed independent, I’m going to expect that the ATEs for the t-test are uncorrelated with the pre-period difference. However, because the DiD uses pre-period information, I’m going to expect a correlation (I just don’t know how big).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">plot <span class="ot" style="color: #003B4F;">&lt;-</span> results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-2">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(tt_ate<span class="sc" style="color: #5E5E5E;">:</span>ancova_ate, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">'analysis'</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">'ate'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-3">  <span class="fu" style="color: #4758AB;">mutate</span>(</span>
<span id="cb3-4">    <span class="at" style="color: #657422;">analysis =</span> <span class="fu" style="color: #4758AB;">case_when</span>(</span>
<span id="cb3-5">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'tt_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'T-test'</span>,</span>
<span id="cb3-6">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'did_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'DiD'</span>,</span>
<span id="cb3-7">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'ancova_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'Ancova'</span></span>
<span id="cb3-8">    )</span>
<span id="cb3-9">  ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-10">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(pre_period_diff, ate)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-11">  <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="at" style="color: #657422;">alpha=</span><span class="fl" style="color: #AD0000;">0.5</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-12">  <span class="fu" style="color: #4758AB;">facet_grid</span>( <span class="sc" style="color: #5E5E5E;">~</span> analysis) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-13">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Pre period difference'</span>,</span>
<span id="cb3-14">       <span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">'ATE'</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-15">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb3-16"></span>
<span id="cb3-17"></span>
<span id="cb3-18">plot </span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Great, this makes sense. The ATE is for the t-test is uncorrelated with the pre-period difference, as expected. The ATE DiD is correlated with the pre-period difference, and that’s likely due to regression to the mean. Now, let’s stratify by cases when the pre-period difference is (erroneously) thought to be non-zero.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">plot <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">facet_grid</span>(detected <span class="sc" style="color: #5E5E5E;">~</span> analysis)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>It isn’t unsurprising that the tails of each of these blobs is cut off. After all, the pre-period difference needs to be extreme enough to reject the null. Let’s first talk about that bottom right cell – the t test when there is a detected pre-period difference. Because there is no correlation between pre-period difference and the ATE, the ATEs are still unbiased. That’s great. What about DiD (bottom middle cell)?</p>
<p>Note that the correlation means that those blobs don’t have the same mean. In fact, if you run K-means on those blobs, you can very easily separate them and estimate the ATE and its very far from 1! Technically, this is unbiased because we have to average over each blob. So it isn’t that the estimates are biased from DiD, but they are <em>inaccurate</em>. Take a look at the MSE for each estimate (shown below). Using DiD when you detect a pre-treatment difference may be unbiased, but it has very high MSE as compared to the two other methods.</p>
<p>While those estimates are unbiased, the low precision/high MSE is a good argument against checking for pre-treatment differences and then deciding on analysis style. This would be like saying “the river is a foot deep” when in reality the river is an inch deep in most places, and a mile deep in others. While the estimates are unbiased, any one particular estimate is fairly far from the true ATE.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-2">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(tt_ate<span class="sc" style="color: #5E5E5E;">:</span>ancova_ate, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">'analysis'</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">'ate'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-3">  <span class="fu" style="color: #4758AB;">mutate</span>(</span>
<span id="cb5-4">    <span class="at" style="color: #657422;">analysis =</span> <span class="fu" style="color: #4758AB;">case_when</span>(</span>
<span id="cb5-5">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'tt_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'T-test'</span>,</span>
<span id="cb5-6">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'did_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'DiD'</span>,</span>
<span id="cb5-7">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'ancova_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'Ancova'</span></span>
<span id="cb5-8">    )</span>
<span id="cb5-9">  ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-10">  <span class="fu" style="color: #4758AB;">group_by</span>(analysis, detected) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-11">  <span class="fu" style="color: #4758AB;">summarise</span>(</span>
<span id="cb5-12">    <span class="at" style="color: #657422;">MSE =</span> <span class="fu" style="color: #4758AB;">mean</span>((ate<span class="dv" style="color: #AD0000;">-1</span>)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb5-13">  ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb5-14">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(analysis, MSE, <span class="at" style="color: #657422;">fill=</span>detected)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-15">  <span class="fu" style="color: #4758AB;">geom_col</span>(<span class="at" style="color: #657422;">position =</span> <span class="fu" style="color: #4758AB;">position_dodge2</span>()) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-16">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">legend.position =</span> <span class="st" style="color: #20794D;">'top'</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb5-17">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Analysis'</span>, <span class="at" style="color: #657422;">fill=</span><span class="st" style="color: #20794D;">''</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="case-2-moderate-correlation-between-pre-and-post" class="level2">
<h2 class="anchored" data-anchor-id="case-2-moderate-correlation-between-pre-and-post">Case 2: Moderate Correlation Between Pre and Post</h2>
<p>Let’s now let pre and post correlation be non-zero, but still moderate in size. We’ll let <img src="https://latex.codecogs.com/png.latex?%5Crho%20=%200.45"> which is still much smaller than most correlation coefficients I’ve seen in real AB tests at Zapier.</p>
<p>Making the same plot as we did before, we see that DiD still suffers from poor accuracy as measured by MSE, bow now the T-test is starting to demonstrate some positive correlation between pre-period difference and ATE. Its worth mentioning again that we would not really do anything different with the information that there was a pre-period difference when using the t-test. The estimate of the ATE would still be unbiased and have low MSE.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">results <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">map_dfr</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>, <span class="sc" style="color: #5E5E5E;">~</span><span class="fu" style="color: #4758AB;">do_analysis</span>(<span class="at" style="color: #657422;">N_per_arm=</span><span class="dv" style="color: #AD0000;">17</span>, <span class="at" style="color: #657422;">tau=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">sigma=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">rho=</span><span class="fl" style="color: #AD0000;">0.45</span>), <span class="at" style="color: #657422;">.id =</span> <span class="st" style="color: #20794D;">'sim'</span>)</span>
<span id="cb6-2"></span>
<span id="cb6-3">results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-4">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(tt_ate<span class="sc" style="color: #5E5E5E;">:</span>ancova_ate, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">'analysis'</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">'ate'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">mutate</span>(</span>
<span id="cb6-6">    <span class="at" style="color: #657422;">analysis =</span> <span class="fu" style="color: #4758AB;">case_when</span>(</span>
<span id="cb6-7">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'tt_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'T-test'</span>,</span>
<span id="cb6-8">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'did_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'DiD'</span>,</span>
<span id="cb6-9">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'ancova_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'Ancova'</span></span>
<span id="cb6-10">    )</span>
<span id="cb6-11">  ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-12">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(pre_period_diff, ate)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-13">  <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="at" style="color: #657422;">alpha=</span><span class="fl" style="color: #AD0000;">0.5</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">'gray 45'</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-14">  <span class="fu" style="color: #4758AB;">facet_grid</span>( <span class="sc" style="color: #5E5E5E;">~</span> analysis) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-15">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Pre period difference'</span>,</span>
<span id="cb6-16">       <span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">'ATE'</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-17">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb6-18">  <span class="fu" style="color: #4758AB;">facet_grid</span>(detected <span class="sc" style="color: #5E5E5E;">~</span> analysis)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="case-3-strong-correlation-between-pre-and-post" class="level2">
<h2 class="anchored" data-anchor-id="case-3-strong-correlation-between-pre-and-post">Case 3: Strong Correlation Between Pre and Post</h2>
<p>In AB testing, it isn’t uncommon to see <img src="https://latex.codecogs.com/png.latex?%5Crho%3E0.9"> between pre and post outcomes. When we run experiments and seek to have an effect on existing user’s monthly payments, many users won’t change at all meaning they are typically paying the same amount this month as they were last. That results in very high <img src="https://latex.codecogs.com/png.latex?%5Crho">.</p>
<p>Let’s rerun the simulation, but this time specify that pre and post have a correlation of 0.95.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1">results <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">map_dfr</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">5000</span>, <span class="sc" style="color: #5E5E5E;">~</span><span class="fu" style="color: #4758AB;">do_analysis</span>(<span class="at" style="color: #657422;">N_per_arm=</span><span class="dv" style="color: #AD0000;">17</span>, <span class="at" style="color: #657422;">tau=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">sigma=</span><span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">rho=</span><span class="fl" style="color: #AD0000;">0.95</span>), <span class="at" style="color: #657422;">.id =</span> <span class="st" style="color: #20794D;">'sim'</span>)</span>
<span id="cb7-2"></span>
<span id="cb7-3"></span>
<span id="cb7-4">myblue <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rgb</span>(<span class="dv" style="color: #AD0000;">48</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">255</span>, <span class="dv" style="color: #AD0000;">61</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">255</span>, <span class="dv" style="color: #AD0000;">78</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">255</span>, <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb7-5"></span>
<span id="cb7-6">plot <span class="ot" style="color: #003B4F;">&lt;-</span> results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb7-7">        <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(tt_ate, did_ate)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb7-8">        <span class="fu" style="color: #4758AB;">geom_point</span>( <span class="at" style="color: #657422;">color=</span>myblue) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb7-9">        <span class="fu" style="color: #4758AB;">labs</span>(</span>
<span id="cb7-10">          <span class="at" style="color: #657422;">x=</span> <span class="st" style="color: #20794D;">'T-test ATE'</span>,</span>
<span id="cb7-11">          <span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'DiD ATE'</span></span>
<span id="cb7-12">        ) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb7-13">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb7-14"></span>
<span id="cb7-15"><span class="fu" style="color: #4758AB;">ggMarginal</span>(plot, <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">'histogram'</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>I’m fairly surprised to see that the results almost completely switch. First, the DiD estimator becomes much more efficient (see the joint distribution of ATEs) which is cool. Second, now the estimates from DiD become unbiased <em>and</em> accurate which is a nice change. Ancova seems to do just as well as DiD in terms of precision and accuracy, which was not expected simply because I didn’t think DiD would work this well under these assumptions.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1">results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb8-2">  <span class="fu" style="color: #4758AB;">pivot_longer</span>(tt_ate<span class="sc" style="color: #5E5E5E;">:</span>ancova_ate, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">'analysis'</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">'ate'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb8-3">  <span class="fu" style="color: #4758AB;">mutate</span>(</span>
<span id="cb8-4">    <span class="at" style="color: #657422;">analysis =</span> <span class="fu" style="color: #4758AB;">case_when</span>(</span>
<span id="cb8-5">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'tt_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'T-test'</span>,</span>
<span id="cb8-6">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'did_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'DiD'</span>,</span>
<span id="cb8-7">      analysis <span class="sc" style="color: #5E5E5E;">==</span> <span class="st" style="color: #20794D;">'ancova_ate'</span> <span class="sc" style="color: #5E5E5E;">~</span> <span class="st" style="color: #20794D;">'Ancova'</span></span>
<span id="cb8-8">    )</span>
<span id="cb8-9">  ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb8-10">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(pre_period_diff, ate)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb8-11">  <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="at" style="color: #657422;">color=</span>myblue) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb8-12">  <span class="fu" style="color: #4758AB;">facet_grid</span>( <span class="sc" style="color: #5E5E5E;">~</span> analysis) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb8-13">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'Pre period difference'</span>,</span>
<span id="cb8-14">       <span class="at" style="color: #657422;">y =</span> <span class="st" style="color: #20794D;">'ATE'</span>) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb8-15">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb8-16">  <span class="fu" style="color: #4758AB;">facet_grid</span>(detected <span class="sc" style="color: #5E5E5E;">~</span> analysis)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-09-03-did-in-ab/index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Randomization creates two artificial groups who have the same distribution of potential outcomes. There is no need to correct for baseline differences in randomized groups a la DiD, and there is certainly no need to check for baseline differences after randomizing groups.</p>
<p>However, deciding to adjust for baseline differences with DiD can result in unbiased but inaccurate estimates of the treatment effect under some circumstances. From this post, those conditions seemed to be when pre and post treatment outcomes were uncorrelated. That can happen when the outcomes are very noisy or when outcomes are homogeneous across units. While unbiased, any one particular estimate from this procedure was very far away from the true ATE, as evidenced by the larger MSE.</p>
<p>However, when outcomes are correlated between pre and post (like they can be in AB tests), then the story flips. DiD becomes very efficient, nearly as efficient as Ancova, with good accuracy and unbiased estimates.</p>
<p>Generally it seems then that using DiD for a randomized experiment is fine. In cases where the outcomes are highly correlated, you could even check for baseline differences with out much harm. But it isn’t needed, and there are circumstances where deciding to use DiD because of detected baseline differences can hurt your estimates.</p>
<p>If you want unbiasedness and low variance/high precision, the results from this post (and advice elsewhere) seems to be to stick with Ancova.</p>


</section>

 ]]></description>
  <category>AB Testing</category>
  <category>Statistics</category>
  <category>Causal Inference</category>
  <guid>https://dpananos.github.io/posts/2023-09-03-did-in-ab/index.html</guid>
  <pubDate>Sun, 03 Sep 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Did You Do Your Homework?</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-06-26-did-you-do-your-homework/index.html</link>
  <description><![CDATA[ 




<p><a href="https://twitter.com/quantian1/status/1673358773178585091">This tweet</a> made its way to my corner of the Bird Site. I’m a glutton for math homework punishment so I attempted it (and failed) but do not confuse my interest as legitimizing the tweet.</p>
<p>Frankly, I think asking questions found in Wooldridge is not a good litmus test of anything aside for your memory from undergrad. But that isn’t the point of this post. The point is to reply to <a href="https://ryxcommar.com/2023/06/26/should-you-ask-data-science-job-candidates-this-tricky-math-question/">Senior Powerpoint Engineer’s (SPE for short here, or should I call them @ryxcommar) blog post</a> which came as a response to the aforementioned tweet.</p>
<p>If I can summarize the post (which you should read), SPE makes the following points:</p>
<ul>
<li>The question should not be if you should ask this tricky math question to candidates, but rather you should ask yourself if you really need to hire a data scientist or some other role like a software engineer, analytics engineer, data analyst, ML engineer, etc.</li>
</ul>
<!-- -->
<ul>
<li><p>The term “data scientist” signals some sort of advanced knowledge in applied math/stats. Many bootcamps go about teaching modelling material as if that is most of the job (it isn’t) and even then most data scientists don’t actually understand one of the most foundational models well enough to even begin answering a homework question correctly. If they aren’t toiling and they don’t understand the theory, what are they really doing?</p></li>
<li><p>Additional anecdotes about how simple heuristics beat some data scientist’s logistic regression.</p></li>
</ul>
<p>The post is paired nicely with his <a href="https://twitter.com/ryxcommar/status/1672265739527417865">thread on why 2023 is a bad year to bootcamp your way to a data science job</a>, which hits on a some of the same points.</p>
<p>In brief, I agree with SPE and we’ve all heard it before. Data science is suffering from being a bullshit job, but clearly quantitative thinking is useful in business. What should be taught if not XGBoost API’s and linear algebra theory? What are we really looking for?</p>
<section id="value-proposition" class="level2">
<h2 class="anchored" data-anchor-id="value-proposition">Value Proposition</h2>
<p>My main contention here is the value proposition of a data scientist. If my summary is a fair, then SPE’s point is that data scientists are not in the trenches working with the data, instead they “twiddle with models in Jupyter”. But even then they don’t understand those models deeply enough (as evidenced by their inability to do homework questions). If they aren’t working with data and they can’t do science, what are they doing? What is their value? You probably don’t know, so think if you need a data scientist or some other position.</p>
<p>That feels reductive. Those can’t be the only two measures of a good data scientist, but granted that we need to answer “what do you do here”. I can only speak to my (limited) experience in tech, but many of the problems facing data scientists are not purely tech or math problems, so it doesn’t make sense to value data scientists on their ability to solve just those kinds of problems. In my experience, the majority of problems data scientists face are people problems.</p>
<p>Along the lines of people problems, the value proposition of a data scientist– in my humble opinion– is to get people to think about their problems scientifically. That is a lot harder than it sounds when the people you work with have not spent 4-10 years ostensibly<sup>1</sup> doing science. But it is also more valuable than just writing dbt models and dashboarding.</p>
<p>Here is an example from my work about getting people to think scientifically. Data analysts routinely made dashboards for the A/B tests we ran prior to my ownership of experimentation protocols. Many of the top of funnel metrics we measured were measured in terms of one another (e.g.&nbsp;Signup Rate = Signups per Visitor, Activation Rate = Activations per Signup, which is the ratio of Activation per Visitor and Signups per Visitor.) etc. Fine from a business perspective.</p>
<p>When it comes to inference, I don’t need to tell you that if you measure those two metrics as I’ve written them then anything that increases signup rate is going to decrease activation rate, so its going to look like everything is a stalemate. But I did have to share that with most everyone I talked to about experimentation, data analysts included. And it wasn’t just one person or team doing it, it was nearly the entire company. Measuring the right thing (and most importantly, knowing how to explain that to people because it isn’t as easy as mentioning conditioning on a post treatment variable) is like 20% math problem and 80% people problem due to all the stakeholder management that has to go into explanation why we are now changing the metrics, and how the numbers don’t match, no we can’t keep doing it like we’ve been doing it, what the source of truth is now, and on and on and on. It seems cheesy for me to say “look how my scientific thinking has changed the company for the better” but I think this is the type of work we should be looking for and training data scientists for. This is the answer to “what do you do here”, and what data scientists should be doing instead of screwing around with ChatGPT.</p>
<p>I think SPE would call this a “job perk” – thought leadership for internal decision making. That assumes you’re in a company that thinks scientifically and you’re free to do higher level stuff. We should all be so lucky. Thought leadership isn’t a perk, <strong>it is the job. You’re leading people to think scientifically</strong>. This is the disconnect I think SPE mentions. If you buy my rant, the title data scientist can imply an advanced understanding of applied math and not be expected to answer homework questions because remembering tricks from econometrics class isn’t used regularly in thinking scientifically. Candidates and interviewers aren’t being measured for/measuring the right thing (the ability to think scientifically and convince/teach others to follow suit) and here we are talking about if remembering <img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7B%5Cbeta%7D_1%5D%20=%20%5Cfrac%7B%5Coperatorname%7BCov%7D(X,%20Y)%7D%7B%5Coperatorname%7BVar%7D(X)%7D"> is a good barometer for being a data scientist or not.</p>
</section>
<section id="we-agree" class="level2">
<h2 class="anchored" data-anchor-id="we-agree">We Agree</h2>
<p>None of this undermines SPE’s points, it isn’t like bootcamps are teaching how to think scientifically. But it isn’t news that data science is over hyped, nor is it news that fresh grads aren’t well equipped for the job, nor is it news that correct linear algebra problem sets are not a good indicator for success on the job. I can’t rebut something we all agree is true, but I can ask for solutions, next steps, jeez anything than another “zero interest rate phenomenon” joke.</p>
<p>If I could amend SPE’s blog post, it would be as follows:</p>
<blockquote class="blockquote">
<p>But I don’t think “should you ask it?” is the right question. The real question here is: “Should you be hiring a data scientist? <strong>If so, are you willing to listen to what they have to say? Are you capable of recognizing scientific ability and probing to further test it?</strong>” And the answer to that is probably not. – (<em>Emphasis mine</em>).</p>
</blockquote>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>I say “ostensibly” because I’m not sure what I did was “science” as much as it was survival.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://dpananos.github.io/posts/2023-06-26-did-you-do-your-homework/index.html</guid>
  <pubDate>Mon, 26 Jun 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>A Practical A/B Testing Brain Dump</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-05-14-ab-brain-dump/index.html</link>
  <description><![CDATA[ 




<section id="what-is-this-post" class="level2">
<h2 class="anchored" data-anchor-id="what-is-this-post">What Is This Post?</h2>
<p>Over the past year or so, I’ve helped move Zapier towards doing better experiments and more of them. I wanted to document some of this for posterity; what I did, what worked, how we know it worked, and where I might want to go in the future. This is more of a story rather than a set of instructions.</p>
<p>My hope is that there is some sort of generalizable knowledge here. I don’t want to turn this into a set of instructions, but I do think there is something to learn (even if this has been written about before. I mean, I couldn’t find anything I needed, so here I am making it). As such, its going to be a <strong>long post</strong>. I might periodically update this post from time to time so that I can just put something out there initially.</p>
</section>
<section id="what-this-post-is-not" class="level2">
<h2 class="anchored" data-anchor-id="what-this-post-is-not">What This Post Is Not</h2>
<p>This is not a list of statistical tests to run (spoiler: its all regression anyway). Nor is this a technical post about feature flagging, persisting experiences across sessions, caching, or any other technical aspects of running online experiments.</p>
</section>
<section id="what-am-i-going-to-cover" class="level2">
<h2 class="anchored" data-anchor-id="what-am-i-going-to-cover">What Am I Going To Cover</h2>
<p>Roughly, I want to go through the steps I took over the last year. Much of it was off the cuff and improvisation, but a lot of it was inspired by working with clinicians and contents of the book <strong>Trustworthy Online Controlled Experiments</strong>. Topics will include:</p>
<ul>
<li>Understanding where we are and where we came from</li>
<li>Determining the quickest way to improve the situation</li>
<li>Building credibility and trust</li>
<li>Scaling experimentation – even if it means buying rather than building</li>
<li>(Counter intuitive) signs we are moving in the right direction</li>
<li>Next Steps</li>
</ul>
<p><strong>You should know:</strong> This was not a one man effort. There are lots of people to thank for any success I’ve had. Guidance from leadership, all the people who let me touch their projects, and all the data scientists who tapped me for help or a consult.</p>
<hr>
</section>
<section id="empathy-understanding-where-we-are-and-where-we-came-from" class="level1">
<h1>Empathy: Understanding Where We Are and Where We Came From</h1>
<p>The first thing I did when I arrived at Zapier was shut up and listen. It would have been very easy for me to come in, see a few experiments, and then insist we do everything as Bayesians and use exotic methods to estimate quantities that didn’t actually matter but sounded important. There is a folk theorem I like that goes like</p>
<blockquote class="blockquote">
<p>“If you have a problem, someone smarter than you has already solved it”.</p>
</blockquote>
<p>A small corollary to that theorem is</p>
<blockquote class="blockquote">
<p>“If you have a possible solution, someone has already tried it and failed.”</p>
</blockquote>
<p>That failure is likely not because they just did it wrong. Don’t recreate stuff that doesn’t work. I set up a “listening tour” (basically a bunch of 1:1s) with anyone who was interested in talking to me or had something to say about experimentation. This was the single best thing I could have done. I learned a lot – which I will get to – but before that I want to talk about how to run these.</p>
<p>Your main job here is to empathize and be curious about what you’re hearing. Don’t solution, don’t look for a nail you can hit with your statistical hammer, just listen. I politely asked everyone if I could record the sessions so that I could come back and listen more intently later. This removed the burden of taking notes during the meeting, I could do that after. Instead, we were able to have a real connection. If you find yourself in the same position as I found myself, do the following: Shut up, listen, and empathize.</p>
<p>What did I learn? A few things:</p>
<ul>
<li><p>There was a culture of experimentation. People were running experiments and generating insights. However, the way experiments were being run was heterogeneous. There was no global process for ideating, hypothesizing, designing, implementing, analyzing, and reporting on an experiment. People were running experiments differently between teams, and in some cases were running them differently <em>within teams</em>. If the process for running experiments differs person to person, there is no consistency. That needs to be solved.</p></li>
<li><p>Experimentation was seen as a roadblock for a lot of people. It took time, engineering effort, and would often not result in a clear winner which was frustrating. Not only that, but it took data some time to actually analyze these experiments. Data was considered a bottleneck because we would have to make a JIRA ticket to analyze the data, and then once we did analyze it stakeholders just had more questions so we’d have to repeat the process.</p></li>
<li><p>Additionally, there was just no guidance on how to run experiments. I remember one conversation so viscerally because a stakeholder told me “If I wanted to run an experiment, I wouldn’t know where to start”. We needed some basic information on why we experiment, how we experiment, and even <em>when</em> to experiment. When we couldn’t run an experiment, we needed some guidance on what other forms of learning we could use and when those were appropriate.</p></li>
</ul>
<p>There were a few other pain points, but these were the biggest three in my opinion.</p>
<p>I don’t have much advice on empathizing except the following: You’re not going to solve someone’s problems with math. Do not go into these conversations looking for evidence that your sexy model is going to work. Instead, be curious, collect data, and try to see if problems you are hearing exist elsewhere. There is a whole art to this that I will not pretend to have mastered. In any case, <strong>do not go in thinking more math will save the day. It won’t</strong>.</p>
<p>After these interviews, I consolidated all my learning and presented them to other data scientists. The one thing I wish I added to my research was <em>why previous attempts to solve these problems failed</em>. There were at least two attempts to improve experimentation at Zapier and it would have been useful to get a more robust understanding of what they attempted to solve and how.</p>
<hr>
</section>
<section id="determining-the-quickest-way-to-improve-the-situation" class="level1">
<h1>Determining The Quickest Way To Improve The Situation</h1>
<p>At this point, I had a pretty good understanding of what people thought the problems with experimentation were. It was time for me to get some skin in the game and analyze a few experiments myself. After getting a few experiment reps in, the first thing I sought to introduce to improve the situation was an A/B Testing Brief Template.</p>
<p>My hope here was that this would be an evergreen place to learn about experiments being run. It includes all the information I would need to know about to help run an experiment. This includes, but is not limited to:</p>
<ul>
<li>Phase of development (are we planning, are we developing, are we running, have we finished, etc)</li>
<li>Experiment start and end dates</li>
<li>Variant names and links to Figma</li>
<li>Hypotheses, metrics, and assumptions, etc</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption">Here is the template brief I created. Its simple, and has a lot more than what is shown here, including some steps to estimate sample size.</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2023-05-14-ab-brain-dump/https:/cdn.zappy.app/a6022ad19406dfa7531506824ad4b395.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>It was a really simple document that tried to extract the most information with the least amount of pain. To my surprise, <em>people loved it</em>. I quickly saw teams adopt this template, and would even encourage other teams to use it too. This was the quickest way to improve the situation. It gave a small global process to think about what was important when planning an experiment. This helped pave the way to introducing more global processes and an eventual repository of experimentation information (more on that later maybe).</p>
<p>Of course, nothing is perfect. I was hoping teams would be able to self serve sample size requests with just a few simple steps. Some of these questions are easy to answer (like baseline performance) but others created a lot of questions. What <em>should</em> our MDE be? what <em>should</em> our significance level be? This section is rarely filled out by stakeholders and left to data scientists, which is too bad because it really should be something a stakeholder can do. I’ll talk about how this was solved later.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption">Sample size instructions</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2023-05-14-ab-brain-dump/https:/cdn.zappy.app/e3e465a29c32f1b889d0691b12686782.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Additionally, the quality and depth of these briefs varied team to team. Some teams were very verbose, offering links and tons of details. Others would kind of slap something in there, maybe a few bullet points at the most. It isn’t my job to say what is good and what is bad, but certainly more info made it feel like the hypotheses were better researched and supported. No word yet on if more complete briefs yielded more successful experiments.</p>
<section id="an-important-side-about-frequentism-vs-bayesianism" class="level3">
<h3 class="anchored" data-anchor-id="an-important-side-about-frequentism-vs-bayesianism">An Important Side About Frequentism vs Bayesianism</h3>
<p>You may have noticed the mention of sample size and significance level, implying a Frequentist and NHST framework being adopted here. I’m well aware of the problems with this framework, so why did I allow us to keep using this approach?</p>
<ul>
<li><p>First, keep in mind that 80% of A/B testing is not about what model you use. It is more about the stuff that comes prior to hitting <code>Run</code> on your R or python script. We needed to work on that 80%, and letting people use Frequentist models at least let them be familliar with the harder parts of AB testing. Keeping that familiarity and not introducing exotic models (again, math solution to a non-math problem) helped me build trust with those data scientists instead of having me be perceived as some egg-head completely detached from business needs. I’m not saying this would be the case for everyone, but I made the decision to leave some things the same as we fixed the more important pieces. P values were one of those things.</p></li>
<li><p>Not only that, but you need to remember that you are working with people on various different stages in their journey. You had some people like <a href="https://twitter.com/statwonk">Statwonk</a>, who really knows his stuff, and you had others who hadn’t taken stats in 10 years and didn’t understand that all statistical tests are forms of regression. Bayesian stats is hard, and once we were to move past Beta-Binomial models for conversion, I would have to hold many people’s hands as they fit and interpreted their Bayesian model. That doesn’t scale very well.</p></li>
<li><p>Not only that, but I’m no longer interested in teaching statistics to people. There are plenty of resources online that are leagues better than what I could put together, so why waste my breath? If you wanna learn Bayesian stats then go read McElreath or Gelman or Kruschke or whomever. Its just not something I’m interested in doing ever again.</p></li>
</ul>
<p>That being said, there is no reason to stay Frequentist. If we feel there is a real advantage to being Bayesian, we can and have used Bayesian models. These are especially useful in revenue experiments where we statistical significance is not our concern and we should instead be using decision theory. So let’s do what we know for now and re-evaluate when all the supporting processes are working as we intend them too. Don’t be so infelxible as to think “Bayes or bust”, that isn’t helpful and lacks empathy.</p>
</section>
</section>
<section id="an-experimentation-hub" class="level1">
<h1>An Experimentation Hub</h1>
<p>The AB testing Brief was a really good move. It saw quick and wide spread adoption and helped Zapier begin to think about planning experiments in the same way by making everyone answer the same questions. This helped a lot with the variance problem.</p>
<p>There was, however, a scalability problem. While I was very keen to consult on as many A/B tests as I could, I very quickly learned that I was not a scalable solution; I could not realistically consult on experiment ideation, design, quality control, monitoring, analysis, and reporting for every experiment we would run. As soon as I got up to about 3 or 4 experiments simultaneously, I started to get really bogged down. Not only that, but I became a victim of my own success. Teams would have a good experience with an experiment I helped run and then come back to me, which lead to another good experience, and soon enough I became their go to person for experimentation. My goal was not to give myself too much work, my goal was to improve experimentation across the org and this workflow was actively working against me.</p>
<p>I’ve learned that as you become more senior, your job is less writing code and more writing documentation<sup>1</sup>. I began writing documentation on experimentation with the help of my VP. Much of the content was mine, but the idea was his and he helped me identify where and what to write about. I didn’t try to re-write textbooks, and neither should you if you’re thinking of doing the same. Instead, my goal was to capture the most salient information so that people would be able to self serve as much as possible, and when I was required to consult I could do the most important work only I could do and then dip. Examples include:</p>
<ul>
<li>Here are the criteria you need in order to experiment. Here is the decision tree for if you should experiment. It isn’t perfect and that is OK.</li>
<li>Sample size? Nope, here is a calculator and what each of the fields mean. Let me know when you’ve got a number and I will check it.</li>
<li>Need to run a test of some sort? Here is the R function. Don’t know R? Here is the python function. Don’t code all that much? Here is a link to an online calculator.</li>
<li>Here are suggested checks you should do on your experiment data.</li>
</ul>
<p>You get the jist. This worked…OK. What I found is that this “just in time” education is extremely prone to edge cases and that means I just created different problems to solve initially (which was good, that is progress).</p>
<p><sup>2</sup>: For more on this, I would recommend reading “The Staff Engineer’s Path”.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>1↩︎</p></li>
<li id="fn2"><p>1↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>AB Tests</category>
  <category>Statistics</category>
  <guid>https://dpananos.github.io/posts/2023-05-14-ab-brain-dump/index.html</guid>
  <pubDate>Sun, 14 May 2023 04:00:00 GMT</pubDate>
  <media:content url="https://cdn.zappy.app/a6022ad19406dfa7531506824ad4b395.png" medium="image" type="image/png"/>
</item>
<item>
  <title>Choosing the Optimal MDE for Experimentation</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-03-31-optimal-mde/index.html</link>
  <description><![CDATA[ 




<div class="cell">

</div>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Planning experiments means deciding how long to run the experiment (by doing a sample size calculation). A big factor in this decision is agreeing on a minimal detectable effect or MDE. Smaller MDEs mean longer experiments and hence fewer of them in a given window of time. Larger MDEs mean shorter experiments, but they also mean there is a chance we could fail to reject smaller effects <sup>1</sup>. Clearly, there is a sweet spot for the MDE; not so large that we are passing over interventions which would improve a given metric, but not so small that we are wasting our time collecting samples.</p>
<p>This blog post is intended to demonstrate how data scientists can empirically estimate the “optimal” MDE for experimenting teams. Here, “optimal” refers to the MDE which optimizes the long run cumulative improvement to the metric (i.e.&nbsp;the MDE which is estimated to improve the metric the most in a given window of time) under some mild and some strong assumptions. I begin by assuming teams have a model of likely effect sizes for their interventions vis a vis a Bayesian model like I have done in my past post <a href="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/">Forecasting Experimental Lift Using Hierarchical Bayesian Modelling</a>.</p>
<p>I begin with an illustration of the larger idea, at a level I hope would be appropriate for a product manager. Then, I formally describe the procedure and implement it in R, making some illustrative assumptions about a hypothetical team running experiments. Lastly, I demonstrate how various assumptions can effect the estimate of the optimal MDE and estimate a linear model for the optimal MDE as a function of the assumptions.</p>
<p>Although imperfect, I believe this approach offers a superior approach to determining the MDE for experimenting teams, and also has the benefit of being re-estimable experiment over experiment thanks to the Bayesian modelling driving the approach.</p>
</section>
<section id="big-idea" class="level2">
<h2 class="anchored" data-anchor-id="big-idea">Big Idea</h2>
<p>Let’s consider how many experiments a team can run in a year. Teams usually have some upper limit for the number of experiments because they need time to tend to their other responsibilities. Let’s assume a team can run 24 experiments in a year (~2 experiments a month on average). While the team has a maximum number of experiments they can run, the number of experiments <em>they actually run</em> will depend on how the experiments are planned. For example, if the team has on average 10,000,000 unique visitors each year, and each experiment needs 1, 500, 000 users, then the team can run 6 experiments (you can’t run half an experiment so you have to round down to the nearest number). So the number of experiments that can be run for this team is the smaller of the ratio of unique visitors per year to total sample size per experiment and 24. The key insight here is that because the MDE determines the sample size per experiment (again, larger/smaller MDEs mean smaller/larger sample sizes), then the MDE implicitly determines the number of experiments we can.</p>
<p>Each change to the product has some true effect that is unknown to us. In fact, the whole reason we run an experiment is to estimate that effect. Because the MDE determines the sample size per experiment, it also determines the probability we detect the effect. Increase the MDE, and the probability we detect a given effect decreases (because the sample size for the experiment decreases). Conversely, decreasing the MDE increases the probability we detect an effect.</p>
<p>We need to choose an MDE so that we can run lots of experiments <em>and</em> have a good chance of detecting positive effects in those experiments. The problem is that we don’t know what kinds of effects our experiments are going to have, which is why we use an MDE. The MDE basically is a stand in for what we think the effect of the change is going to be <em>at its smallest</em>. If the effect of the change is bigger than the MDE, then we have a really good chance to detect the effect.</p>
<p>However, we actually can estimate what kinds of effects our changes will produce. It isn’t worth getting into, but we can estimate a distribution of likely effect sizes, meaning we can reasonably guess what kinds of effects we are going to see in future experiments. This means that we can use this distribution of plausible future effects to simulate future experiments. These simulations can then be used to determine the MDE which strikes the balance we need. A good way to determine which MDE is best is to consider the “cumulative impact” on the metric. Think of it this way; if we run lots of experiments and they all have a reasonable chance of detecting effects then the metric we’re seeking to improve is going to change in a big way. So our goal is to find the MDE which results in the largest improvement to our metric over a given window of time in which we can experiment. This MDE which results in the largest improvement is called the “optimal MDE”.</p>
<p>Let’s formalize the optimization problem using some math.</p>
</section>
<section id="mathematical-details" class="level2">
<h2 class="anchored" data-anchor-id="mathematical-details">Mathematical Details</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?N_%7Buuid%7D"> be the number of unique subjects available for experimentation in a given time frame, let <img src="https://latex.codecogs.com/png.latex?%5Cdelta%20%5Cin%20%5Cmathbb%7BR%7D_+"> be an arbitrary MDE, and let <img src="https://latex.codecogs.com/png.latex?K"> be the maximum number of experiments for a given team. Let <img src="https://latex.codecogs.com/png.latex?n_%7Bss%7D:%20%5Cmathbb%7BR%7D_+%20%5Cto%20%5Cmathbb%7BN%7D"> be a function which maps MDEs to sample sizes for experiments. The number of experiments which can be run in a given time frame is <img src="https://latex.codecogs.com/png.latex?n_%7B%5Cexp%7D(%5Cdelta)%20=%20%5Cmin%5Cleft(K,%20%5Clfloor%5Cfrac%7BN_%7Buuid%7D%7D%7Bn_%7Bss%7D(%5Cdelta)%7D%20%5Crfloor%20%5Cright)">. Here, I have included an explicit dependency of <img src="https://latex.codecogs.com/png.latex?n_%7B%5Cexp%7D"> on <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> to remind us that the MDE implicitly determines the number of experiments. I’ve made the assumption that experiments can be run back to back.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Ctheta_k"> be a latent effect from an intervention, and let <img src="https://latex.codecogs.com/png.latex?%5Cpsi(%5Ctheta_k;%20%5Cdelta)"> be the statistical power to detect an effect of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> when the experiment is designed with an MDE of <img src="https://latex.codecogs.com/png.latex?%5Cdelta">. I assume that: a) All interventions are independent of one another, and the effect of one intervention is not changed by the implementation of another, b) effects of interventions are additive (on the appropriate scale), and c) effects persist through time (there is now decay of an effect once implemented).</p>
<p>The objective function we seek to optimize is the expected cumulative improvement to the metric we are opting to experiment on. We get <img src="https://latex.codecogs.com/png.latex?n_%7B%5Cexp%7D"> draws from our population distribution of effects (because we are running that many experiments), and the expected cumulative improvement is the sum of the products of the effects and the probability we detect the effect</p>
<p><img src="https://latex.codecogs.com/png.latex?%20C(%5Cdelta)%20=%20%5Csum_%7Bk=1%7D%5E%7Bn_%7B%5Cexp%7D(%5Cdelta)%7D%20%5Ctheta_k%20%5Cpsi(%5Ctheta_k;%20%5Cdelta)%20%20"> Note here that the MDE <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> determines both the number of experiments run (<img src="https://latex.codecogs.com/png.latex?n_%7B%5Cexp%7D">) <em>and</em> the probability those experiments detect an effect. The optimal MDE is then</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdelta_%7B%5Cmbox%7Bopt%7D%7D%20=%20%5Cunderset%7B%5Cdelta%20%5Cin%20%5Cmathbb%7BR%7D_+%7D%7B%5Carg%5Cmax%7D%20%20%5CBig%5C%7B%20C(%5Cdelta)%20%5CBig%20%5C%7D"> While the <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> are latent, this quantity can still be optimized by using draws from a Bayesian model for experimental effects. See <a href="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/">this previous post of mine</a> for an example of what I mean.</p>
<p>In the next section, I demonstrate how to estimate <img src="https://latex.codecogs.com/png.latex?%5Cdelta_%7B%5Cmbox%7Bopt%7D%7D"> using simulation. To fill in some missing information (e.g.&nbsp;the maximum number of experiments run by the team), I explicitly write out some assumptions.</p>
</section>
<section id="additional-assumptions" class="level2">
<h2 class="anchored" data-anchor-id="additional-assumptions">Additional Assumptions</h2>
<p>Imagine a team who runs experiments. I make the following assumptions about the team:</p>
<ul>
<li>The team’s entire job is running experiments. Due to resourcing constraints, they can only run a finite number experiments per year. I assume the team can run 24 experiments a year (or 2 per month on average). The team can run experiments back to back.</li>
<li>The team works in a frequentist framework, and they always run 2 tailed tests because there is a chance they could hurt the product, and they would want to know that.</li>
<li>The main causal contrast is relative risk. In industry, we call this the “lift”.</li>
<li>The outcome is a binary outcome, and the baseline rate is 8%.</li>
<li>10,000,000 unique visitors to your website per year.</li>
<li>The team generates lift fairly reliably and these lifts sustain through time. There is no decay of the effect, no interaction between experiments, nor is there any seasonality. These are blatantly false, but they simplify enough for us to get traction.</li>
<li>The population level lift distribution is log normal, with parameters <img src="https://latex.codecogs.com/png.latex?%5Cmu=%5Clog(1.01)"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma=0.1"> on the log scale. This means the team increases the metric by approximately 1% on average.</li>
<li>The team is really only interested in positive effects (lift &gt; 1) so they will not implement anything with lift &lt; 1, and if the null fails to be rejected they will stick with the status quo.</li>
<li>The same MDE is used to plan all experiments.</li>
</ul>
<p>Under these assumptions, a procedure can be devised to optimize the cumulative improvement to the metric of interest.</p>
</section>
<section id="results-from-a-simulation-experiment" class="level2">
<h2 class="anchored" data-anchor-id="results-from-a-simulation-experiment">Results from a Simulation Experiment</h2>
<p>Shown in the code cell below is simulation of the process for finding the optimal MDE under the assumptions listed above. Rather than simulate every experiment (e.g.&nbsp;by drawing random numbers and performing a statistical test), we can draw a binomial random variable with probability of success equal to the statistical power of detecting the latent lift with the indicated MDE and hence sample size.</p>
<p>The optimal lift is somewhere between 5% and 6%. Explicit optimization methods could be used find the optima, but I think for the purposes of experimentation you just want to be in the right ballpark, so a plot is more than enough.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">one_sided_power <span class="ot" style="color: #003B4F;">=</span> <span class="cf" style="color: #003B4F;">function</span>(real_lift, n_per_group){</span>
<span id="cb1-2">  <span class="co" style="color: #5E5E5E;"># Only interested in the case when the estimated lift is</span></span>
<span id="cb1-3">  <span class="co" style="color: #5E5E5E;"># Greater than one, which corresponds to a one sided test.</span></span>
<span id="cb1-4">  <span class="co" style="color: #5E5E5E;"># However, you always run 2 tailed tests, so the significance level</span></span>
<span id="cb1-5">  <span class="co" style="color: #5E5E5E;"># is half of what is typically is.</span></span>
<span id="cb1-6">  <span class="fu" style="color: #4758AB;">pwr.2p.test</span>(<span class="at" style="color: #657422;">h =</span> <span class="fu" style="color: #4758AB;">ES.h</span>(real_lift<span class="sc" style="color: #5E5E5E;">*</span>baseline, baseline), </span>
<span id="cb1-7">              <span class="at" style="color: #657422;">n =</span> n_per_group,</span>
<span id="cb1-8">              <span class="at" style="color: #657422;">alternative =</span> <span class="st" style="color: #20794D;">'greater'</span>,</span>
<span id="cb1-9">              <span class="at" style="color: #657422;">sig.level =</span> <span class="fl" style="color: #AD0000;">0.025</span></span>
<span id="cb1-10">              )<span class="sc" style="color: #5E5E5E;">$</span>power</span>
<span id="cb1-11">}</span>
<span id="cb1-12"></span>
<span id="cb1-13">f <span class="ot" style="color: #003B4F;">=</span> <span class="cf" style="color: #003B4F;">function</span>(mde, <span class="at" style="color: #657422;">baseline=</span><span class="fl" style="color: #AD0000;">0.08</span>, <span class="at" style="color: #657422;">n_uuids=</span><span class="dv" style="color: #AD0000;">2500000</span>, <span class="at" style="color: #657422;">latent_lift =</span> <span class="fl" style="color: #AD0000;">1.01</span>){</span>
<span id="cb1-14">  </span>
<span id="cb1-15">  <span class="co" style="color: #5E5E5E;"># Draw lifts for experiments from this distribution</span></span>
<span id="cb1-16">  lift_dist <span class="ot" style="color: #003B4F;">&lt;-</span>\(n) <span class="fu" style="color: #4758AB;">rlnorm</span>(n, <span class="fu" style="color: #4758AB;">log</span>(latent_lift), <span class="fl" style="color: #AD0000;">0.1</span>)</span>
<span id="cb1-17">  </span>
<span id="cb1-18">  <span class="co" style="color: #5E5E5E;"># Given the MDE, here is how many users you need per group in each experiment.</span></span>
<span id="cb1-19">  n_per_group <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">ceiling</span>(<span class="fu" style="color: #4758AB;">pwr.2p.test</span>(<span class="at" style="color: #657422;">h =</span> <span class="fu" style="color: #4758AB;">ES.h</span>(mde<span class="sc" style="color: #5E5E5E;">*</span>baseline, baseline), <span class="at" style="color: #657422;">power =</span> <span class="fl" style="color: #AD0000;">0.8</span>)<span class="sc" style="color: #5E5E5E;">$</span>n)</span>
<span id="cb1-20">  </span>
<span id="cb1-21">  <span class="co" style="color: #5E5E5E;"># Here is how many experiments you could run per year</span></span>
<span id="cb1-22">  <span class="co" style="color: #5E5E5E;"># Why the factor of 2?  Because the computation above is the szie of each group.</span></span>
<span id="cb1-23">  n_experiments_per_year <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">pmin</span>(<span class="dv" style="color: #AD0000;">24</span>, <span class="fu" style="color: #4758AB;">floor</span>(n_uuids<span class="sc" style="color: #5E5E5E;">/</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>n_per_group)))</span>
<span id="cb1-24">  </span>
<span id="cb1-25">  <span class="co" style="color: #5E5E5E;"># Here is a grid of experiments.  Simulate </span></span>
<span id="cb1-26">  <span class="co" style="color: #5E5E5E;"># Running these experiments 1000 times</span></span>
<span id="cb1-27">  <span class="co" style="color: #5E5E5E;"># each experiment has n_per_group users in each group</span></span>
<span id="cb1-28">  simulations <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">crossing</span>(</span>
<span id="cb1-29">    <span class="at" style="color: #657422;">sim =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">4000</span>, </span>
<span id="cb1-30">    <span class="at" style="color: #657422;">experiment =</span> <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>n_experiments_per_year,</span>
<span id="cb1-31">    <span class="at" style="color: #657422;">n_per_group =</span> n_per_group</span>
<span id="cb1-32">  )</span>
<span id="cb1-33">  </span>
<span id="cb1-34">  simulations <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-35">    <span class="fu" style="color: #4758AB;">mutate</span>(</span>
<span id="cb1-36">      <span class="co" style="color: #5E5E5E;"># draw a real lift for each experiment from your lift distribution</span></span>
<span id="cb1-37">      <span class="at" style="color: #657422;">real_lift =</span> <span class="fu" style="color: #4758AB;">lift_dist</span>(<span class="fu" style="color: #4758AB;">n</span>()),</span>
<span id="cb1-38">      <span class="co" style="color: #5E5E5E;"># Compute the power to detect that lift given the sample size you have</span></span>
<span id="cb1-39">      <span class="at" style="color: #657422;">actual_power =</span> <span class="fu" style="color: #4758AB;">map2_dbl</span>(real_lift, n_per_group, one_sided_power),</span>
<span id="cb1-40">      <span class="co" style="color: #5E5E5E;"># Simulate detecting the lift</span></span>
<span id="cb1-41">      <span class="at" style="color: #657422;">detect =</span> <span class="fu" style="color: #4758AB;">as.logical</span>(<span class="fu" style="color: #4758AB;">rbinom</span>(<span class="fu" style="color: #4758AB;">n</span>(), <span class="dv" style="color: #AD0000;">1</span>, actual_power)),</span>
<span id="cb1-42">      <span class="co" style="color: #5E5E5E;"># Did you implement the result or not?</span></span>
<span id="cb1-43">      <span class="co" style="color: #5E5E5E;"># If you didn't, this is equivalent to a lift of 1</span></span>
<span id="cb1-44">      <span class="co" style="color: #5E5E5E;"># and won't change the product.</span></span>
<span id="cb1-45">      <span class="at" style="color: #657422;">result =</span> <span class="fu" style="color: #4758AB;">if_else</span>(detect, real_lift, <span class="dv" style="color: #AD0000;">1</span>),</span>
<span id="cb1-46">    ) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-47">    <span class="fu" style="color: #4758AB;">group_by</span>(sim) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-48">    <span class="co" style="color: #5E5E5E;">#finally, take the product, grouping among simulations.</span></span>
<span id="cb1-49">    <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">lift =</span> <span class="fu" style="color: #4758AB;">prod</span>(result)) </span>
<span id="cb1-50">  </span>
<span id="cb1-51">}</span>
<span id="cb1-52"></span>
<span id="cb1-53"></span>
<span id="cb1-54">mdes <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">mde =</span> <span class="fu" style="color: #4758AB;">seq</span>(<span class="fl" style="color: #AD0000;">1.01</span>, <span class="fl" style="color: #AD0000;">1.2</span>, <span class="fl" style="color: #AD0000;">0.01</span>)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-55">        <span class="fu" style="color: #4758AB;">mutate</span>(<span class="at" style="color: #657422;">mde_id =</span> <span class="fu" style="color: #4758AB;">as.character</span>(<span class="fu" style="color: #4758AB;">seq_along</span>(mde)))</span>
<span id="cb1-56"></span>
<span id="cb1-57">results <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">map_dfr</span>(mdes<span class="sc" style="color: #5E5E5E;">$</span>mde, f, <span class="at" style="color: #657422;">.id =</span> <span class="st" style="color: #20794D;">'mde_id'</span>)  <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-58">          <span class="fu" style="color: #4758AB;">left_join</span>(mdes)</span>
<span id="cb1-59"></span>
<span id="cb1-60"></span>
<span id="cb1-61">results <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-62"><span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(mde, lift)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-63">  <span class="fu" style="color: #4758AB;">stat_summary</span>(<span class="at" style="color: #657422;">fun.data =</span> \(x) <span class="fu" style="color: #4758AB;">mean_se</span>(x, <span class="dv" style="color: #AD0000;">2</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-64">  <span class="fu" style="color: #4758AB;">scale_x_continuous</span>(<span class="at" style="color: #657422;">labels =</span> \(x) scales<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">percent</span>(x<span class="dv" style="color: #AD0000;">-1</span>, <span class="fl" style="color: #AD0000;">0.01</span>)) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-65">  <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">labels =</span> \(x) scales<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">percent</span>(x<span class="dv" style="color: #AD0000;">-1</span>, <span class="fl" style="color: #AD0000;">0.01</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-66">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="st" style="color: #20794D;">'MDE'</span>, <span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Cumulative Improvement Over all Experiments'</span>,</span>
<span id="cb1-67">       <span class="at" style="color: #657422;">title =</span> <span class="st" style="color: #20794D;">'Swing for the Fences'</span>,</span>
<span id="cb1-68">       <span class="at" style="color: #657422;">subtitle =</span> <span class="st" style="color: #20794D;">'The optimal MDE is not the expected lift the team generates'</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-03-31-optimal-mde/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="how-do-the-various-parts-of-the-problem-change-the-objective-function" class="level2">
<h2 class="anchored" data-anchor-id="how-do-the-various-parts-of-the-problem-change-the-objective-function">How Do The Various Parts of The Problem Change The Objective Function?</h2>
<p>Changing the baseline of the metric moves the optima, with larger MDEs being considered optimal for smaller baselines.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-03-31-optimal-mde/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>As the number of unique visitors to the website increases, the optimal MDE decreases, but only slightly.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-03-31-optimal-mde/index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>As the expectation of the latent lift increases, the optima does not move but the expected cumulative improvement to the metric increases. This is unsurprising.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-03-31-optimal-mde/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The “M” in MDE is really misleading. A better term would be Smallest Effect Size of Interest, because we <em>can</em> detect smaller effects than the MDE, albeit with lower probability.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>AB Tests</category>
  <category>Statistics</category>
  <guid>https://dpananos.github.io/posts/2023-03-31-optimal-mde/index.html</guid>
  <pubDate>Fri, 31 Mar 2023 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Causal Risk Ratios in AB Tests with One Sided Non-Compliance</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2023-02-11-iv-risk-ratio/index.html</link>
  <description><![CDATA[ 




<p>Sometimes we run AB tests where engaging with the treatment (and hence being treated) is optional. Since assignment to treatment is random, we can use the randomization as an instrumental variable (assuming there is a strong correlation between the instrument and the treatment, and that there are no backdoor paths between randomization and the final outcome).</p>
<p>There are a few libraries to do estimate the LATE from an instrumental variable. However, none of them report a causal risk ratio, which is usually our choice of causal contrast for better or worse.</p>
<p>In this post, I’m putting together some much appreciated advice from <a href="https://twitter.com/guilhermejd1/status/1624196853947416576"><span class="citation" data-cites="guilhermejd1">@guilhermejd1</span></a> and <a href="https://stats.stackexchange.com/a/605042/111259">dimitry on cross validated</a> on how to estimate a causal risk ratio in a randomized experiment with one sided non-compliance. I’ll first demonstrate how to do this with a simulation where we will actually have potential outcomes with which to compare. Then, I’ll apply this approach to a real experiment I helped run at Zapier.</p>
<section id="simulation" class="level2">
<h2 class="anchored" data-anchor-id="simulation">Simulation</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?A"> be a binary indicator for assignment to treatment (1) or control (0). Let <img src="https://latex.codecogs.com/png.latex?d_j%5Ea"> be the potential outcome for engaging with the treatment under treatment <img src="https://latex.codecogs.com/png.latex?A=a"> for user <img src="https://latex.codecogs.com/png.latex?j">. Due to construction of the experiment, <img src="https://latex.codecogs.com/png.latex?d_j%5E0%20=%200%20%5Cforall%20j"> because users in control can not engage in the treatment by design. Dimitry writes</p>
<blockquote class="blockquote">
<p>“This is different than the typical experiment in labor economics, where people can take up job training somewhere else even if they are in the control group”.</p>
</blockquote>
<p>This means that we have two types of users in treatment group: <img src="https://latex.codecogs.com/png.latex?d_j%5E1%20=%201"> is a “complier” and <img src="https://latex.codecogs.com/png.latex?d_j%5E1%20=%200"> is a “never taker”. If our outcome is <img src="https://latex.codecogs.com/png.latex?y">, then LATE is the ATE for compliers and is also the ATT.</p>
<p>Let’s set up a simulation. Here are some details:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?A"> is decided by a coinflip (i.e.&nbsp;bernoulli with probability of success 0.5)</li>
<li>There is an unmeasured confounder <img src="https://latex.codecogs.com/png.latex?w">, which is also distributed in the population via a coinflip.</li>
<li>The confounder and the treatment effect the probability of engaging with the treatment (being a complier). <img src="https://latex.codecogs.com/png.latex?P%5Cleft(d_j%5E1%20=%201%20%5Cmid%20A=1,%20w%5Cright)%20=%200.4%20-%200.2w">. Because of the one sided compliance, <img src="https://latex.codecogs.com/png.latex?P(d_j%5E0=1)%20=%200">.</li>
<li>Probability of the outcome is <img src="https://latex.codecogs.com/png.latex?P%5Cleft(%20y%5Ea_j=1%20%5Cmid%20d%5E%7Ba%7D_j,%20w%20%5Cright)%20=%200.1%20+%200.5w%20+%200.1d_j%5E%7Ba%7D">. So the instrument only effects the outcome through compliance.</li>
</ul>
<p>Let’s simuilate this in R</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(kableExtra)</span>
<span id="cb1-3">my_blue <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rgb</span>(<span class="dv" style="color: #AD0000;">45</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">250</span>, <span class="dv" style="color: #AD0000;">62</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">250</span>, <span class="dv" style="color: #AD0000;">80</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">250</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-4"><span class="fu" style="color: #4758AB;">theme_set</span>(<span class="fu" style="color: #4758AB;">theme_classic</span>()) </span>
<span id="cb1-5"></span>
<span id="cb1-6"><span class="fu" style="color: #4758AB;">update_geom_defaults</span>(<span class="st" style="color: #20794D;">"point"</span>,   <span class="fu" style="color: #4758AB;">list</span>(<span class="at" style="color: #657422;">fill =</span> my_blue, <span class="at" style="color: #657422;">shape=</span><span class="dv" style="color: #AD0000;">21</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">'black'</span>))</span></code></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">set.seed</span>(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb2-2">N<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">as.integer</span>(<span class="fl" style="color: #AD0000;">1e6</span>) <span class="co" style="color: #5E5E5E;"># Lots of precision</span></span>
<span id="cb2-3">A <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(N, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb2-4">w <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(N, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="co" style="color: #5E5E5E;"># Potential outcomes</span></span>
<span id="cb2-7">d_0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb2-8">d_1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(N, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.3</span> <span class="sc" style="color: #5E5E5E;">-</span> <span class="fl" style="color: #AD0000;">0.2</span><span class="sc" style="color: #5E5E5E;">*</span>w <span class="sc" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.1</span><span class="sc" style="color: #5E5E5E;">*</span>A)</span>
<span id="cb2-9">y_0 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(N, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.1</span> <span class="sc" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.5</span><span class="sc" style="color: #5E5E5E;">*</span>w)</span>
<span id="cb2-10">y_1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(N, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.1</span> <span class="sc" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.5</span><span class="sc" style="color: #5E5E5E;">*</span>w <span class="sc" style="color: #5E5E5E;">+</span> <span class="fl" style="color: #AD0000;">0.1</span><span class="sc" style="color: #5E5E5E;">*</span>d_1)</span>
<span id="cb2-11"></span>
<span id="cb2-12"><span class="co" style="color: #5E5E5E;"># and now the observed data via switching equation</span></span>
<span id="cb2-13">y <span class="ot" style="color: #003B4F;">&lt;-</span> A<span class="sc" style="color: #5E5E5E;">*</span>y_1 <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>A)<span class="sc" style="color: #5E5E5E;">*</span>y_0</span>
<span id="cb2-14">d <span class="ot" style="color: #003B4F;">&lt;-</span> A<span class="sc" style="color: #5E5E5E;">*</span>d_1 <span class="sc" style="color: #5E5E5E;">+</span> (<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>A)<span class="sc" style="color: #5E5E5E;">*</span>d_0</span>
<span id="cb2-15">complier <span class="ot" style="color: #003B4F;">&lt;-</span> d<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span></span></code></pre></div>
</div>
<p>From our setup, <img src="https://latex.codecogs.com/png.latex?LATE%20=%200.1">. Let’s compute that from our potential outcomes and estimate it using <img src="https://latex.codecogs.com/png.latex?A"> as an instrument.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">sample_late <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(y_1[d_1<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>]) <span class="sc" style="color: #5E5E5E;">-</span> <span class="fu" style="color: #4758AB;">mean</span>(y_0[d_1<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb3-2">est_late <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">cov</span>(y, A)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">cov</span>(d, A)</span></code></pre></div>
</div>
<div class="cell">
<div class="cell-output-display">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
True LATE
</th>
<th style="text-align:right;">
Sample LATE
</th>
<th style="text-align:right;">
IV Estimate of LATE
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
0.101
</td>
<td style="text-align:right;">
0.103
</td>
</tr>
</tbody>

</table>
<p>True LATE, sample LATE, and estimated LATE. All 3 agree to within 3 decimal places and any differences are just sampling variability.</p>
</div>
</div>
</section>
<section id="estimating-the-causal-risk-ratio" class="level2">
<h2 class="anchored" data-anchor-id="estimating-the-causal-risk-ratio">Estimating The Causal Risk Ratio</h2>
<p>In order to compute the causal risk ratio we need two quantities:</p>
<ul>
<li>An estimate of <img src="https://latex.codecogs.com/png.latex?E%5By%5E1_j%20%5Cmid%20d%5E1%5D">, and</li>
<li>An estimate of <img src="https://latex.codecogs.com/png.latex?E%5By%5E0_j%20%5Cmid%20d%5E1%5D">.</li>
</ul>
<p><img src="https://latex.codecogs.com/png.latex?E%5By%5E1_j%20%5Cmid%20d%5E1%5D"> is easy to estimate; just compute the average outcome of those users in treatment who engaged with the treatment. Now because <img src="https://latex.codecogs.com/png.latex?LATE%20=%20E%5By%5E1_j%20%5Cmid%20d%5E1%5D%20-%20E%5By%5E0_j%20%5Cmid%20d%5E1%5D">, the second estimate we need is obtained from some algebra.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># Estimate from the data</span></span>
<span id="cb4-2">E_y1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(y[d<span class="sc" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb4-3">E_y0 <span class="ot" style="color: #003B4F;">&lt;-</span> E_y1 <span class="sc" style="color: #5E5E5E;">-</span> est_late <span class="co" style="color: #5E5E5E;"># use the estimate, not the truth</span></span>
<span id="cb4-4"></span>
<span id="cb4-5">E_y1<span class="sc" style="color: #5E5E5E;">/</span>E_y0</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.389267</code></pre>
</div>
</div>
<p>Let’s compare this to the true estimate of the causal risk ratio using potential outcomes</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">mean</span>(y_1[complier])<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">mean</span>(y_0[complier])</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.381259</code></pre>
</div>
</div>
<p>Which is pretty damn close.</p>
</section>
<section id="a-real-example" class="level1">
<h1>A Real Example</h1>
<p>We ran an experiment where users could optionally watch a video. The video was intended to drive some other metric <img src="https://latex.codecogs.com/png.latex?y">. Here are the results from the experiment.</p>
<div class="cell">
<div class="cell-output-display">

<div id="zvewtafbqe" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>html {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;
}

:where(#zvewtafbqe) .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

:where(#zvewtafbqe) .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

:where(#zvewtafbqe) .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

:where(#zvewtafbqe) .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

:where(#zvewtafbqe) .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

:where(#zvewtafbqe) .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

:where(#zvewtafbqe) .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

:where(#zvewtafbqe) .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

:where(#zvewtafbqe) .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

:where(#zvewtafbqe) .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

:where(#zvewtafbqe) .gt_from_md > :first-child {
  margin-top: 0;
}

:where(#zvewtafbqe) .gt_from_md > :last-child {
  margin-bottom: 0;
}

:where(#zvewtafbqe) .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

:where(#zvewtafbqe) .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#zvewtafbqe) .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

:where(#zvewtafbqe) .gt_row_group_first td {
  border-top-width: 2px;
}

:where(#zvewtafbqe) .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#zvewtafbqe) .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_first_summary_row.thick {
  border-top-width: 2px;
}

:where(#zvewtafbqe) .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#zvewtafbqe) .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

:where(#zvewtafbqe) .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-left: 4px;
  padding-right: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#zvewtafbqe) .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

:where(#zvewtafbqe) .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

:where(#zvewtafbqe) .gt_left {
  text-align: left;
}

:where(#zvewtafbqe) .gt_center {
  text-align: center;
}

:where(#zvewtafbqe) .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

:where(#zvewtafbqe) .gt_font_normal {
  font-weight: normal;
}

:where(#zvewtafbqe) .gt_font_bold {
  font-weight: bold;
}

:where(#zvewtafbqe) .gt_font_italic {
  font-style: italic;
}

:where(#zvewtafbqe) .gt_super {
  font-size: 65%;
}

:where(#zvewtafbqe) .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 75%;
  vertical-align: 0.4em;
}

:where(#zvewtafbqe) .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

:where(#zvewtafbqe) .gt_indent_1 {
  text-indent: 5px;
}

:where(#zvewtafbqe) .gt_indent_2 {
  text-indent: 10px;
}

:where(#zvewtafbqe) .gt_indent_3 {
  text-indent: 15px;
}

:where(#zvewtafbqe) .gt_indent_4 {
  text-indent: 20px;
}

:where(#zvewtafbqe) .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table">
  
  <thead class="gt_col_headings">
    <tr>
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1" scope="col" id="<strong>Characteristic</strong>"><strong>Characteristic</strong></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="<strong>Overall</strong>, N = 10,119<sup class=&quot;gt_footnote_marks&quot;>1</sup>"><strong>Overall</strong>, N = 10,119<sup class="gt_footnote_marks">1</sup></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="<strong>Complied</strong>, N = 219<sup class=&quot;gt_footnote_marks&quot;>1</sup>"><strong>Complied</strong>, N = 219<sup class="gt_footnote_marks">1</sup></th>
      <th class="gt_col_heading gt_columns_bottom_border gt_center" rowspan="1" colspan="1" scope="col" id="<strong>Never Taker</strong>, N = 9,900<sup class=&quot;gt_footnote_marks&quot;>1</sup>"><strong>Never Taker</strong>, N = 9,900<sup class="gt_footnote_marks">1</sup></th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="label" class="gt_row gt_left">Y</td>
<td headers="stat_0" class="gt_row gt_center"></td>
<td headers="stat_1" class="gt_row gt_center"></td>
<td headers="stat_2" class="gt_row gt_center"></td></tr>
    <tr><td headers="label" class="gt_row gt_left">&nbsp;&nbsp;&nbsp;&nbsp;0</td>
<td headers="stat_0" class="gt_row gt_center">5,932.0 (58.6%)</td>
<td headers="stat_1" class="gt_row gt_center">145.0 (66.2%)</td>
<td headers="stat_2" class="gt_row gt_center">5,787.0 (58.5%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">&nbsp;&nbsp;&nbsp;&nbsp;1</td>
<td headers="stat_0" class="gt_row gt_center">4,187.0 (41.4%)</td>
<td headers="stat_1" class="gt_row gt_center">74.0 (33.8%)</td>
<td headers="stat_2" class="gt_row gt_center">4,113.0 (41.5%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">Treatment</td>
<td headers="stat_0" class="gt_row gt_center"></td>
<td headers="stat_1" class="gt_row gt_center"></td>
<td headers="stat_2" class="gt_row gt_center"></td></tr>
    <tr><td headers="label" class="gt_row gt_left">&nbsp;&nbsp;&nbsp;&nbsp;Control</td>
<td headers="stat_0" class="gt_row gt_center">5,083.0 (50.2%)</td>
<td headers="stat_1" class="gt_row gt_center">0.0 (0.0%)</td>
<td headers="stat_2" class="gt_row gt_center">5,083.0 (51.3%)</td></tr>
    <tr><td headers="label" class="gt_row gt_left">&nbsp;&nbsp;&nbsp;&nbsp;Treatment</td>
<td headers="stat_0" class="gt_row gt_center">5,036.0 (49.8%)</td>
<td headers="stat_1" class="gt_row gt_center">219.0 (100.0%)</td>
<td headers="stat_2" class="gt_row gt_center">4,817.0 (48.7%)</td></tr>
  </tbody>
  
  <tfoot class="gt_footnotes">
    <tr>
      <td class="gt_footnote" colspan="4"><sup class="gt_footnote_marks">1</sup> n (%)</td>
    </tr>
  </tfoot>
</table>
</div>
</div>
</div>
<p>We can see that 74 of the 219 compliers actually had the outcome. This means we can estimate <img src="https://latex.codecogs.com/png.latex?E%5By%5E1%5Cmid%20d%5E1%5D%20%5Capprox%2074/219%20%5Capprox%2033.8%5C%25">. Using <code>ivreg</code>, the LATE is estimated to be 0.256, so the lift is estimated to be <img src="https://latex.codecogs.com/png.latex?0.338/(0.388-0.256)%20%5Capprox%202.56">. But there is still a problem.</p>
<section id="oh-shit-i-forgot-a-bound" class="level2">
<h2 class="anchored" data-anchor-id="oh-shit-i-forgot-a-bound">Oh Shit, I Forgot a Bound</h2>
<p>Note that there is no bound on the LATE because it is estimated via OLS (sure, there are realistic bounds on how big this can be, but OLS isn’t enforcing those). In particular, what if <img src="https://latex.codecogs.com/png.latex?LATE%20=%20E%5By%5E1%20%5Cmid%20d%5E1%5D">? Then the denominator of the causal risk ratio would be 0. That’s…bad.</p>
<p>More over, what if <img src="https://latex.codecogs.com/png.latex?LATE%20%5Capprox%20E%5By%5E1%20%5Cmid%20d%5E1%5D"> so that the denominator was really small? Then the causal risk ratio would basically blow up (that’s a technical term for “embiggen”).</p>
<p>The only reason I bring this up is because it happens in this example. Let’s bootstrap the estimated causal risk ratio (what we call “lift”) and look at the distribuion of bootstrap replicates.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-02-11-iv-risk-ratio/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>LMAO look at that tail! The long tail us due to the problems I’ve highlighted. In fact, we can highlight the “Oh shit zone” on a plot of the bootstraps (below in the figure below). The red line is where the tail behavior comes from; if you have a bootstrap replicate on that line, you should be saying “oh shit”.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2023-02-11-iv-risk-ratio/index_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>In fact, there are some estimates from the bootstrap which yield <img src="https://latex.codecogs.com/png.latex?E%5By%5E0%5Cmid%20d%5E1%5D%3C0"> so… what was the point if this?</p>
</section>
<section id="what-was-the-point-of-this-post" class="level2">
<h2 class="anchored" data-anchor-id="what-was-the-point-of-this-post">What Was The Point Of This Post</h2>
<p>Ok, so estimating the causal risk ratio in a randomized experiment with one sided non-compliance is technically possible, but the math can get…weird. In particular, bootstrapping the standard errors (which is probably the most sensible way of estimating the standard errors unless you’re a glutton for delta method punishment) shows that we can get non-nonsensical bootstrapped estimates of the counterfacutal average outcome for compliers.</p>
<p>Honestly…I’m not sure where to go from here. Point estimates are possible but incomplete. Bootstrapping is a sanest way to get standard errors, but have no way of ensuring the estimates are bounded appropriately. All is not lost, its nice to know this sort of thing can happen. Maybe the most sensible thing to say here is “do not ask for causal risk ratios for these types of experiments” and that is worth its weight in gold.</p>


</section>
</section>

 ]]></description>
  <category>AB Tests</category>
  <category>Statistics</category>
  <category>Causal Inference</category>
  <guid>https://dpananos.github.io/posts/2023-02-11-iv-risk-ratio/index.html</guid>
  <pubDate>Sat, 11 Feb 2023 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Journeyman Statistics</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-12-06-journeyman/index.html</link>
  <description><![CDATA[ 




<p>A number of people have asked me how to learn statistics. I don’t have a good answer for them. I find all books are deficient in some way: some too theoretical, others not theoretical enough. Some too focused on pen and paper calculation, others provide code that was very likely written decades ago and do not use modern packages or tidy principles (e.g they begin their analysis with <code>rm(list=ls()</code> or use <code>attach</code> and <code>detatch</code>). I can’t recommend my own path to learning statistics either, mostly because I did a Ph.D in statistics and had the benefit of a Masters in Applied Mathematics.</p>
<p>I want to write a book for people who are not statisticians but need to make use of statistics anyway. The medical residents who need to do logistic regression for a research project, the social science grad student (save econometricians, I’d say we could learn something from them but we probably know all the same things by different names) who is more interested in their research than their statistical models, the business intelligence analyst who has to to analyze a (poorly planned) A/B test and would love nothing than to improve the experiment the next time around.</p>
<p>This sequence of blog posts is going to be a sort of first go at that book. Not even an alpha or a rough draft, but rather somewhere to put some thoughts that might eventually make it into the book. This first post is about the motivation for the book, which I am tentatively calling “Journeyman Statistics”.</p>
<section id="what-is-journeyman-statistics" class="level2">
<h2 class="anchored" data-anchor-id="what-is-journeyman-statistics">What is Journeyman Statistics?</h2>
<p>A journeyperson/woman/man is</p>
<blockquote class="blockquote">
<p>“a worker, skilled in a given building trade or craft, who has successfully completed an official apprenticeship qualification. Journeymen are considered competent and authorized to work in that field as a fully qualified employee. They earn their license by education, supervised experience and examination. Although journeymen have completed a trade certificate and are allowed to work as employees, they may not yet work as self-employed master craftsmen.</p>
</blockquote>
<p>What do I mean then by “journeyman statistics” and who are “journeyman statisticians”? Borrowing heavily from the definition above, journeyman statisticians are people who are trained in some field and are currently doing statistics in service of someone or something else. Journeyman statistics are then the statistical analyses performed by these people. I don’t think its tough to pick our journeyman statisticians; they are to a first approximation those who perform statistical analysis but are not statisticians first and foremost. They are biologists, medical residents, sociologists, analysts of several varietys, etc. To them, statistics is <em>the means</em> whereas statistics are <em>the end</em> to research (perhaps “pure”) statisticians.</p>
<p>Its important to further distinguish journeyman statisticians from applied statisticians. Applied statisticians can, as Tukey once said, “play in everyone’s backyard”. They possess the necessary mathematical maturity and statistical expertise to move from field to field. A journeyman statistician, though they may be well versed in statistics, would likely stay in their own backyard (to continue the metaphor) in order to tackle problems there.</p>
<p>Of course, I don’t mean to place people in boxes. You don’t need to subscribe to my taxonomy of statisticians (in fact, outside of this book I don’t think its a particularly useful taxonomy), and I think there are edge cases which threaten the taxonomy as a whole. The taxonomy is simply a model, and this model is useful for one thing: understanding motivations for learning statistics, and designing a path through statistical literature so as to serve those motivations.</p>
</section>
<section id="why-do-we-need-a-book-on-journeyman-statistics" class="level2">
<h2 class="anchored" data-anchor-id="why-do-we-need-a-book-on-journeyman-statistics">Why Do We Need a Book on Journeyman Statistics?</h2>
<p>The taxonomy allows us to understand who journeyman statisticians are, what their intentions are, what they may lack in terms of statistical understanding, what is enough to satiate their desire to learn statistics, and what details contribute “noise” rather than “signal”. As an example, I don’t think biology grad students need to know what <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Bplim%7D"> means, or any of the other topics adjacent to mathematical analysis in Casella and Berger’s <em>Statistical Inference</em>. They do need to go slightly beyond their sophmore classes which tell them they can use the t-test when <img src="https://latex.codecogs.com/png.latex?N%3E30">. Likewise, medical residents need to go beyond Martin Bland’s <em>An Introduction to Medical Statistics</em> and need to be able to confidently say “we shouldn’t do that” when their supervisors or superiors insist on a clearly flawed mode of analysis. However, they may get bogged down by the integrals in Frank Harrell’s <em>Regression Modelling Strategies</em> (as well as the sea of references to methodological papers). There are few books for people like that. I find that books are typically for sophmore students learning statistics for the first time, or applied statisticians. Journeyman statisticians need something in the middle. I hope this book is that something.</p>
</section>
<section id="what-will-this-book-contain" class="level2">
<h2 class="anchored" data-anchor-id="what-will-this-book-contain">What Will This Book Contain?</h2>
<p>Statisticians like to joke “its all regression”. There is truth in that phrase, and so this book will take the perspective that regression is the primary means of estimation. We’ll cover all the typical analyses as regression methods. This includes estimation of the mean, its just a one parameter regression. I want to get to GLM’s as quickly as possible while not getting bogged down by mathematical details, like whatever “mild regularity conditions” means. GLMs are the workhorse of applied statistics, and I see no point in leaving GLMs to later chapters. One thing that will be absent from the book is p values. The book will take an estimation approach and report only confidence intervals.</p>
<p>The book will also contain code in both python and R, though I encourage readers to use R rather than python. Python’s statistical tools have a distinct econometrics flavor.</p>
</section>
<section id="what-benefit-is-there-to-reading-this-book" class="level2">
<h2 class="anchored" data-anchor-id="what-benefit-is-there-to-reading-this-book">What Benefit is There to Reading This Book?</h2>
<p>Before discussing why you should read this book, I want to discuss what I call “The Precision-Usefulness Tradeoff”, as I anticipate I will refer to this many times. In short, the tradeoff states</p>
<blockquote class="blockquote">
<p>Perfectly precise statements are completely useless. In order to become useful, the statement must be made less precise. The less precise, the more useful.</p>
</blockquote>
<p>Again, this is a model rather than a law. As an example, consider the definition for a 95% confidence interval.</p>
<blockquote class="blockquote">
<p>A 95% confidence interval is an interval estimate which, upon repeated construction under identical and ideal conditions, contains the estimand 95% of the time.</p>
</blockquote>
<p>I can make this statement more precise by writing it down mathematically</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P%20%5Cleft(%20%5Cbar%7Bx%7D%20-%20z_%7B0.975%7D%20%5Csigma/%5Csqrt%7Bn%7D%20%5Cleq%20%5Cmu%20%5Cleq%20%5Cbar%7Bx%7D%20+%20z_%7B0.975%7D%20%5Csigma/%5Csqrt%7Bn%7D%20%5Cright)%20=%200.95%20"></p>
<p>but it loses usefulness. This doesn’t really tell me what a confidence interval is, when to use one, or how to interpret one. However, the definition I initially presented perhaps permits some pathological cases. We can make the definition more useful by removing precision further</p>
<blockquote class="blockquote">
<p>A 95% confidence interval contains parameters consistent with the data.</p>
</blockquote>
<p>Now, we have a better idea how to interpret the interval, but the 95% is opaque to us.</p>
<p>The reason I bring up this trade off is because the book is intended to give journeyman statisticians the tools required to move in Precision-Usefulness space. My hope is that when the time comes, you will be able to artfully trade precision for usefulness (e.g.&nbsp;to be pedantic when it is necessary, or to break the rules precisely because you know how and when they can be broken safely). This is the primary benefit of the book. Obviously, I can’t tell people <em>when</em> or <em>where</em> to move within that space. I will have to leave that to their best judgement.</p>
<p>The second benefit is to solve the problem we began with; to answer “how to I go about learning statistics” in a satisfactory way.</p>


</section>

 ]]></description>
  <guid>https://dpananos.github.io/posts/2022-12-06-journeyman/index.html</guid>
  <pubDate>Sat, 31 Dec 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Way Too Many Taylor Series</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-11-25-taylor-series/index.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?Y(a)"> be a potential outcome for a continuous measure under treatment status <img src="https://latex.codecogs.com/png.latex?A=a">, which for the purpose of this blog post can be considered a binary treatment. When is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%20%5Cdelta%20=%20%5Cdfrac%7BE%5BY(1)%5D%7D%7BE%5BY(0)%5D%7D%20"> well approximated by <img src="https://latex.codecogs.com/png.latex?%5Cexp(%5Cdelta%5E%5Cstar)"> where</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdelta%5E%5Cstar%20=%20E%20%5Cleft%5B%20%5Cln%20%5Cleft(%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20%5Cright)%20%5Cright%5D%20=%20E%5B%5Cln(Y(1)%5D%20%20-%20E%5B%5Cln(Y(0)%5D%20%5C%3E."></p>
<p>It seems to be a <a href="https://stats.stackexchange.com/questions/163518/interpreting-test-results-on-log-transformed-data">fairly well accepted proposition</a> that the difference in means on the log scale can be interpreted as a relative change in means on the natural scale, but upon closer inspection they aren’t equivalent. Firstly, Jensen’s inequality prevents interchanging the expectation operator and the logarithm and second <img src="https://latex.codecogs.com/png.latex?E%5BX/Y%5D%20%5Cneq%20E%5BX%5D/E%5BY%5D"> since expectation is a linear operation. I think a more nuanced discussion is needed as to if and when we can interpret <img src="https://latex.codecogs.com/png.latex?%5Cexp(%5Cdelta%5E%5Cstar)"> as <img src="https://latex.codecogs.com/png.latex?%5Cdelta">.</p>
<p>To be clear, I’m sure this holds fairly readily. I don’t want to overstate the importance of this blog post, but I don’t want to understate it either. This question came up when considering how AB tests should measure changes in continuous metrics, like revenue. Log-transforming revenue to reign in tails is common advice – and I think that’s fine, especially when it makes the sampling distribution of the sample mean more normal looking. Additionally, talking about changes in a relative sense (i.e.&nbsp;“lift” in the metric) is something that comes natural to a lot of companies. But if we’re going to use <img src="https://latex.codecogs.com/png.latex?%5Cdelta%5E%5Cstar"> as the metric for our experiment, then I personally would like to understand under what conditions this is a good approximation. What assumptions am I implicitly making? I don’t think curiosity in that sense is a bad thing, or a waste of time.</p>
</section>
<section id="taylor-series-for-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="taylor-series-for-random-variables">Taylor Series for Random Variables</h2>
<p>Before getting into the meat of the blog post, it might be worthwhile to revisit Taylor series for random variables (which we will make heavy use of in this post). Recall that a Taylor series for a continuously differentiable function <img src="https://latex.codecogs.com/png.latex?f"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20f(x)%20=%20%5Csum_%7Bk=0%7D%5E%7B%5Cinfty%7D%20%5Cdfrac%7Bf%5E%7B(k)%7D(x_0)%7D%7Bk!%7D%20(x%20-%20x_0)%5Ek%20%5C%3E,%20%20%5Cquad%20%5Cmid%20x%20-%20x_0%20%5Cmid%20%5Clt%20d"></p>
<p>and that the error made in approximating <img src="https://latex.codecogs.com/png.latex?f"> with first <img src="https://latex.codecogs.com/png.latex?n+1"> terms of this sum, <img src="https://latex.codecogs.com/png.latex?R_n(x)">, can be bounded by</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cmid%20R_n(x)%20%5Cmid%20%5Cleq%20%5Cdfrac%7BM%7D%7B(n+1)!%7D(x%20-%20x_0)%5E%7Bn+1%7D%20%5C%3E,%20%5Cquad%20%5Cmid%20x%20-%20x_0%20%5Cmid%20%5Clt%20d%20%5C%3E."> We can also expand a function of a random variable, <img src="https://latex.codecogs.com/png.latex?X">, in a Taylor series by considering the variation of <img src="https://latex.codecogs.com/png.latex?X-%5Cmu"> about <img src="https://latex.codecogs.com/png.latex?%5Cmu"></p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0Af(X)%20&amp;=%20f((X-%5Cmu)%20+%20%5Cmu)%20%5C%3E,%20%5C%5C%0A%20%20%20%20%20&amp;=%20f(%5Cmu)%20+%20f%5E%7B%5Cprime%7D(%5Cmu)(X-%5Cmu)%20+%20%5Cdfrac%7Bf%5E%7B%5Cprime%5Cprime%7D(%5Cmu)%7D%7B2%7D(X-%5Cmu)%5E2%20+%20%5Cmathcal%7BO%7D%5Cleft(%20(X-%5Cmu)%5E3%20%5Cright)%20%5C%3E.%0A%5Cend%7Balign%7D">
<p>From here, we can take expectations and leverage the linearity of <img src="https://latex.codecogs.com/png.latex?E"> to get a nice second order approximation of <img src="https://latex.codecogs.com/png.latex?E(f(X))"></p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AE%5Bf(X)%5D%20&amp;%5Capprox%20%20E%5Bf(%5Cmu)%5D%20+%20f%5E%7B%5Cprime%7D(%5Cmu)E%5B(X-%5Cmu)%5D%20+%20%5Cdfrac%7Bf%5E%7B%5Cprime%5Cprime%7D(%5Cmu)%7D%7B2%7DE%5B(X-%5Cmu)%5E2%5D%20%5C%3E,%20%5C%5C%0A%20%20%20%20%20%20%20%20&amp;%5Capprox%20%20f(%5Cmu)%20+%20%5Cdfrac%7Bf%5E%7B%5Cprime%5Cprime%7D(%5Cmu)%7D%7B2%7D%5Csigma%5E2%20%5C%3E.%0A%5Cend%7Balign%7D">
</section>
<section id="applying-taylor-series-to-our-problem" class="level2">
<h2 class="anchored" data-anchor-id="applying-taylor-series-to-our-problem">Applying Taylor Series To Our Problem</h2>
<p>The quantity <img src="https://latex.codecogs.com/png.latex?%5Cexp(%5Cdelta%5E%5Cstar)"> could be approximately be <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> under the right circumstances. Let’s expand <img src="https://latex.codecogs.com/png.latex?%5Cln(Y(1)/Y(0))"> in a Taylor series centered around <img src="https://latex.codecogs.com/png.latex?Y(1)/Y(0)%20=%201">. We’re going to be doing quite a few Taylor expansions, so I’m going to color code some of them in order to keep track of which terms belong to which expansion.</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AE%5Cleft%5B%5Cln%20%5Cleft(%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20%5Cright)%5Cright%5D%20&amp;%5Capprox%20E%20%5Cleft%20%5B%20%5Ctextcolor%7B#1f77b4%7D%7B%5Cleft(%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20-1%20%5Cright)%20+%20%5Cdfrac%7B1%7D%7B2%7D%20%5Cleft(%20%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20-1%20%5Cright)%5E2%7D%20%5Cright%5D%20%5C%3E,%5C%5C%0A&amp;%5Capprox%20%5Ctextcolor%7B#ff7f0e%7D%7BE%20%5Cleft%5B%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20%5Cright%5D%7D%20%20%5Ctextcolor%7B#1f77b4%7D%7B-%201%20+%20%5Cdfrac%7B1%7D%7B2%7D%20E%20%5Cleft%5B%5Cleft(%20%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20-1%20%5Cright)%5E2%20%20%5Cright%5D%7D%20%5C%3E.%0A%5Cend%7Balign%7D">
<p>This approximation is only valid when <img src="https://latex.codecogs.com/png.latex?0%20%5Clt%20Y(1)/Y(0)%20%5Cleq%202">, and the error in this approximation is bounded by <img src="https://latex.codecogs.com/png.latex?E%5Cleft%5B%5Cleft(%5Cfrac%7BY(1)%7D%7BY(0)%7D%20-1%5Cright)%5E3%5Cright%5D">. Note that we almost have <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> in our Taylor series expansion, but not quite. We can apply yet another Taylor series expansion on the part in orange to yield</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Ctextcolor%7B#ff7f0e%7D%7BE%5Cleft%5B%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20%5Cright%5D%20%5Capprox%20%5Cdfrac%7BE%5BY(1)%5D%7D%7BE%5BY(0)%5D%7D%20-%5Cfrac%7B%5Coperatorname%7Bcov%7D%5BY(1),%20Y(0)%5D%7D%7B%5Cmathrm%7BE%7D%5BY(0)%5D%5E2%7D+%5Cfrac%7B%5Cmathrm%7BE%7D%5BY(1)%5D%7D%7B%5Cmathrm%7BE%7D%5BY(0)%5D%5E3%7D%20%5Coperatorname%7Bvar%7D%5BY(0)%5D%7D%20%5C%3E.%0A%5Cend%7Balign%7D">
<p>Let’s assemble this all together now. Our approximation is now</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AE%5Cleft%5B%5Cln%20%5Cleft(%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20%5Cright)%5Cright%5D%20%20&amp;%5Capprox%20%5Ctextcolor%7B#ff7f0e%7D%7B%20%5Cdfrac%7BE%5BY(1)%5D%7D%7BE%5BY(0)%5D%7D%20-%5Cfrac%7B%5Coperatorname%7Bcov%7D%5BY(1),%20Y(0)%5D%7D%7B%5Cmathrm%7BE%7D%5BY(0)%5D%5E2%7D+%5Cfrac%7B%5Cmathrm%7BE%7D%5BY(1)%5D%7D%7B%5Cmathrm%7BE%7D%5BY(0)%5D%5E3%7D%20%5Coperatorname%7Bvar%7D%5BY(0)%5D%7D%20%20%5Ctextcolor%7B#1f77b4%7D%7B-%201%20+%20%5Cdfrac%7B1%7D%7B2%7D%20E%20%5Cleft%5B%5Cleft(%20%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20-1%20%5Cright)%5E2%20%20%5Cright%5D%7D%20%5C%3E,%20%5C%5C%0A&amp;%5Capprox%20%5Cdfrac%7BE%5BY(1)%5D%7D%7BE%5BY(0)%5D%7D%20-%201%20%5Ctextcolor%7B#ff7f0e%7D%7B-%5Cfrac%7B%5Coperatorname%7Bcov%7D%5BY(1),%20Y(0)%5D%7D%7B%5Cmathrm%7BE%7D%5BY(0)%5D%5E2%7D+%5Cfrac%7B%5Cmathrm%7BE%7D%5BY(1)%5D%7D%7B%5Cmathrm%7BE%7D%5BY(0)%5D%5E3%7D%20%5Coperatorname%7Bvar%7D%5BY(0)%5D%7D%20%5Ctextcolor%7B#1f77b4%7D%7B%20+%20%5Cdfrac%7B1%7D%7B2%7D%20E%20%5Cleft%5B%5Cleft(%20%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20-1%20%5Cright)%5E2%20%20%5Cright%5D%7D%20%5C%3E.%0A%5Cend%7Balign%7D">
<p>Finally <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> appears in our long and tedious approximation of <img src="https://latex.codecogs.com/png.latex?%5Cdelta%5E%5Cstar">. Let’s ignore the terms colored in blue and orange for a moment and come back to them later.</p>
<p>Our approximation is now</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5Cleft%5B%5Cln%20%5Cleft(%20%5Cdfrac%7BY(1)%7D%7BY(0)%7D%20%5Cright)%5Cright%5D%20%5Capprox%20%5Cdfrac%7BE%5BY(1)%5D%7D%7BE%5BY(0)%5D%7D%20-%201"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdelta%5E%5Cstar%20%5Capprox%20%5Cdelta%20-1%20%20"></p>
<p>Exponentiating both sides</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%5Cexp(%5Cdelta%5E%5Cstar)%20&amp;%5Capprox%20%5Cexp(%5Cdelta-1)%20%5C%5C%0A&amp;%20%5Capprox%201%20+%20%5Cdelta%20-1%20+%20%20%5Cmathcal%7BO%7D((%5Cdelta-1)%5E2)%5C%5C%0A&amp;%20%5Capprox%20%5Cdelta%0A%5Cend%7Balign%7D">
</section>
<section id="we-did-itnow-what" class="level2">
<h2 class="anchored" data-anchor-id="we-did-itnow-what">We Did It…Now What?</h2>
<p>Now that we’ve written down all the necessary approximations and assumptions, let’s go back and determine under what circumstances this is a valid approximation. Can we break this approximation? Can we break it really badly?</p>
<p>Let’s leave that for the next blog post. I’m tired from doing all this algebra.</p>


</section>

 ]]></description>
  <category>Statistics</category>
  <category>AB Testing</category>
  <guid>https://dpananos.github.io/posts/2022-11-25-taylor-series/index.html</guid>
  <pubDate>Fri, 25 Nov 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>Bootstrapping in SQL</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-11-16-bootstrapping-in-sql/index.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Remember the <a href="https://en.wikipedia.org/wiki/Double_Down_(sandwich)">“Double Down”</a> from KFC? It was bacon and cheese sandwiched between two deep fried pieces of chicken. I’m willing to bet we all conceived of it independently (as in “LOL wouldn’t it be crazy if…”), realistically could have made it ourselves, but were smart enough not to because “sure we could but… why?”.</p>
<p>This blog post is the Double Down of Statistics.</p>
</section>
<section id="bootstrapping-in-sql.-no-really." class="level2">
<h2 class="anchored" data-anchor-id="bootstrapping-in-sql.-no-really.">Bootstrapping in SQL. No, Really.</h2>
<p>Two things which have made my stats like easier:</p>
<ul>
<li>Bootstrapping, and</li>
<li>Tidy data</li>
</ul>
<p>R’s <code>rsample::bootstraps</code> seems to do one in terms of the other. Take a look at the output of that function. We have, in essence, one bootstrapped dataset per row.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb1-2"><span class="fu" style="color: #4758AB;">library</span>(rsample )</span>
<span id="cb1-3"></span>
<span id="cb1-4">rsample<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">bootstraps</span>(cars)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># Bootstrap sampling 
# A tibble: 25 × 2
   splits          id         
   &lt;list&gt;          &lt;chr&gt;      
 1 &lt;split [50/18]&gt; Bootstrap01
 2 &lt;split [50/16]&gt; Bootstrap02
 3 &lt;split [50/16]&gt; Bootstrap03
 4 &lt;split [50/18]&gt; Bootstrap04
 5 &lt;split [50/21]&gt; Bootstrap05
 6 &lt;split [50/19]&gt; Bootstrap06
 7 &lt;split [50/20]&gt; Bootstrap07
 8 &lt;split [50/18]&gt; Bootstrap08
 9 &lt;split [50/20]&gt; Bootstrap09
10 &lt;split [50/19]&gt; Bootstrap10
# … with 15 more rows</code></pre>
</div>
</div>
<p>In theory, I could unnest this and have one observation from each bootstrap per row, with <code>id</code> serving as an indicator to tell me to which resample the observation belongs to. Which means…I could theoretically bootstrap in SQL.</p>
<p>So, let’s do that. I’m going to use duckdb because its SQL-like and has some stats functions (whereas SQLite does not).</p>
<p>Let’s sample some pairs <img src="https://latex.codecogs.com/png.latex?(x_i,%20y_i)"> from the relationship <img src="https://latex.codecogs.com/png.latex?y_i%20=%202x_i%20+%201%20+%20%5Cvarepsilon_i">, where the <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon"> are iid draws from a standard Gaussian Let’s stick that in a dataframe along with a row number column into our database. The data are shown in Table&nbsp;1.</p>
<div class="cell">

</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-original-data" class="anchored">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;"><caption>Table&nbsp;1:  My Data </caption>
 <thead>
  <tr>
   <th style="text-align:center;"> original_rownum </th>
   <th style="text-align:center;"> x </th>
   <th style="text-align:center;"> y </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 1 </td>
   <td style="text-align:center;"> 2.09 </td>
   <td style="text-align:center;"> 5.73 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 0.97 </td>
   <td style="text-align:center;"> 3.58 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
   <td style="text-align:center;"> 0.70 </td>
   <td style="text-align:center;"> 1.37 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
   <td style="text-align:center;"> 1.44 </td>
   <td style="text-align:center;"> 4.84 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
   <td style="text-align:center;"> 2.07 </td>
   <td style="text-align:center;"> 4.14 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 6 </td>
   <td style="text-align:center;"> 1.67 </td>
   <td style="text-align:center;"> 3.09 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</div>
<p>To bootstrap in SQL, we need to emulate what the unnested results of <code>rsample::bootstraps</code> would look like. We need rows of (<code>strap_id</code>, <code>original_data_rownum</code>, and <code>bootstrap_rownum</code>). Let’s discuss the interpretation and purpose of each column.</p>
<ul>
<li><p><code>strap_id</code> plays the part of <code>id</code> in <code>rsample::bootstraps</code>. We’re just going to group by this column and aggregate the resampled data later.</p></li>
<li><p><code>original_data_rownum</code> doesn’t really serve a purpose. It contains integers 1 through <img src="https://latex.codecogs.com/png.latex?N"> (where <img src="https://latex.codecogs.com/png.latex?N"> is our original sample size). We can do a cross join to get pairs (<code>strap_id</code>, <code>original_data_rownum</code>). This means there will be <img src="https://latex.codecogs.com/png.latex?N"> copies of <code>strap_id</code>, meaning we can get <img src="https://latex.codecogs.com/png.latex?N"> resamples of our data for each <code>strap_id</code>.</p></li>
<li><p><code>bootstrap_rownum</code> is a random integer between 1 and <img src="https://latex.codecogs.com/png.latex?N">. This column DOES serve a purpose, its basically the sampling with replacement bit for the bootstrap. Now, duckdb doesn’t have a function to sample random integers. To do this, I basically sample random numbers on the unit interval do some arithmetic to turn those into integers.</p></li>
</ul>
<p>Let’s set that up now. The hardest part really is creating a sequence of numbers, but duckdb makes that pretty easy.</p>
<section id="query-to-make-strap_id" class="level3">
<h3 class="anchored" data-anchor-id="query-to-make-strap_id">Query To Make <code>strap_id</code></h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb3-1"><span class="co" style="color: #5E5E5E;">-- Set up strap_ids in a table</span></span>
<span id="cb3-2"><span class="kw" style="color: #003B4F;">CREATE</span> <span class="kw" style="color: #003B4F;">OR</span> <span class="kw" style="color: #003B4F;">REPLACE</span> <span class="kw" style="color: #003B4F;">TABLE</span> strap_ids(strap_id <span class="dt" style="color: #AD0000;">INTEGER</span>);</span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;">-- Do 1000 bootstraps</span></span>
<span id="cb3-4"><span class="kw" style="color: #003B4F;">INSERT</span> <span class="kw" style="color: #003B4F;">INTO</span> strap_ids(strap_id) <span class="kw" style="color: #003B4F;">select</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="kw" style="color: #003B4F;">from</span> <span class="kw" style="color: #003B4F;">range</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1001</span>, <span class="dv" style="color: #AD0000;">1</span>);</span></code></pre></div>
</div>
<div class="cell" data-output.var="tbl1">

</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-strap-ids" class="anchored">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;"><caption>Table&nbsp;2:  Contents of strap_ids. These play the role of id in the rsample output. </caption>
 <thead>
  <tr>
   <th style="text-align:center;"> strap_id </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="query-to-make-original_data_rownum" class="level3">
<h3 class="anchored" data-anchor-id="query-to-make-original_data_rownum">Query To Make <code>original_data_rownum</code></h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb4-1"><span class="co" style="color: #5E5E5E;">-- Set up original_data_rownum in a table</span></span>
<span id="cb4-2"><span class="kw" style="color: #003B4F;">CREATE</span> <span class="kw" style="color: #003B4F;">OR</span> <span class="kw" style="color: #003B4F;">REPLACE</span> <span class="kw" style="color: #003B4F;">TABLE</span> original_data_rownum(original_rownum <span class="dt" style="color: #AD0000;">INTEGER</span>);</span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;">-- I have 2500 observations in my data</span></span>
<span id="cb4-4"><span class="kw" style="color: #003B4F;">INSERT</span> <span class="kw" style="color: #003B4F;">INTO</span> original_data_rownum(original_rownum) <span class="kw" style="color: #003B4F;">select</span> <span class="op" style="color: #5E5E5E;">*</span> <span class="kw" style="color: #003B4F;">from</span> <span class="kw" style="color: #003B4F;">range</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2500</span><span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>);</span></code></pre></div>
</div>
<div class="cell" data-output.var="tbl2">

</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-original-data-rownum" class="anchored">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;"><caption>Table&nbsp;3:  Contents of original_data_rownum. These play the role of id in the rsample output. </caption>
 <thead>
  <tr>
   <th style="text-align:center;"> original_rownum </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 1 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 3 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 5 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Ok, now we have the two tables <code>strap_ids</code> and <code>original_data_rownum</code>. All we need to do now is cross join then, and do the random number magic. That’s shown below in table Table&nbsp;4.</p>
</section>
<section id="query-to-make-bootstrap_rownum" class="level3">
<h3 class="anchored" data-anchor-id="query-to-make-bootstrap_rownum">Query To Make <code>bootstrap_rownum</code></h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb5-1"><span class="kw" style="color: #003B4F;">create</span> <span class="kw" style="color: #003B4F;">or</span> <span class="kw" style="color: #003B4F;">replace</span> <span class="kw" style="color: #003B4F;">table</span> resample_template <span class="kw" style="color: #003B4F;">as</span> </span>
<span id="cb5-2"><span class="kw" style="color: #003B4F;">select</span></span>
<span id="cb5-3">  strap_ids.strap_id,</span>
<span id="cb5-4">  original_data_rownum.original_rownum,</span>
<span id="cb5-5">  <span class="co" style="color: #5E5E5E;">-- I have 2500 observations in my data</span></span>
<span id="cb5-6">  <span class="fu" style="color: #4758AB;">round</span>( <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.5</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">2501</span><span class="op" style="color: #5E5E5E;">*</span><span class="kw" style="color: #003B4F;">random</span>()) <span class="kw" style="color: #003B4F;">as</span> bootstrap_rownum,</span>
<span id="cb5-7"><span class="kw" style="color: #003B4F;">from</span></span>
<span id="cb5-8">  strap_ids</span>
<span id="cb5-9"><span class="kw" style="color: #003B4F;">cross</span> <span class="kw" style="color: #003B4F;">join</span> </span>
<span id="cb5-10">  original_data_rownum;</span></code></pre></div>
</div>
<div class="cell" data-output.var="tbl3">

</div>
<div class="cell">
<div class="cell-output-display">
<div id="tbl-resample-template" class="anchored">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;"><caption>Table&nbsp;4:  A sample from the table resampel_template. </caption>
 <thead>
  <tr>
   <th style="text-align:center;"> strap_id </th>
   <th style="text-align:center;"> original_rownum </th>
   <th style="text-align:center;"> bootstrap_rownum </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> 573 </td>
   <td style="text-align:center;"> 2216 </td>
   <td style="text-align:center;"> 4 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 312 </td>
   <td style="text-align:center;"> 2227 </td>
   <td style="text-align:center;"> 1792 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 2 </td>
   <td style="text-align:center;"> 554 </td>
   <td style="text-align:center;"> 1577 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 440 </td>
   <td style="text-align:center;"> 381 </td>
   <td style="text-align:center;"> 688 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> 969 </td>
   <td style="text-align:center;"> 1352 </td>
   <td style="text-align:center;"> 1840 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
</section>
<section id="actually-doing-the-bootstrapping-its-just-a-left-join" class="level2">
<h2 class="anchored" data-anchor-id="actually-doing-the-bootstrapping-its-just-a-left-join">Actually Doing The Bootstrapping: Its Just A Left Join!</h2>
<p>Now all we have to do is join the original data onto <code>resample_template</code>. The join is going to happen <code>on original_data.original_rownum = resample_template.bootstrap_rownum</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb6-1"><span class="kw" style="color: #003B4F;">create</span> <span class="kw" style="color: #003B4F;">or</span> <span class="kw" style="color: #003B4F;">replace</span> <span class="kw" style="color: #003B4F;">table</span> resampled_data <span class="kw" style="color: #003B4F;">as</span></span>
<span id="cb6-2"><span class="kw" style="color: #003B4F;">select</span></span>
<span id="cb6-3">  resample_template.strap_id,</span>
<span id="cb6-4">  resample_template.bootstrap_rownum,</span>
<span id="cb6-5">  original_data.x,</span>
<span id="cb6-6">  original_data.y</span>
<span id="cb6-7"><span class="kw" style="color: #003B4F;">from</span> </span>
<span id="cb6-8">  resample_template</span>
<span id="cb6-9"><span class="kw" style="color: #003B4F;">left</span> <span class="kw" style="color: #003B4F;">join</span> </span>
<span id="cb6-10">  original_data <span class="kw" style="color: #003B4F;">on</span> original_data.original_rownum <span class="op" style="color: #5E5E5E;">=</span> resample_template.bootstrap_rownum;</span></code></pre></div>
</div>
<p>And congratulations, you have what is in essence an unnested <code>rsample::bootstraps</code> output. This happens shockingly fast in duckdb (actually, a bit faster than <code>rsample</code> does it, but that is anecdote I didn’t actually time them). The hard part now is the aggregation function. Obviously, you can’t do very complex statsitical aggregations in duckdb (or any other SQL dialect), but there are a few you can do. For example, let’s bootstrap the mean of <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y">, as well as the estimated regression coefficient.</p>
<div class="cell" data-output.var="bsr">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode sql code-with-copy"><code class="sourceCode sql"><span id="cb7-1"><span class="kw" style="color: #003B4F;">select</span></span>
<span id="cb7-2">  <span class="st" style="color: #20794D;">'Bootstrap'</span> <span class="op" style="color: #5E5E5E;">||</span> <span class="fu" style="color: #4758AB;">lpad</span>(strap_id,<span class="dv" style="color: #AD0000;">4</span>,<span class="dv" style="color: #AD0000;">0</span>) <span class="kw" style="color: #003B4F;">as</span> <span class="kw" style="color: #003B4F;">id</span>,</span>
<span id="cb7-3">  <span class="st" style="color: #20794D;">'SQL'</span> <span class="kw" style="color: #003B4F;">as</span> <span class="kw" style="color: #003B4F;">method</span>,</span>
<span id="cb7-4">  <span class="fu" style="color: #4758AB;">avg</span>(x) <span class="kw" style="color: #003B4F;">as</span> mean_x,</span>
<span id="cb7-5">  <span class="fu" style="color: #4758AB;">avg</span>(y) <span class="kw" style="color: #003B4F;">as</span> mean_y,</span>
<span id="cb7-6">  <span class="fu" style="color: #4758AB;">corr</span>(y, x) <span class="op" style="color: #5E5E5E;">*</span> <span class="fu" style="color: #4758AB;">stddev</span>(y) <span class="op" style="color: #5E5E5E;">/</span> <span class="fu" style="color: #4758AB;">stddev</span>(x) <span class="kw" style="color: #003B4F;">as</span> beta</span>
<span id="cb7-7"><span class="kw" style="color: #003B4F;">from</span> resampled_data</span>
<span id="cb7-8"><span class="kw" style="color: #003B4F;">group</span> <span class="kw" style="color: #003B4F;">by</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-9"><span class="kw" style="color: #003B4F;">order</span> <span class="kw" style="color: #003B4F;">by</span> <span class="dv" style="color: #AD0000;">1</span>;</span></code></pre></div>
</div>
<p>We can easily compare the distributions obtained via the SQL bootstrap with distributions obtained from <code>rsample::bootstrap</code></p>
<div class="cell">

</div>
</section>
<section id="but-does-it-work" class="level2">
<h2 class="anchored" data-anchor-id="but-does-it-work">But Does It Work</h2>
<p>Yes…I think. The averages for <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?y"> look really good, but the SQL bootstrap tails for the regression coefficient are a little thin.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-11-16-bootstrapping-in-sql/index_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="960"></p>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This is pretty silly, and probably inefficient. I’m no data engineer, I’m just a guy with a Ph.D in stats and a lot of time on the weekend. I should get a hobby or something.</p>


</section>

 ]]></description>
  <category>Statistics</category>
  <guid>https://dpananos.github.io/posts/2022-11-16-bootstrapping-in-sql/index.html</guid>
  <pubDate>Wed, 16 Nov 2022 05:00:00 GMT</pubDate>
</item>
<item>
  <title>PCA on The Tags for Cross Validated</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-08-16-pca/index.html</link>
  <description><![CDATA[ 




<p>Julia Silge gave a <a href="https://www.rstudio.com/resources/rstudioconf-2018/understanding-pca-using-shiny-and-stack-overflow-data/">really good talk</a> in 2018 about PCA ond tags on stack overflow. She was able to interpret some of the components to infer some subgroups of users of stack overflow (front-end vs back-end, are they a Microsoft tech developer or not, are you an android dev or not, etc). These principal components were able to shed some light on what drove the variation in questions asked.</p>
<p>I love this talk, and I crib it all the time. As of late, I’ve not been doing much SQL, so I figured I would recreate Julia’s analysis using data from <a href="https://stats.stackexchange.com/">cross validated</a>. But this time, with a twist!</p>
<p>What if instead of understanding the drivers of variance in questions asked, we analyze the kinds of questions users answer. This could give us insight into the type of analysts we have on cross validated. The site is intended to be for statistical analysis, but it has a mix of prediction questions, machine learning questions, econometrics questions, and much more. Hang around there long enough and you will see some familiar faces (mine included) and you get a pretty good sense of who answers what kinds of questions.</p>
<p>I’m going to use data available from the stack exchange data explorer available <a href="https://data.stackexchange.com/">here</a>. I’ve included a code box in this post with the query I’ve used. I’ve sliced out the top 250 users as ranked by reputation and the top 100 tags as calculated by prevalence. We can use <code>{tidymodels}</code> to do a lot of the heay lifting. Let’s get to it.</p>
<details>
<summary>
Click to see SQL Query
</summary>
<p>
</p><pre><code>with QA_Tags as (
select

A.Id as QuestionId,
A.Title as QuestionTitle,
A.Body as QuestionBody,

B.Id as AnswerId,
B.OwnerUserId,
B.Body as AnswerBody,

C.iD as UserId,
C.DisplayName,
dense_rank() over (order by C.Reputation desc) as rnk,

D.TagId,
E.TagName

from
--Extract Answers from Posts Table
(select * from Posts where PostTypeId = 2)  as B
--Extract Questions from Posts Table
left join (select * from Posts where PostTypeId = 1)  as A
on  A.Id = B.ParentId
left join Users as C
on B.OwnerUserId = C.Id
left join PostTags as D
on A.Id = D.PostId
left join (select top 100 * from Tags where TagName is not NULL order by Count desc) as E
on D.TagId = E.Id)

select
rnk,
UserId, 
DisplayName, 
TagName,
count(distinct QuestionId) as N
from QA_tags
where rnk&lt;=250
group by rnk, UserId, DisplayName, TagName
order by rnk</code></pre>
<p></p>
</details>
<section id="data-modelling" class="level2">
<h2 class="anchored" data-anchor-id="data-modelling">Data &amp; Modelling</h2>
<p>Let’s Take a peek at the data, using me as an example. Below are my top 10 tags as a percent of my total answers. Looks like I like to answer questions about regression, hypothesis testing, and R most frequently. Each of the top 250 users has data similar to this in my dataset. I need to pivot it so that tags become features. Then, I can normalize the data and perform PCA.</p>
<div class="cell">
<div class="cell-output-display">

<table class="table table-striped" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:left;"> Name </th>
   <th style="text-align:left;"> Tag </th>
   <th style="text-align:left;"> Percent </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Regression </td>
   <td style="text-align:left;"> 11.40% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Hypothesis-Testing </td>
   <td style="text-align:left;"> 6.01% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> R </td>
   <td style="text-align:left;"> 5.90% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Machine-Learning </td>
   <td style="text-align:left;"> 5.45% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Logistic </td>
   <td style="text-align:left;"> 4.85% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Bayesian </td>
   <td style="text-align:left;"> 3.58% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Statistical-Significance </td>
   <td style="text-align:left;"> 3.42% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Confidence-Interval </td>
   <td style="text-align:left;"> 2.75% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> T-Test </td>
   <td style="text-align:left;"> 2.64% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Probability </td>
   <td style="text-align:left;"> 2.31% </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Demetri Pananos </td>
   <td style="text-align:left;"> Generalized-Linear-Model </td>
   <td style="text-align:left;"> 2.31% </td>
  </tr>
</tbody>
</table>

</div>
</div>
<p>Checkout how dummy easy the analysis is with tidymodels. I think I spent more time cleaning the data than I did modelling it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">rec <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">recipe</span>(rnk <span class="sc" style="color: #5E5E5E;">+</span> UserId <span class="sc" style="color: #5E5E5E;">+</span> DisplayName <span class="sc" style="color: #5E5E5E;">~</span> ., <span class="at" style="color: #657422;">data =</span> d) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-2">       <span class="fu" style="color: #4758AB;">step_normalize</span>(<span class="fu" style="color: #4758AB;">all_numeric_predictors</span>()) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-3">       <span class="fu" style="color: #4758AB;">step_pca</span>(<span class="fu" style="color: #4758AB;">all_numeric_predictors</span>(), <span class="at" style="color: #657422;">num_comp =</span> <span class="dv" style="color: #AD0000;">3</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-4">       <span class="fu" style="color: #4758AB;">prep</span>()</span>
<span id="cb2-5"></span>
<span id="cb2-6">prin_comps <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">bake</span>(rec, <span class="at" style="color: #657422;">new_data =</span> d) </span>
<span id="cb2-7">weights <span class="ot" style="color: #003B4F;">&lt;-</span> rec <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-8">           <span class="fu" style="color: #4758AB;">tidy</span>(<span class="at" style="color: #657422;">number =</span> <span class="dv" style="color: #AD0000;">2</span>, <span class="at" style="color: #657422;">type =</span> <span class="st" style="color: #20794D;">"coef"</span>)</span></code></pre></div>
</div>
<p>The last two lines extract both the principal components and the weights for each tag on each component. Now. we’re ready to make some plots.</p>
</section>
<section id="principal-components" class="level2">
<h2 class="anchored" data-anchor-id="principal-components">Principal Components</h2>
<p>The results for the first 3 principal componensts are shown below. I’ve shown the 20 most extreme components for clarity.</p>
<p>The first principal compnent has tags like “Anova”, “T-Test” and “SPSS” as heavily weighted positive, while “Machine Learning”, “Mathematical Statistics” and “Probability” are all weighted heavily negative (the direction of the weights doesn’t matter, it isn’t like one direction is better or worse). To me, I read this as “Beginner” vs “Advanced” answers. Questions with the former tags are usually from users who are maybe taking a stats class for the first time and are learning about the t test or Anova. The dead give away for this is the “SPSS” tag being weighted so heavily<sup>1</sup>. Looking at more weights verifies this, with the negative weights being associated with topics like “Neural Nets”, and “Maximum Likelihod” while the positive weights have tags like “Statistical Significance” and “Interpretation”.</p>
<p>Now remember, these components do not explain variance in questions. They explain variance in the question <em>answers</em>! So the first component is really about people who choose to answer simple versus complex topics. The second principal component has a fairly straightforward interpretation. This component explains variation between users who answer classical statistical questions versus those who opt to answer machine learning type questions. Lastly, the third principal component seems to be distinguishing users who answer forecasting type questions (see tags like “Arima”, “Time Series”, “Forecasting”, and “Econometrics”) versus non forecasting type questions.</p>
<div class="cell">

</div>
<div class="cell" data-layout-row="3" data-fig.dpi="240">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-08-16-pca/index_files/figure-html/prin-comp-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-08-16-pca/index_files/figure-html/prin-comp-2.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-08-16-pca/index_files/figure-html/prin-comp-3.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>I think what is more interesting is that we can plot some of the more popular users on the site using the principal components. In the first plot, I’ve scattered PC1 vs PC2. Left to right means advanced vs simple questions. It is no surpriuse to see whuber farther left and BruceET farther right. Whuber can answer most anything, and I feel like he often accepts the challenge of a complex answer, opting to comment on simpler questions. Bruce, on the other hand, will always answer a simple question very robustly. Top to bottom means machine learning vs classical stats. I’m not surprised to see Frank Harrell closer to the top, as he has appeared in many questions if not only to scold people for using accuracy as a metric. No surprise Topepo is on the top of this PC. Interestingly, I’m kind of near the origin, if not a bit right of it. Seems like I strike a good balance between ML and stats, but often opt to answer simpler questions.</p>
<p>Plotting PC1 vs PC3 shows a very predictable pattern. Users near the bottom are more forcasting types, so its no surprise that Rob Hyndman, Dimitris Rizopoulus (who does a lot of longitudinal work), and Stephan Kolassa are near the bottom. I’m near the top, I have no clue about any of that stuff to be honest.</p>
<div class="cell quarto-layout-panel" data-fig.dpi="240">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="https://dpananos.github.io/posts/2022-08-16-pca/index_files/figure-html/prin-comp-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><img src="https://dpananos.github.io/posts/2022-08-16-pca/index_files/figure-html/prin-comp-3-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>There appear to be at least three dimensions about which analysts on cross validated can be placed. Analysts can either opt to answer easy or difficult questions, which lean classical statistics or machine learning, with additional focus on forecasting on non-forecasting problems. That’s a fairly useful interpretation of the first three principal components!</p>
<p>It could be fun to think about how best to use this information. Now that we know these three kinds of subgroups, could we use it to recommend users questions to answer? Its been the case that <a href="https://stats.meta.stackexchange.com/questions/5325/we-have-a-very-large-widening-gap-between-questions-and-answers-how-do-we-fix">there is a large gap between questions and answers</a>, so maybe this could be useful but also maybe not.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Because once you’ve taken a few stats courses, you know bette than to use SPSS.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Machine Learning</category>
  <guid>https://dpananos.github.io/posts/2022-08-16-pca/index.html</guid>
  <pubDate>Tue, 16 Aug 2022 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Interim Analysis &amp; Group Sequential Designs Pt 2</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/index.html</link>
  <description><![CDATA[ 




<p>This is part 2 of an ongoing series on group sequential designs for AB testing. Previous parts are shown below</p>
<blockquote class="blockquote">
<p><a href="https://dpananos.github.io/posts/2022-07-06-gsd/">Part 1</a></p>
</blockquote>
<p>Last time, we noted that we want our AB tests to be shorter so we could learn quicker. Peeking – testing the data before the end of the experiment – inflates the probability we make a false positive unless we choose the critical values of the tests a little more carefully. The reason this happens is because requiring that any of the cumulative test statistics be larger than 1.96 in magnitude defines a region in the space of cumulative means which has a probability density larger than 5%. One way to fix that is just to redefine the space to be smaller by requiring the cumulative test statistics to be larger in magnitude than some other value. I noted this puts the unnecessary requirement on us that the thresholds all be the same. In this blog post, we will discuss other approaches to that problem and their pros and cons.</p>
<p>In order to have that discussion, we need to understand what “alpha” (<img src="https://latex.codecogs.com/png.latex?%5Calpha">) is and how it can be “spent”. That will allow us to talk about “alpha spending functions”.</p>
<section id="preliminaries" class="level2">
<h2 class="anchored" data-anchor-id="preliminaries">Preliminaries</h2>
<p>In the last post, we looked at a <img src="https://latex.codecogs.com/png.latex?K=2"> GSD with equal sized groups. Of course, we don’t need to have equal sized groups and we can have more than two stages. Let <img src="https://latex.codecogs.com/png.latex?n_k"> be the sample size of the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> group. Then <img src="https://latex.codecogs.com/png.latex?N%20=%20%5Csum_k%20n_k"> is the total sample size. It is sometimes more convenient to refer to the <em>information rates</em> <img src="https://latex.codecogs.com/png.latex?t_k%20=%20n_k/N">. We will do the same for consistency with other sources on GSDs.</p>
</section>
<section id="what-is-alpha-and-how-do-we-spend-it" class="level2">
<h2 class="anchored" data-anchor-id="what-is-alpha-and-how-do-we-spend-it">What is <img src="https://latex.codecogs.com/png.latex?%5Calpha">, and How Do We Spend It?</h2>
<p>The probability we reject the null, <img src="https://latex.codecogs.com/png.latex?H_0">, when it is true is called <img src="https://latex.codecogs.com/png.latex?%5Calpha">. In a classical test, we would reject <img src="https://latex.codecogs.com/png.latex?H_0"> when <img src="https://latex.codecogs.com/png.latex?%5Cvert%20Z%20%5Cvert%20%3E%20z_%7B1-%5Calpha/2%7D">, and so <img src="https://latex.codecogs.com/png.latex?P(%5Cvert%20Z%20%5Cvert%20%3E%20z_%7B1-%5Calpha/2%7D%20%5Cvert%20H_0)%20=%20%5Calpha">. Now consider a <img src="https://latex.codecogs.com/png.latex?K=4"> GSD so we can work with a concrete example. Let <img src="https://latex.codecogs.com/png.latex?Z%5E%7B(k)%7D"> be the test statistic after seeing the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> group, and let <img src="https://latex.codecogs.com/png.latex?u_k"> be the threshold so that if <img src="https://latex.codecogs.com/png.latex?%5Cvert%20Z%5E%7B(k)%7D%20%5Cvert%20%3E%20u_k"> then we would reject the null. Then a type one error could happen when <sup>1</sup> …</p>
<p><span id="eq-rejections"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A&amp;%5Cleft(%20u_1%20%5Clt%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cright)%20%20%5Cquad%20%5Cmbox%7Bor%7D%20%5C%5C%0A&amp;%20%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Clt%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5Cright)%20%5Cquad%20%5Cmbox%7Bor%7D%20%5C%5C%0A&amp;%20%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Cgeq%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_3%20%5Clt%20%5Cvert%20Z%5E%7B(3)%7D%20%5Cvert%20%5Cright)%20%5Cquad%20%5Cmbox%7Bor%7D%20%5C%5C%0A&amp;%20%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Cgeq%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_3%20%5Cgeq%20%5Cvert%20Z%5E%7B(3)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_4%20%5Clt%20%5Cvert%20Z%5E%7B(4)%7D%20%5Cvert%20%5Cright)%0A%5Cend%7Balign%7D%0A%5Ctag%7B1%7D"></span></p>
<p>So a type one error can occur in multiple ways, but we still want the probability we make a type one error to be <img src="https://latex.codecogs.com/png.latex?%5Calpha">, which means we’re going to need to evaluate the probability of the expression above. Note that each line in (Equation&nbsp;1) are mutually exclusive, so the probability of the expression above is just the sum of the probabilities of each expression. This gives us</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Calpha%20=%20&amp;P%5Cleft(%20u_1%20%5Clt%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5CBig%5Cvert%20H_0%5Cright)%20%20+%20%20%5C%5C%0A&amp;%20P%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Clt%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5CBig%5Cvert%20H_0%5Cright)%20+%20%5C%5C%0A&amp;%20P%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Cgeq%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_3%20%5Clt%20%5Cvert%20Z%5E%7B(3)%7D%20%5Cvert%20%5CBig%5Cvert%20H_0%5Cright)%20+%20%5C%5C%0A&amp;%20P%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Cgeq%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_3%20%5Cgeq%20%5Cvert%20Z%5E%7B(3)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_4%20%5Clt%20%5Cvert%20Z%5E%7B(4)%7D%20%5Cvert%20%5CBig%5Cvert%20H_0%5Cright)%0A%5Cend%7Balign%7D%0A"></p>
<p>The test at each stage contributes towards the probability we make a type one error <img src="https://latex.codecogs.com/png.latex?%5Calpha">. You can think of <img src="https://latex.codecogs.com/png.latex?%5Calpha"> as a “budget”, and at each stage we have to “spend” (see where I’m going?) some of that alpha, with the added condition that we can never buy it back (meaning our <img src="https://latex.codecogs.com/png.latex?%5Calpha"> spending must be increasing). How much we decide to spend dictates what the <img src="https://latex.codecogs.com/png.latex?u_k"> are going to be.</p>
<p>But how do we decide how to spend our <img src="https://latex.codecogs.com/png.latex?%5Calpha">? If <img src="https://latex.codecogs.com/png.latex?%5Calpha"> is our budget for type one error, we need some sort of spending plan. Or perhaps a spending…function.</p>
</section>
<section id="alpha-spending-functions" class="level2">
<h2 class="anchored" data-anchor-id="alpha-spending-functions"><img src="https://latex.codecogs.com/png.latex?%5Calpha">-Spending Functions</h2>
<p>An <img src="https://latex.codecogs.com/png.latex?%5Calpha">-spending function <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%5Cstar(t_k)"> can be any non-decreasing function of the information rate <img src="https://latex.codecogs.com/png.latex?t_k"> such that <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%5Cstar(0)=0"> and <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%5Cstar(1)%20=%20%5Calpha">. Using this approach, we don’t need to specify the number of looks (though we may plan for <img src="https://latex.codecogs.com/png.latex?K"> of them), nor the number of observations at those looks. Only the maximum sample size needed, <img src="https://latex.codecogs.com/png.latex?N">.</p>
<p>Each time we make an analysis, we spend some of our budgeted <img src="https://latex.codecogs.com/png.latex?%5Calpha">. In our first analysis (at <img src="https://latex.codecogs.com/png.latex?t_1%20=%20n_1/N">), we spend</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P%5Cleft(%20u_1%20%5Clt%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5CBig%5Cvert%20H_0%5Cright)%20=%20%5Calpha%5E%5Cstar(t_1)%20%5C%3E.%20"></p>
<p>At the second analysis, we spend</p>
<p><img src="https://latex.codecogs.com/png.latex?P%5Cleft(u_1%20%5Cgeq%20%5Cvert%20Z%5E%7B(1)%7D%20%5Cvert%20%5Cmbox%7B%20and%20%7D%20u_2%20%5Clt%20%5Cvert%20Z%5E%7B(2)%7D%20%5Cvert%20%5CBig%5Cvert%20H_0%5Cright)%20=%20%5Calpha%5E%5Cstar(t_2)%20-%20%5Calpha%5E%5Cstar(t_1)%20%5C%3E,"></p>
<p>and so on, with the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> analysis being the difference in the alpha spending functions at the successive information rates. The spend is defined in this way so that the sum of the spend totals <img src="https://latex.codecogs.com/png.latex?%5Calpha"> since <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20%5Csum_%7Bk=2%7D%5EK%20%5Calpha%5E%5Cstar(t_k)%20-%20%5Calpha%5E%5Cstar(t_%7Bk-1%7D)">. The quantities <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%5Cstar(t_k)%20-%20%5Calpha%5E%5Cstar(t_%7Bk-1%7D)"> determine what the <img src="https://latex.codecogs.com/png.latex?u_k"> should be through something called the <em>recursive integration formula</em>, which I will not be covering because wow is it every mathy and I need some time.</p>
<p>Two popular <img src="https://latex.codecogs.com/png.latex?%5Calpha">-spending functions are the Pockock Spending function and the O’Brien Flemming spending function, shown in Figure&nbsp;1. The equations don’t matter, what matters is the qualitative behavior.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-spending-function" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Pocock and O’Brien <img src="https://latex.codecogs.com/png.latex?%5Calpha">-spending functions</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/index_files/figure-html/fig-spending-function-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>In gray is the line <img src="https://latex.codecogs.com/png.latex?y%20=%200.05x">, which would correspond to an alpha spending function in which our spend is proportional to the difference in information rates. Note how the Pocock function is kind of close to the diagonal (but not exactly on it), while the O’Brien Flemmming is constant up until <img src="https://latex.codecogs.com/png.latex?t_k%20%5Capprox%200.3"> and then starts to increase. The result of this qualitative behavioiur is evident when we plot our rejection regions (the <img src="https://latex.codecogs.com/png.latex?u_k">, which remember depend on the spending function).</p>
</section>
<section id="plotting-the-rejection-regions" class="level2">
<h2 class="anchored" data-anchor-id="plotting-the-rejection-regions">Plotting the Rejection Regions</h2>
<p>In my <a href="https://dpananos.github.io/posts/2022-07-06-gsd/">last post</a>, the rejection region was plotted on the joint distribution of the <img src="https://latex.codecogs.com/png.latex?Z%5E%7B(1)%7D"> and <img src="https://latex.codecogs.com/png.latex?Z%5E%7B(2)%7D">. That is easy for two dimensions, doable for 3, and impossible for us to comprehend beyond that. Luckily, there is a simpler way of visualizing these rejection regions. We can simply plot the rejection regions <img src="https://latex.codecogs.com/png.latex?u_k"> as a function of the information rate <img src="https://latex.codecogs.com/png.latex?t_k">. Let’s plot the rejection regions for the Pocock and O’Brien Flemming spending functions now. But, I’m not going to label them just yet. I want you to think about which one might be which and why (knowing what we know about spending functions and the qualitative behaviour we saw above).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1">extract_lims <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(sfu){</span>
<span id="cb1-2">  </span>
<span id="cb1-3">  gs <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">gsDesign</span>(</span>
<span id="cb1-4">  <span class="at" style="color: #657422;">k=</span><span class="dv" style="color: #AD0000;">11</span>, </span>
<span id="cb1-5">  <span class="at" style="color: #657422;">timing =</span> <span class="fu" style="color: #4758AB;">seq</span>(<span class="fl" style="color: #AD0000;">0.1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">length.out =</span> <span class="dv" style="color: #AD0000;">11</span>),</span>
<span id="cb1-6">  <span class="at" style="color: #657422;">test.type=</span><span class="dv" style="color: #AD0000;">2</span>,</span>
<span id="cb1-7">  <span class="at" style="color: #657422;">alpha=</span><span class="fl" style="color: #AD0000;">0.025</span>, </span>
<span id="cb1-8">  <span class="at" style="color: #657422;">beta =</span> <span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb1-9">  <span class="at" style="color: #657422;">sfu=</span>sfu</span>
<span id="cb1-10">)</span>
<span id="cb1-11">  </span>
<span id="cb1-12"><span class="fu" style="color: #4758AB;">tibble</span>(<span class="at" style="color: #657422;">tk =</span> gs<span class="sc" style="color: #5E5E5E;">$</span>timing,</span>
<span id="cb1-13">       <span class="at" style="color: #657422;">lower =</span> gs<span class="sc" style="color: #5E5E5E;">$</span>lower<span class="sc" style="color: #5E5E5E;">$</span>bound,</span>
<span id="cb1-14">       <span class="at" style="color: #657422;">upper =</span> gs<span class="sc" style="color: #5E5E5E;">$</span>upper<span class="sc" style="color: #5E5E5E;">$</span>bound,</span>
<span id="cb1-15">       <span class="at" style="color: #657422;">spending =</span> <span class="fu" style="color: #4758AB;">if_else</span>(sfu<span class="sc" style="color: #5E5E5E;">==</span><span class="st" style="color: #20794D;">'OF'</span>, <span class="st" style="color: #20794D;">"O'Brien Flemming"</span>, <span class="st" style="color: #20794D;">"Pocock"</span>)</span>
<span id="cb1-16">       )</span>
<span id="cb1-17"></span>
<span id="cb1-18">}</span>
<span id="cb1-19"></span>
<span id="cb1-20"></span>
<span id="cb1-21">sfus<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">'Pocock'</span>, <span class="st" style="color: #20794D;">'OF'</span>)</span>
<span id="cb1-22"></span>
<span id="cb1-23">lims <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">map_dfr</span>(sfus, extract_lims) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-24">      <span class="fu" style="color: #4758AB;">pivot_longer</span>(<span class="at" style="color: #657422;">cols =</span> lower<span class="sc" style="color: #5E5E5E;">:</span>upper, <span class="at" style="color: #657422;">names_to =</span> <span class="st" style="color: #20794D;">'which'</span>, <span class="at" style="color: #657422;">values_to =</span> <span class="st" style="color: #20794D;">'uk'</span> )</span>
<span id="cb1-25"></span>
<span id="cb1-26">lims <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb1-27">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(tk, uk, <span class="at" style="color: #657422;">linetype =</span> <span class="fu" style="color: #4758AB;">interaction</span>(which, spending))) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-28">  <span class="fu" style="color: #4758AB;">geom_line</span>(<span class="at" style="color: #657422;">color=</span>my_blue, <span class="at" style="color: #657422;">size=</span><span class="dv" style="color: #AD0000;">1</span>) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-29">  <span class="fu" style="color: #4758AB;">scale_linetype_manual</span>(<span class="at" style="color: #657422;">values =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="dv" style="color: #AD0000;">2</span>)) <span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-30">  <span class="fu" style="color: #4758AB;">guides</span>(<span class="at" style="color: #657422;">linetype=</span>F) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb1-31">  <span class="fu" style="color: #4758AB;">theme</span>(<span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span>, </span>
<span id="cb1-32">        <span class="at" style="color: #657422;">panel.grid.major =</span> <span class="fu" style="color: #4758AB;">element_line</span>()</span>
<span id="cb1-33">        )<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb1-34">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">expression</span>(t[k]), </span>
<span id="cb1-35">       <span class="at" style="color: #657422;">y=</span><span class="fu" style="color: #4758AB;">expression</span>(u[k]))</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/index_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">ggsave</span>(<span class="st" style="color: #20794D;">'spending.png'</span>, <span class="at" style="color: #657422;">dpi =</span> <span class="dv" style="color: #AD0000;">240</span>)</span></code></pre></div>
</details>
</div>
<details>
<summary>
Click to see which is which
</summary>
<p>
</p><div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/index_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p></p>
</details>
<p>One of the spending functions results in the same <img src="https://latex.codecogs.com/png.latex?u_k">, regardless of information rate, while the other seems to put a relatively low chance of rejecting the null (low alpha spend) in the beginning but then allows for a larger chance to reject the null later (larger alpha spend). Now, knowing what we know about the spending function qualitative behaviour, which function corresponds to which spending function?</p>
<p>The solid line is very clearly the O’Brien Flemming spending function. When <img src="https://latex.codecogs.com/png.latex?t_k"> is small, then the O’Brien Flemming spending function has a small amount of spend (because <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%5Cstar(t_k)%20-%20%5Calpha%5E%5Cstar(t_%7Bk-1%7D)"> is very small when <img src="https://latex.codecogs.com/png.latex?t_k"> is small). But, when <img src="https://latex.codecogs.com/png.latex?t_k"> is large, then <img src="https://latex.codecogs.com/png.latex?%5Calpha%5E%5Cstar(t_k)%20-%20%5Calpha%5E%5Cstar(t_%7Bk-1%7D)"> is large, leading to more spend and hence smaller <img src="https://latex.codecogs.com/png.latex?u_k">. The Pocock function is the dashed line, but I have no good rationale why constant lines should come from a spending function which is not on the diagonal. I’d love an explanation if you have one.</p>
<p>So what is the difference? Why might you choose one spending function over another? Note that the O’Brien Flemming function results in critical values which are really big in the beginning and really small at the end. This means we have the best chance to reject the null near the end of the experiment. I mean…that’s nice and all, but for AB testing we want to save as much time as possible, and the large critical values in the beginning work against us in that regard. On the other hand Pocock spending has constant critical values, meaning we are more likely to save time and detect an effect early. However, we run the risk of not detecting an effect over the experimental period, so there is risk of commiting a type II error. If it were me, I would choose Pocock boundaries. Move faster, try more things, and you can likely make up for type II erros by doing so.</p>
<p>What would be kind of cool is to specify our own alpha spending function, perhaps one which makes it realy easy to reject the null early but later punishes us for spending so much early on. Maybe that would be a good idea for another post.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I feel like we are more or less ready to start doing GSD in AB testing. We know why peeking is bad, and more or less how to fix it now. We also have a better understanding of some of the tradeoffs between the two most prevalent spending functions. There is still a lot to talk about, including confidence intervals for these sorts of tests (which are NOT intuitive in the least) as well as stopping for futility as well as binding/non-binding boundaries. Honestly, I don’t have a good idea for what the next part will be about, but I promise it will make sense.</p>
</section>
<section id="visualising-alpha-spending-in-action" class="level2">
<h2 class="anchored" data-anchor-id="visualising-alpha-spending-in-action">Visualising Alpha Spending in Action</h2>
<p>Its one thing to talk about alpha spending (Equation&nbsp;1 and the probability statements that follow it), but it is another thing completely to see it in action.</p>
<p>I’m going to use <code>{rpact}</code> to obtain the <img src="https://latex.codecogs.com/png.latex?u_k"> for a <img src="https://latex.codecogs.com/png.latex?K=4"> stage GSD using the O’Brien Flemming spending function. Then, I’m going to simulate some data from a GSD and compute the spend to show you how it works. I really hope you take the time to do the same, it can really clear up how the spending works.</p>
<p>First, we need data, and a lot of it. Some of the spend can be on the order of 1e-5, so I’m going to cheat a little. The book I’m working from writes down the joint distribution of the <img src="https://latex.codecogs.com/png.latex?Z"> under some assumptions (namely that the standard deviation is known and the data are normal). Let’s use that joint distirbution to simulate 10, 000, 000 samples. This should give me about 3 decimal places of accuracy.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><span class="fu" style="color: #4758AB;">set.seed</span>(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb3-2">n <span class="ot" style="color: #003B4F;">=</span> <span class="dv" style="color: #AD0000;">250</span></span>
<span id="cb3-3">K <span class="ot" style="color: #003B4F;">=</span> <span class="dv" style="color: #AD0000;">4</span></span>
<span id="cb3-4"></span>
<span id="cb3-5"><span class="co" style="color: #5E5E5E;"># Construct the Cholesky Factor, row-wise</span></span>
<span id="cb3-6">A <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">rep</span>(<span class="dv" style="color: #AD0000;">0</span>, K<span class="sc" style="color: #5E5E5E;">*</span>K), <span class="at" style="color: #657422;">nrow =</span> K)</span>
<span id="cb3-7">A[<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>] <span class="ot" style="color: #003B4F;">=</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb3-8"><span class="cf" style="color: #003B4F;">for</span>(i <span class="cf" style="color: #003B4F;">in</span> <span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">:</span>K){</span>
<span id="cb3-9">  A[i, <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span>i] <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">sqrt</span>(n)<span class="sc" style="color: #5E5E5E;">/</span><span class="fu" style="color: #4758AB;">sqrt</span>(i<span class="sc" style="color: #5E5E5E;">*</span>n)</span>
<span id="cb3-10">}</span>
<span id="cb3-11"></span>
<span id="cb3-12"><span class="co" style="color: #5E5E5E;"># Construct the covariance </span></span>
<span id="cb3-13">S <span class="ot" style="color: #003B4F;">=</span> A <span class="sc" style="color: #5E5E5E;">%*%</span> <span class="fu" style="color: #4758AB;">t</span>(A)</span>
<span id="cb3-14"><span class="co" style="color: #5E5E5E;"># Draw from a multivariate normal</span></span>
<span id="cb3-15"><span class="co" style="color: #5E5E5E;"># Lots of draws because the alpha spend will be small</span></span>
<span id="cb3-16">X <span class="ot" style="color: #003B4F;">=</span> MASS<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">mvrnorm</span>(<span class="fl" style="color: #AD0000;">10e6</span>, <span class="fu" style="color: #4758AB;">rep</span>(<span class="dv" style="color: #AD0000;">0</span>, K), S)</span></code></pre></div>
</div>
<p>Now, let’s use <code>{rpart}</code> to get those critical values as well as how much alpha should be spent at each stage.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">r <span class="ot" style="color: #003B4F;">=</span> rpact<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">getDesignGroupSequential</span>(</span>
<span id="cb4-2">  <span class="at" style="color: #657422;">kMax =</span> K, </span>
<span id="cb4-3">  <span class="at" style="color: #657422;">sided=</span><span class="dv" style="color: #AD0000;">2</span>, </span>
<span id="cb4-4">  <span class="at" style="color: #657422;">alpha=</span><span class="fl" style="color: #AD0000;">0.05</span>, </span>
<span id="cb4-5">  <span class="at" style="color: #657422;">beta=</span><span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb4-6">  <span class="at" style="color: #657422;">informationRates =</span> <span class="fu" style="color: #4758AB;">seq</span>(<span class="fl" style="color: #AD0000;">0.25</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="at" style="color: #657422;">length.out=</span>K),</span>
<span id="cb4-7">  <span class="at" style="color: #657422;">typeOfDesign =</span> <span class="st" style="color: #20794D;">'OF'</span></span>
<span id="cb4-8">  )</span>
<span id="cb4-9"></span>
<span id="cb4-10">z_vals <span class="ot" style="color: #003B4F;">=</span> r<span class="sc" style="color: #5E5E5E;">$</span>criticalValues</span>
<span id="cb4-11">aspend <span class="ot" style="color: #003B4F;">=</span> r<span class="sc" style="color: #5E5E5E;">$</span>alphaSpent</span></code></pre></div>
</div>
<p>Now, its just a matter of taking means. The <code>ith</code> column of <code>X</code> represents a mean I might see in a group sequential design at the <code>ith</code> stage. We know what the critical value is for each stage, so we just have to estimate the proportion of observations in each column which are beyond the associated critical value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1">X1 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(X[, <span class="dv" style="color: #AD0000;">1</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span>z_vals[<span class="dv" style="color: #AD0000;">1</span>]</span>
<span id="cb5-2">X2 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(X[, <span class="dv" style="color: #AD0000;">2</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span>z_vals[<span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb5-3">X3 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(X[, <span class="dv" style="color: #AD0000;">3</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span>z_vals[<span class="dv" style="color: #AD0000;">3</span>]</span>
<span id="cb5-4">X4 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(X[, <span class="dv" style="color: #AD0000;">4</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span>z_vals[<span class="dv" style="color: #AD0000;">4</span>]</span></code></pre></div>
</div>
<p>To compute the alpha spend, we just compute the probability statement above</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">my_spend <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">c</span>(</span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">mean</span>(X1),</span>
<span id="cb6-3">  <span class="fu" style="color: #4758AB;">mean</span>((<span class="sc" style="color: #5E5E5E;">!</span>X1)<span class="sc" style="color: #5E5E5E;">&amp;</span>X2),</span>
<span id="cb6-4">  <span class="fu" style="color: #4758AB;">mean</span>((<span class="sc" style="color: #5E5E5E;">!</span>X1)<span class="sc" style="color: #5E5E5E;">&amp;</span>(<span class="sc" style="color: #5E5E5E;">!</span>X2)<span class="sc" style="color: #5E5E5E;">&amp;</span>X3),</span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">mean</span>((<span class="sc" style="color: #5E5E5E;">!</span>X1)<span class="sc" style="color: #5E5E5E;">&amp;</span>(<span class="sc" style="color: #5E5E5E;">!</span>X2)<span class="sc" style="color: #5E5E5E;">&amp;</span>(<span class="sc" style="color: #5E5E5E;">!</span>X3)<span class="sc" style="color: #5E5E5E;">&amp;</span>X4)</span>
<span id="cb6-6">)</span></code></pre></div>
</div>
<p>Now, we just take the cumulative sum of <code>my_spend</code> to determine how much alpha we spend up to the <code>ith</code> stage</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><span class="fu" style="color: #4758AB;">plot</span>(<span class="fu" style="color: #4758AB;">cumsum</span>(my_spend), aspend, <span class="at" style="color: #657422;">xlab =</span> <span class="st" style="color: #20794D;">'Simulated Spend'</span>, <span class="at" style="color: #657422;">ylab=</span><span class="st" style="color: #20794D;">'Spend From rpact'</span>)</span>
<span id="cb7-2"><span class="fu" style="color: #4758AB;">abline</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span></code></pre></div>
</details>
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/index_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Oh MAN does that feel good! We spend very nearly the projected alpha at each stage. THAT is alpha spending in action!</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Or if you like set theory, we could write each line as <img src="https://latex.codecogs.com/png.latex?%5Cleft(%20%5Cbigcap_%7Bk=1%7D%5E%7Bj-1%7D%20%5Cvert%7BZ%5E%7B(k)%7D%7D%20%5Cvert%20%5Clt%20u_k%20%5Cright)%20%5Ccap%20%5Cleft(%20%5Cvert%7BZ%5E%7B(j)%7D%7D%20%5Cvert%20%5Cgeq%20u_j%20%5Cright)">.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Statistics</category>
  <category>AB Testing</category>
  <guid>https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/index.html</guid>
  <pubDate>Sat, 30 Jul 2022 04:00:00 GMT</pubDate>
  <media:content url="https://dpananos.github.io/posts/2022-07-31-gsd-pt-2/spending.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Forecasting Experimental Lift Using Hierarchical Bayesian Modelling</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-07-20-pooling-experiments/index.html</link>
  <description><![CDATA[ 




<p>You’re part of a team at a company who is tasked with improving conversion on some web page. You’ve run a few experiments already with mixed results and now it is time to set some goals for the next year. Here is a question:</p>
<blockquote class="blockquote">
<p>Based on your team’s performance to date, how do you set realistic goals for incremental conversion?</p>
</blockquote>
<p>Maybe your approach for your end of year targets would look like</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CBig(%20%5Cmbox%7BAverage%20Lift%7D%5CBig)%5E%7B%5Cmbox%7BNumber%20of%20Planned%20Experiments%7D%7D%20"></p>
<p>Its a good back-of-the-napkin approach to the problem. But if you come up short is that <em>neccesarily</em> a failure? Or, could it be well within expectation?</p>
<p>This post is forecasting how much a given team can move a metric within some time frame. You’re going to forecast the lift the team can generate given some of their past performance. The forecasting is Bayesian, but assumes the team works within a frequentist framework.</p>
<section id="assumptions" class="level2">
<h2 class="anchored" data-anchor-id="assumptions">Assumptions</h2>
<p>Your team can run approximately 1 experiment per month or 12 in a calendar year (but the method we develop can be extended to an arbitrary number of experiments per month). Let’s say you start experimenting on January 1 and will evaluate your performance December 31. In addition to this, assume:</p>
<ul>
<li>All your experiments are A/B tests with two and only two groups: test and control.</li>
<li>Your main metric is a conversion rate and the baseline value is 1%.</li>
<li>Every intervention has an effect, though it may be small. The null is never true.</li>
<li>Your site sees 100,000 unique users per month. You split all 100,000 into two groups at random, and</li>
<li>You measure lift in a relative sense (this is sometimes called <em>relative risk</em> in epidemiology).</li>
</ul>
<p>Let’s make some additional assumptions about experiments:</p>
<ul>
<li>Your team is relatively reliable. They don’t get better at thinking up interventions over time, so the effects they generate do not change over time, except for random variation.</li>
<li>Experiments effects are independent of one another, so the implementation of one change does not alter the effect of the next experiment.</li>
</ul>
</section>
<section id="scenario" class="level2">
<h2 class="anchored" data-anchor-id="scenario">Scenario</h2>
<p>Shown in the table below are your results over the last year. Nice job, lots of wins, a few failures to reject the null, but overall very good. Using the estimated relative lifts where you did , you managed to increase conversion by 80%. Now, you’re PM is asking you to shoot for 2x conversion this year.</p>
<p>Is that reasonable<sup>1</sup>? How probable are you to generate at least 2x lift over 12 months given your past performance? I mean, it’s only a little better than you did this past year, right? Luckily, you’re a good data scientist. Even though your team uses frequentism to evaluate their A/B tests, you are not beholden to one ideology over another. So, you decide to use a hierarchical Bayesian model to estimate what kinds of lifts your team is likely to generate in the future.</p>
<div class="cell">
<div class="cell-output-display">

<table class="table" style="margin-left: auto; margin-right: auto;">
 <thead>
  <tr>
   <th style="text-align:left;"> N </th>
   <th style="text-align:right;"> Treatment Conversions </th>
   <th style="text-align:right;"> Control Conversions </th>
   <th style="text-align:right;"> Relative Lift </th>
   <th style="text-align:right;"> p </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 541 </td>
   <td style="text-align:right;"> 496 </td>
   <td style="text-align:right;"> 1.09 </td>
   <td style="text-align:right;"> 0.08 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 557 </td>
   <td style="text-align:right;"> 524 </td>
   <td style="text-align:right;"> 1.06 </td>
   <td style="text-align:right;"> 0.16 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 559 </td>
   <td style="text-align:right;"> 486 </td>
   <td style="text-align:right;"> 1.15 </td>
   <td style="text-align:right;"> 0.01 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 556 </td>
   <td style="text-align:right;"> 500 </td>
   <td style="text-align:right;"> 1.11 </td>
   <td style="text-align:right;"> 0.04 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 530 </td>
   <td style="text-align:right;"> 516 </td>
   <td style="text-align:right;"> 1.03 </td>
   <td style="text-align:right;"> 0.34 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 532 </td>
   <td style="text-align:right;"> 475 </td>
   <td style="text-align:right;"> 1.12 </td>
   <td style="text-align:right;"> 0.04 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 516 </td>
   <td style="text-align:right;"> 507 </td>
   <td style="text-align:right;"> 1.02 </td>
   <td style="text-align:right;"> 0.40 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 532 </td>
   <td style="text-align:right;"> 475 </td>
   <td style="text-align:right;"> 1.12 </td>
   <td style="text-align:right;"> 0.04 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 528 </td>
   <td style="text-align:right;"> 490 </td>
   <td style="text-align:right;"> 1.08 </td>
   <td style="text-align:right;"> 0.12 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 544 </td>
   <td style="text-align:right;"> 506 </td>
   <td style="text-align:right;"> 1.08 </td>
   <td style="text-align:right;"> 0.13 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 519 </td>
   <td style="text-align:right;"> 512 </td>
   <td style="text-align:right;"> 1.01 </td>
   <td style="text-align:right;"> 0.43 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> 50,000 </td>
   <td style="text-align:right;"> 552 </td>
   <td style="text-align:right;"> 489 </td>
   <td style="text-align:right;"> 1.13 </td>
   <td style="text-align:right;"> 0.03 </td>
  </tr>
</tbody>
</table>

</div>
</div>
</section>
<section id="hierarchical-model" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-model">Hierarchical Model</h2>
<p>Let <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BRR%7D_i"> be the estimated relative lift<sup>2</sup> from experiment <img src="https://latex.codecogs.com/png.latex?i">. The sampling distribution of relative lift is asymptotically normally distributed on the log scale. Assuming we know the standard error exactly (using the delta rule), this means</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Clog%20%5CBig(%5Cwidehat%7BRR%7D_i%20%5CBig)%20%5Csim%20%5Cmathcal%7BN%7D(%5Clog(%5Ctheta_i),%20%5Csigma)"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Clog(%5Ctheta_i)"> is the relative lift on the log scale for experiment <img src="https://latex.codecogs.com/png.latex?i"> (whereas <img src="https://latex.codecogs.com/png.latex?%5Cwidehat%7BRR%7D_i"> is just the estimated relative lift). We can model the <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> hierarchically as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Clog(%5Ctheta_i)%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu,%20%5Ctau)%20"></p>
<p>Now, you just need to place priors on <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Ctau"> (assume you used good priors).</p>
</section>
<section id="forcasting-lift" class="level2">
<h2 class="anchored" data-anchor-id="forcasting-lift">Forcasting Lift</h2>
<p>Once you fit your model, you can generate hypothetical relative lifts by sampling from the model. Let <img src="https://latex.codecogs.com/png.latex?%5Cpsi"> be a relative lift, so that</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Clog(%5Cpsi)%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmu,%20%5Csigma)%20%5C%3E.%20"></p>
<p>If your team were to implement an experiment for which had a relative lift of <img src="https://latex.codecogs.com/png.latex?%5Cpsi">, you would get an estimated relative lift. Depending on the size of that estimate, you may or may not reject the null hypothesis. The probability you reject the null hypothesis is when it is false (and it is always false by assumption) is known as the statistical power. Since you have a fixed sample size in each experiment, and every experiment is a 50/50 split, you can calculate the statistical power that you detect a relative lift of <img src="https://latex.codecogs.com/png.latex?%5Cpsi">. Call that <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cpsi%7D">.</p>
<p>Now for the fun part. Say you run <img src="https://latex.codecogs.com/png.latex?n"> experiments per month for <img src="https://latex.codecogs.com/png.latex?K"> months. The lift you generate in month <img src="https://latex.codecogs.com/png.latex?k">, <img src="https://latex.codecogs.com/png.latex?LG_k">, would be</p>
<p><img src="https://latex.codecogs.com/png.latex?%20LG_k%20=%20%5Cexp%5CBigg(%20%5Csum_%7Bj=1%7D%5En%20%5Clog(%5Cpsi_j)%20p_%7B%5Cpsi,%20j%7D%20%5CBigg)%20"></p>
<p>and the forecasted lift, <img src="https://latex.codecogs.com/png.latex?FL">, up to and including month <img src="https://latex.codecogs.com/png.latex?k"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20FL_k%20=%20%5Cprod_%7Bi=1%7D%5E%7Bk%7D%20LG_i%20"></p>
<p>Think this through. If you were to implement every intervention, your lift would simply be <img src="https://latex.codecogs.com/png.latex?%5Cprod_%7Bj=1%7D%5Ek%20%5Cpsi_j">, or on the log scale <img src="https://latex.codecogs.com/png.latex?%5Csum_j%20%5Clog(%5Cpsi_j)">. But you don’t detect every effect. The probability you detect the effect of the <img src="https://latex.codecogs.com/png.latex?j%5E%7Bth%7D"> intervention is <img src="https://latex.codecogs.com/png.latex?p_%7B%5Cpsi,%20j%7D">. So <img src="https://latex.codecogs.com/png.latex?%5Csum_j%20%5Clog(%5Cpsi_j)%20p_%7B%5Cpsi,%20j%7D"> is the expected lift you would accrue over the <img src="https://latex.codecogs.com/png.latex?k"> experiments. Take the exponential to convert this sum back to a product and you’ve got a generated lift after <img src="https://latex.codecogs.com/png.latex?n"> experiments in a given month. Multiply the lift month over month to get a forecasted lift. Now, because there is uncertainty in the <img src="https://latex.codecogs.com/png.latex?%5Cpsi">, there is uncertainty in the forecasted lift. However, your hierarchical model will make it more or less easy to integrate over that uncertainty. Just sample from the model and average over the samples.</p>
</section>
<section id="modelling" class="level2">
<h2 class="anchored" data-anchor-id="modelling">Modelling</h2>
<div class="cell">

</div>
<p>Luckily, all of the computation above – even the power calculation – can be done inside Stan (and you’re pretty good at writing Stan code<sup>3</sup>).</p>
<p>Shown in Figure&nbsp;1 is the forecasted lift as compared to baseline after the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> month Good job, if you keep doing things as you’re doing, you’re going to probably increase conversion rate by a little more than 50% (a little less than the 80% but still nothing to sneeze at). The shaded blue regions indicate the uncertainty in that estimate. Note that although your forecasted lift seems to always be increasing, that isn’t necessarily the case. You could implement a change which hurts our conversion because of chance, so if you were to plot simulated trajectories you might see some decreases in the metric.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-forecast" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Forecasted lift after the 12 months. Shown in blue are credible interval estimates. The conditional distirbution is log-normal since the forecasted lift is the sum of normals on the log scale.</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/index_files/figure-html/fig-forecast-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<details>
<summary>
Click to see individual trajectories
</summary>
<p>
</p><div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption">The red lines are draws where you would have implemented a change to hurt the conversion rate. See how sometimes those lines actually decrease? Such is life, can’t win em all!</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p></p>
</details>
<p>Now, what about that goal of increasing conversion by 2x? Well, it isn’t looking good. Looks like there is only a 12% chance you meet or exceed the 2x goal. Could it be your performance last year was just extreme? The distribution of forecasted lifts <em>is</em> long tailed. Maybe you’re just going to regress to the mean. Sounds like a good time to push back on your boss and come prepared with data.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-conditional" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Conditional posterior distirbution of forecasted lifts after completing the 12 experiments you had planned this year. Indicated point/interval is mean and 95% credible interval.</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/index_files/figure-html/fig-conditional-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>You’re clever and realized you could use a hierarchical model to simulate future experiment results and use those to forecast your team’s performance. Your boss’ goal of a 2x increase is nice in so far as it shows they have confidence in you and your team, but the model says it isn’t super achievable.</p>
<p>If 2x isn’t achievable, what is a better target? Or maybe, what is a better range of targets. I’m not sure, that isn’t the point of the post. The post was to equip you with a means of answering that question yourself, and I know you’re capable of answering it. I mean…look at all this cool modelling you did.</p>
</section>
<section id="post-script" class="level2">
<h2 class="anchored" data-anchor-id="post-script">Post Script</h2>
<p>Ok, breaking away from the narrative for a moment…this is a continuous approximation to a discrete process. We should simulate this to see how real experiments would stack up against my forecast. I’ve gone ahead and actually simulated running the 12 tests and computed the lift after the 12 tests. Shown below is the forecasted lift versus relative error as compared to simulation. I’ll let you come to your own conclusion about the quality of the approximation.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Just go with it, its a work of fiction, who knows if it is reasonable. Let’s pull out the forecasted lifts after the final experiment.↩︎</p></li>
<li id="fn2"><p><img src="https://latex.codecogs.com/png.latex?RR"> for relative risk, sorry my epidemiology is showing↩︎</p></li>
<li id="fn3"><p>Speaking of Stan code, the Stan file in the github repo for this post (see the “Edit this page on github” on the right hand side).↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Statistics</category>
  <category>AB Testing</category>
  <category>Bayes</category>
  <guid>https://dpananos.github.io/posts/2022-07-20-pooling-experiments/index.html</guid>
  <pubDate>Wed, 20 Jul 2022 04:00:00 GMT</pubDate>
  <media:content url="https://dpananos.github.io/posts/2022-07-20-pooling-experiments/forecast.png" medium="image" type="image/png" height="103" width="144"/>
</item>
<item>
  <title>Interim Analysis &amp; Group Sequential Designs Pt 1</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-07-06-gsd/index.html</link>
  <description><![CDATA[ 




<blockquote class="blockquote">
<p>Special thanks to <a href="https://twitter.com/jfiksel1">Jacob Fiksel</a> for writing a great <a href="file:///Users/demetripananos/Zotero/storage/HQG5JJ4N/2021-02-03-alpha_spending_explained.html">blog post</a> which inspired me to write my own.</p>
</blockquote>
<p>At Zapier, AB testing kind of has a bad rap. AB testing is perceived as slow – sometimes taking up to a month to complete a single test– with the chance that we don’t get a definitive result (i.e.&nbsp;we fail to reject the null). One of our priorities (and hence my priority) is to find a way to speed up AB testing so we can learn faster.</p>
<p>Peeking is one way to do that. Peeking involves testing the experimental data before the end of the experiment (“peeking” at the results to see if they indicate a change). As you may know from other <a href="https://www.evanmiller.org/how-not-to-run-an-ab-test.html">popular posts</a> on the matter, or from sophomore stats, this can inflate the type one error. That’s a real shame, because peeking is a really attractive way to end an experiment early and save some time. Additionally, people are curious! They want to know how things are going. Fortunately, there are ways to satisfy the urge to peek while preserving the type one error rate.</p>
<p>One way to peek while preserving the type one error rate is through Group Sequential Designs (GSDs). This series of blog posts is intended to delve into some of the theory of GSDs. To me, theoretical understanding – knowing why something works, or at least being able to understand how in principle I could do this myself – is the key to learning. I’m happy to just do this in isolation, but I bet someone else may benefit too.</p>
<p>I’m working mainly from <a href="https://link.springer.com/book/10.1007/978-3-319-32562-0">this</a> book, but I don’t anticipate I will discuss the entirety of the book. I really want to know a few key things:</p>
<ul>
<li>What is the foundational problem for peeking?</li>
<li>How can we address that problem (i.e.&nbsp;How can we preserve the type one error when we peek)?</li>
<li>How else can we speed up experiments (e.g.&nbsp;by declaring an experiment futile)?</li>
<li>What is the theory underlying each of the above?</li>
</ul>
<section id="goal-for-this-post" class="level2">
<h2 class="anchored" data-anchor-id="goal-for-this-post">Goal For This Post</h2>
<p>We know that under “peeking conditions” – just testing the data as they roll in – inflates the type one error rate. In this post, I want to understand <em>why</em> that happens. Like…where is the problem <em>exactly</em>? Where will be our theoretical basis for attacking the problem of controlling the type one error rate?</p>
<p>But first, a little background on GSDs.</p>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>The “<em>G</em>” in GSD means that the hypothesis test is performed on groups of observations. Given a maximum number of groups <img src="https://latex.codecogs.com/png.latex?K">, the sample size of <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> each group is <img src="https://latex.codecogs.com/png.latex?n_k">.</p>
<p>The “<em>S</em>” in GSD means the test is performed sequentially. If after observing the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> group the test statistic (computed using all the data observed up to that point) is beyond some threshold, then the null is rejected and the experiment is finished. If not, the next group of observations is made and added to the existing data, wherein the process continues until the final group has been observed. If after observing the final group the test statistic does not exceed the threshold, then we fail to reject the null. The process for <img src="https://latex.codecogs.com/png.latex?K=2"> is illustrated in Figure&nbsp;1.</p>
<div class="cell fig-cap-location-top">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode default code-with-copy"><code class="sourceCode default"><span id="cb1-1">flowchart TD</span>
<span id="cb1-2">  A[Observe Group k=1] --&gt; B[Perform Test]</span>
<span id="cb1-3">  B --&gt; C{Data From k=1 \n Significant?}</span>
<span id="cb1-4">  C -- Yes --&gt; D[Reject Null]</span>
<span id="cb1-5">  C -- No --&gt; E[Observe Group k=2]</span>
<span id="cb1-6">  E --&gt; G{Data From k=1 and \n k=2 Significant?}</span>
<span id="cb1-7">  G -- Yes --&gt; D</span>
<span id="cb1-8">  G -- No --&gt; H[Fail To Reject Null]</span></code></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-gsd" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;1: A GSD for <img src="https://latex.codecogs.com/png.latex?K=2"></figcaption><p></p>
<p>
</p><pre class="mermaid mermaid-js" data-tooltip-selector="#mermaid-tooltip-1">flowchart TD
  A[Observe Group k=1] --&gt; B[Perform Test]
  B --&gt; C{Data From k=1 \n Significant?}
  C -- Yes --&gt; D[Reject Null]
  C -- No --&gt; E[Observe Group k=2]
  E --&gt; G{Data From k=1 and \n k=2 Significant?}
  G -- Yes --&gt; D
  G -- No --&gt; H[Fail To Reject Null]
</pre>
<div id="mermaid-tooltip-1" class="mermaidTooltip">

</div>
<p></p>
</figure>
</div>
</div>
</div>
</section>
<section id="some-math-on-means" class="level2">
<h2 class="anchored" data-anchor-id="some-math-on-means">Some Math on Means</h2>
<p>Means are a fairly standard place to start for a statsitical test, so we will start there too. Let <img src="https://latex.codecogs.com/png.latex?X_%7Bk,%20i%7D"> be the <img src="https://latex.codecogs.com/png.latex?i%5E%7Bth%7D"> observation in the <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> group. Then the mean of group <img src="https://latex.codecogs.com/png.latex?k"> is</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbar%7BX%7D_k%20=%20%5Cdfrac%7B1%7D%7Bn_k%7D%20%5Csum_%7Bi=1%7D%5E%7Bn_%7Bk%7D%7D%20X_%7Bk,%20i%7D%20"></p>
<p>Since we are accumulating data, let’s write the cumulative mean up to and including group <img src="https://latex.codecogs.com/png.latex?k"> as <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(k)%7D">, and let the cumulative standard deviation up to and including group <img src="https://latex.codecogs.com/png.latex?k"> be <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E%7B(k)%7D">. We can actually write <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(k)%7D"> in terms of the group means <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_%7Bk%7D"> using some algebra. Its just a weighted mean of the previous <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_%7Bk%7D"> weighted by the sample size.</p>
<p><span id="eq-eq-1"><img src="https://latex.codecogs.com/png.latex?%0A%5Cbar%7BX%7D%5E%7B(k)%7D%20=%20%20%5Cdfrac%7B%5Csum_%7B%5Ctilde%7Bk%7D%20=%201%7D%5E%7Bk%5E%5Cprime%7D%20n_%7B%5Ctilde%7Bk%7D%7D%20%5Cbar%7BX%7D_%7B%5Ctilde%7Bk%7D%7D%7D%7B%5Csum_%7B%5Ctilde%7Bk%7D%20=%201%7D%5E%7Bk%5E%5Cprime%7D%20n_%7B%5Ctilde%7Bk%7D%7D%7D%0A%5Ctag%7B1%7D"></span></p>
</section>
<section id="a-simple-example" class="level2">
<h2 class="anchored" data-anchor-id="a-simple-example">A Simple Example</h2>
<p>Remember that our goal is to understand <em>why</em> the type one error rate increases when we peek as data accumulates, as we might do in an AB test. Answering <em>how much</em> is a little easier, so let’s do that first. Let’s do so by analyzing a <img src="https://latex.codecogs.com/png.latex?K=2"> GSD where we assume:</p>
<ul>
<li>That each group has the same sample size <img src="https://latex.codecogs.com/png.latex?n_1%20=%20n_2%20=%20n">.</li>
<li>That the data we observe are IID bernoulli trials <img src="https://latex.codecogs.com/png.latex?X_%7Bk,%20i%7D%20%5Csim%20%5Coperatorname%7BBernoulli%7D(p=0.5)"> for <img src="https://latex.codecogs.com/png.latex?k=1,%202"> and <img src="https://latex.codecogs.com/png.latex?j=1,%20%5Cdots,%20n">.</li>
<li>That our false postie rate <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%200.05"></li>
</ul>
<section id="how-much-does-the-type-one-inflate" class="level3">
<h3 class="anchored" data-anchor-id="how-much-does-the-type-one-inflate"><em>How Much</em> Does The Type One Inflate?</h3>
<p>Let’s just simulate data under the assumptions above. At each stage, let’s test the null that <img src="https://latex.codecogs.com/png.latex?H_0:%20p=0.5"> against <img src="https://latex.codecogs.com/png.latex?H_A:%20p%20%5Cneq%200.5"> and see how frequently we reject the null. In our simulation, we will assume “peeking” conditions, meaning we’re just going to do a test of proportions at each stage.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><span class="fu" style="color: #4758AB;">library</span>(tidyverse)</span>
<span id="cb2-2"></span>
<span id="cb2-3"><span class="fu" style="color: #4758AB;">set.seed</span>(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb2-4"></span>
<span id="cb2-5"><span class="co" style="color: #5E5E5E;"># Simulation Parameters</span></span>
<span id="cb2-6">p<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fl" style="color: #AD0000;">0.5</span></span>
<span id="cb2-7">n<span class="ot" style="color: #003B4F;">&lt;-</span> <span class="dv" style="color: #AD0000;">250</span></span>
<span id="cb2-8">nsims <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">as.integer</span>((<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.01</span>)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb2-9"></span>
<span id="cb2-10"><span class="co" style="color: #5E5E5E;"># Run the simulation</span></span>
<span id="cb2-11">sims<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">rerun</span>(nsims, {</span>
<span id="cb2-12">  <span class="co" style="color: #5E5E5E;"># K=1</span></span>
<span id="cb2-13">  x1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rbinom</span>(<span class="dv" style="color: #AD0000;">1</span>, n, p)</span>
<span id="cb2-14">  <span class="co" style="color: #5E5E5E;"># K=2, accumulating data from each state</span></span>
<span id="cb2-15">  x2 <span class="ot" style="color: #003B4F;">&lt;-</span> x1 <span class="sc" style="color: #5E5E5E;">+</span> <span class="fu" style="color: #4758AB;">rbinom</span>(<span class="dv" style="color: #AD0000;">1</span>, n, p)</span>
<span id="cb2-16">  </span>
<span id="cb2-17">  <span class="co" style="color: #5E5E5E;"># Compute some various quntities we will need, like the Z score</span></span>
<span id="cb2-18">  K <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">str_c</span>(<span class="st" style="color: #20794D;">'K='</span>,<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb2-19">  X <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(x1, x2) <span class="sc" style="color: #5E5E5E;">/</span> ((<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">2</span>)<span class="sc" style="color: #5E5E5E;">*</span>n)</span>
<span id="cb2-20">  mu <span class="ot" style="color: #003B4F;">&lt;-</span> p</span>
<span id="cb2-21">  sds <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(p<span class="sc" style="color: #5E5E5E;">*</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>p)<span class="sc" style="color: #5E5E5E;">/</span>(n<span class="sc" style="color: #5E5E5E;">*</span><span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">:</span><span class="dv" style="color: #AD0000;">2</span>))</span>
<span id="cb2-22">  Z <span class="ot" style="color: #003B4F;">&lt;-</span> (X<span class="sc" style="color: #5E5E5E;">-</span>p)<span class="sc" style="color: #5E5E5E;">/</span>sds</span>
<span id="cb2-23">  reject <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">abs</span>(Z)<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="fl" style="color: #AD0000;">1.96</span></span>
<span id="cb2-24">  </span>
<span id="cb2-25">  <span class="fu" style="color: #4758AB;">tibble</span>(K, X, mu, sds, Z, reject)</span>
<span id="cb2-26">}) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-27">  <span class="fu" style="color: #4758AB;">bind_rows</span>(<span class="at" style="color: #657422;">.id=</span><span class="st" style="color: #20794D;">'sim'</span>)</span>
<span id="cb2-28"></span>
<span id="cb2-29">fpr<span class="ot" style="color: #003B4F;">&lt;-</span>sims <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-30">  <span class="fu" style="color: #4758AB;">group_by</span>(sim) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-31">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">result=</span><span class="fu" style="color: #4758AB;">any</span>(reject)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-32">  <span class="fu" style="color: #4758AB;">summarise</span>(<span class="at" style="color: #657422;">fpr =</span> <span class="fu" style="color: #4758AB;">mean</span>(result)) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb2-33">  <span class="fu" style="color: #4758AB;">pull</span>(fpr)</span></code></pre></div>
</div>
<p>From our simulation, we reject the null around 8.6% of the time. That is certainly higher than the nominal 5%, but if we recall our sophomore stats classes, isn’t there a 9.8% (<img src="https://latex.codecogs.com/png.latex?1-0.95%5E2">) chance we reject the null?</p>
<p>The 8.6% isn’t simulation error. We forgot that <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(1)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(2)%7D"> are <em>correlated</em>. The correlation between <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(1)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(2)%7D"> makes intuitive sense. If the sample mean for the first group is small, then the accumulated mean is also likely to be small than if we were to just take a new sample. Let’s take a more detailed look at Equation&nbsp;1. Note that</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbar%7BX%7D%5E%7B(2)%7D%20=%20%5Cdfrac%7Bn_1%20%5Cbar%7BX%7D_1%20+%20n_2%5Cbar%7BX%7D_2%7D%7Bn_1%20+%20n_2%7D%20%5C%3E."></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(1)%7D"> (which is just <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D_1"> ) appears in the expression for <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(2)%7D">. In the extreme case where <img src="https://latex.codecogs.com/png.latex?n_2=1">, the stage 2 mean is going to be <img src="https://latex.codecogs.com/png.latex?n_1%20%5Cbar%7BX%7D_1/(n_1+1)%20+%20X_%7B2,%201%7D/(n_1+1)">. How much could a single observation change the sample mean? It depends on the observation, but also on how big that sample is. The stuff you learned in sophmore stats about type one error inflating like <img src="https://latex.codecogs.com/png.latex?1%20-%20(1-%5Calpha)%5Ek"> assumes the test statistics are independent. So where does the 8.6% come from? To answer that, we need to understand the joint distribution of the <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(k)%7D">.</p>
</section>
<section id="why-the-type-one-inflates" class="level3">
<h3 class="anchored" data-anchor-id="why-the-type-one-inflates"><em>Why</em> The Type One Inflates</h3>
<p>The assumptions we made above allow us to get a little analytic traction. We know that the sampling distribution of <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(1)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(2)%7D"> are asymptotic normal thanks to the CLT</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbar%7BX%7D%5E%7B(1)%7D%20%5Csim%20%5Coperatorname%7BNormal%7D%5Cleft(p,%20%5Cdfrac%7Bp(1-p)%7D%7Bn%7D%5Cright)%20%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbar%7BX%7D%5E%7B(2)%7D%5Csim%20%5Coperatorname%7BNormal%7D%5Cleft(%20p,%20%5Cdfrac%7Bp(1-p)%7D%7B2%20n%7D%20%5Cright)%20%20"></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1">my_blue <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">rgb</span>(<span class="dv" style="color: #AD0000;">45</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">250</span>, <span class="dv" style="color: #AD0000;">62</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">250</span>, <span class="dv" style="color: #AD0000;">80</span><span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">250</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb3-2"><span class="fu" style="color: #4758AB;">theme_set</span>(<span class="fu" style="color: #4758AB;">theme_classic</span>())</span>
<span id="cb3-3"></span>
<span id="cb3-4"></span>
<span id="cb3-5">sims <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb3-6">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(X))<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-7">  <span class="fu" style="color: #4758AB;">geom_histogram</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">y=</span>..density..), <span class="at" style="color: #657422;">fill =</span> <span class="st" style="color: #20794D;">'light gray'</span>, <span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">'black'</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-8">  <span class="fu" style="color: #4758AB;">facet_wrap</span>(<span class="sc" style="color: #5E5E5E;">~</span>K) <span class="sc" style="color: #5E5E5E;">+</span> </span>
<span id="cb3-9">  <span class="fu" style="color: #4758AB;">geom_line</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">y =</span> <span class="fu" style="color: #4758AB;">dnorm</span>(X,</span>
<span id="cb3-10">                          <span class="at" style="color: #657422;">mean =</span> mu,</span>
<span id="cb3-11">                          <span class="at" style="color: #657422;">sd =</span> sds[PANEL])),</span>
<span id="cb3-12">            <span class="at" style="color: #657422;">color =</span> my_blue, </span>
<span id="cb3-13">            <span class="at" style="color: #657422;">size =</span> <span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-14">  <span class="fu" style="color: #4758AB;">theme</span>(</span>
<span id="cb3-15">    <span class="at" style="color: #657422;">panel.grid.major =</span> <span class="fu" style="color: #4758AB;">element_line</span>()</span>
<span id="cb3-16">  )<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb3-17">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">y=</span><span class="st" style="color: #20794D;">'Density'</span>,</span>
<span id="cb3-18">       <span class="at" style="color: #657422;">x =</span> <span class="fu" style="color: #4758AB;">expression</span>(<span class="fu" style="color: #4758AB;">bar</span>(X)<span class="sc" style="color: #5E5E5E;">^</span>(k)))</span></code></pre></div>
</details>
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p></p><figcaption class="figure-caption">10,000 simulations of a <img src="https://latex.codecogs.com/png.latex?K=2"> GSD. Each group has 250 observations. Note that <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(2)%7D"> has smaller standard error due to the fact that 500 (250 + 250) observations are used in the computation. Sampling distributions show in blue.</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-06-gsd/index_files/figure-html/marginal-density-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Consider the random vector <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5Cleft(%5Cbar%7BX%7D%5E%7B(1)%7D,%20%5Cbar%7BX%7D%5E%7B(2)%7D%5Cright)">. Since each components has a normal marginal then the joint must be multivariate normal</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctheta%20%5Csim%20%5Cmathcal%7BN%7D(%5Cmathbf%7Bp%7D,%20%5CSigma)%20"></p>
<p>with mean <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7Bp%7D%20=%20(p,p)"> and covariance<sup>1</sup></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CSigma=%20p(1-p)%5Cbegin%7Bbmatrix%7D%0A%5Cdfrac%7B1%7D%7Bn_1%7D%20&amp;%20%20%5Cdfrac%7B1%7D%7Bn_1%20+%20n_2%7D%20%5C%5C%0A%5Cdfrac%7B1%7D%7Bn_1%20+%20n_2%7D%20&amp;%20%5Cdfrac%7B1%7D%7Bn_1%20+%20n_2%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">sigma_1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">qchisq</span>(<span class="fl" style="color: #AD0000;">0.95</span>, <span class="dv" style="color: #AD0000;">2</span>))</span>
<span id="cb4-2">sigma_2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">qchisq</span>(<span class="fl" style="color: #AD0000;">0.99</span>, <span class="dv" style="color: #AD0000;">2</span>))</span>
<span id="cb4-3">sig<span class="ot" style="color: #003B4F;">&lt;-</span> p<span class="sc" style="color: #5E5E5E;">*</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">-</span>p) <span class="sc" style="color: #5E5E5E;">*</span> <span class="fu" style="color: #4758AB;">matrix</span>(<span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>n, <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>n), <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>n), <span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>n) ), <span class="at" style="color: #657422;">nrow =</span> <span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb4-4">tt <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">seq</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb4-5">x <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">cos</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>pi<span class="sc" style="color: #5E5E5E;">*</span>tt)</span>
<span id="cb4-6">y <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">sin</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>pi<span class="sc" style="color: #5E5E5E;">*</span>tt)</span>
<span id="cb4-7">R <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">cbind</span>(x,y)</span>
<span id="cb4-8">e <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">eigen</span>(sig)</span>
<span id="cb4-9">V <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">diag</span>(e<span class="sc" style="color: #5E5E5E;">$</span>values))</span>
<span id="cb4-10"></span>
<span id="cb4-11">level_curve_1 <span class="ot" style="color: #003B4F;">&lt;-</span> sigma_1<span class="sc" style="color: #5E5E5E;">*</span>R <span class="sc" style="color: #5E5E5E;">%*%</span> (e<span class="sc" style="color: #5E5E5E;">$</span>vectors <span class="sc" style="color: #5E5E5E;">%*%</span> V <span class="sc" style="color: #5E5E5E;">%*%</span> <span class="fu" style="color: #4758AB;">t</span>(e<span class="sc" style="color: #5E5E5E;">$</span>vectors)) <span class="sc" style="color: #5E5E5E;">+</span> p</span>
<span id="cb4-12"><span class="fu" style="color: #4758AB;">colnames</span>(level_curve_1) <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"X1"</span>, <span class="st" style="color: #20794D;">"X2"</span>)</span>
<span id="cb4-13">level_curve_1 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">as_tibble</span>(level_curve_1)</span>
<span id="cb4-14">level_curve_2 <span class="ot" style="color: #003B4F;">&lt;-</span> sigma_2<span class="sc" style="color: #5E5E5E;">*</span>R <span class="sc" style="color: #5E5E5E;">%*%</span> (e<span class="sc" style="color: #5E5E5E;">$</span>vectors <span class="sc" style="color: #5E5E5E;">%*%</span> V <span class="sc" style="color: #5E5E5E;">%*%</span> <span class="fu" style="color: #4758AB;">t</span>(e<span class="sc" style="color: #5E5E5E;">$</span>vectors)) <span class="sc" style="color: #5E5E5E;">+</span> p</span>
<span id="cb4-15"><span class="fu" style="color: #4758AB;">colnames</span>(level_curve_2) <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="st" style="color: #20794D;">"X1"</span>, <span class="st" style="color: #20794D;">"X2"</span>)</span>
<span id="cb4-16">level_curve_2 <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">as_tibble</span>(level_curve_2)</span>
<span id="cb4-17"></span>
<span id="cb4-18">joint <span class="ot" style="color: #003B4F;">&lt;-</span>sims <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-19">  <span class="fu" style="color: #4758AB;">select</span>(sim, K, X) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-20">  <span class="fu" style="color: #4758AB;">pivot_wider</span>(<span class="at" style="color: #657422;">names_from=</span><span class="st" style="color: #20794D;">'K'</span>, <span class="at" style="color: #657422;">values_from=</span><span class="st" style="color: #20794D;">'X'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-21">  <span class="fu" style="color: #4758AB;">rename</span>(<span class="at" style="color: #657422;">X1 =</span> <span class="st" style="color: #20794D;">`</span><span class="at" style="color: #657422;">K=1</span><span class="st" style="color: #20794D;">`</span>, <span class="at" style="color: #657422;">X2=</span><span class="st" style="color: #20794D;">`</span><span class="at" style="color: #657422;">K=2</span><span class="st" style="color: #20794D;">`</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-22">  <span class="fu" style="color: #4758AB;">select</span>(<span class="sc" style="color: #5E5E5E;">-</span>sim) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-23">  <span class="fu" style="color: #4758AB;">sample_n</span>(<span class="dv" style="color: #AD0000;">1000</span>)</span>
<span id="cb4-24"></span>
<span id="cb4-25">joint <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb4-26">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(X1, X2))<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-27">  <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">'dark gray'</span>, <span class="at" style="color: #657422;">fill=</span><span class="st" style="color: #20794D;">'gray'</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>, <span class="at" style="color: #657422;">shape=</span><span class="dv" style="color: #AD0000;">21</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-28">  <span class="fu" style="color: #4758AB;">geom_path</span>(<span class="at" style="color: #657422;">data=</span>level_curve_1, <span class="fu" style="color: #4758AB;">aes</span>(X1, X2), <span class="at" style="color: #657422;">color =</span> my_blue, <span class="at" style="color: #657422;">linetype=</span><span class="st" style="color: #20794D;">'dashed'</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-29">  <span class="fu" style="color: #4758AB;">geom_path</span>(<span class="at" style="color: #657422;">data=</span>level_curve_2, <span class="fu" style="color: #4758AB;">aes</span>(X1, X2), <span class="at" style="color: #657422;">color =</span> my_blue)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-30">  <span class="fu" style="color: #4758AB;">lims</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">c</span>(.<span class="dv" style="color: #AD0000;">4</span>, .<span class="dv" style="color: #AD0000;">6</span>), <span class="at" style="color: #657422;">y=</span><span class="fu" style="color: #4758AB;">c</span>(.<span class="dv" style="color: #AD0000;">4</span>, .<span class="dv" style="color: #AD0000;">6</span>))<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-31">  <span class="fu" style="color: #4758AB;">theme</span>(</span>
<span id="cb4-32">    <span class="at" style="color: #657422;">panel.grid.major =</span> <span class="fu" style="color: #4758AB;">element_line</span>(),</span>
<span id="cb4-33">    <span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb4-34">  )<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb4-35">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">expression</span>(<span class="fu" style="color: #4758AB;">bar</span>(X)<span class="sc" style="color: #5E5E5E;">^</span>(<span class="dv" style="color: #AD0000;">1</span>)),</span>
<span id="cb4-36">       <span class="at" style="color: #657422;">y=</span><span class="fu" style="color: #4758AB;">expression</span>(<span class="fu" style="color: #4758AB;">bar</span>(X)<span class="sc" style="color: #5E5E5E;">^</span>(<span class="dv" style="color: #AD0000;">2</span>)))</span></code></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-level-curves" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;2: 1000 samples from the density of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Dashed line indicates where region of 95% probability, solid line indicates region of 99% probability.</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-06-gsd/index_files/figure-html/fig-level-curves-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Now that we know the joint sampling distribution for our statistics of interest (namely <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(1)%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BX%7D%5E%7B(2)%7D">), let’s examine when we would reject the null under “peeking” conditions. For brevity, let’s call <img src="https://latex.codecogs.com/png.latex?Z%5E%7B(k)%7D"> the standardized cumulative means. Then we would reject the null under “peeking” conditions if <img src="https://latex.codecogs.com/png.latex?%5CBig%5Cvert%20Z%5E%7B(k)%7D%20%5CBig%5Cvert%20%3E%201.96"> for at least one <img src="https://latex.codecogs.com/png.latex?k=1,%202">. As a probabilistic statement, we want to know</p>
<p><img src="https://latex.codecogs.com/png.latex?%20Pr%5Cleft(%20%5CBig%5Cvert%20Z%5E%7B(1)%7D%20%5CBig%5Cvert%20%3E%201.96%20%5Ccup%201.96%20%3C%20%5CBig%5Cvert%20Z%5E%7B(2)%7D%20%5CBig%5Cvert%20%5Cright)%20%5C%3E.%20"></p>
<p>Because the joint is multivariate normal, we can compute this probability directly. However, I’m just going to simulate it.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># Standardize the MVN by converting covariance matrix into a correlation matrix</span></span>
<span id="cb5-2">D <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">solve</span>(<span class="fu" style="color: #4758AB;">diag</span>(<span class="fu" style="color: #4758AB;">sqrt</span>(<span class="fu" style="color: #4758AB;">diag</span>(sig))))</span>
<span id="cb5-3">cormat <span class="ot" style="color: #003B4F;">&lt;-</span> D <span class="sc" style="color: #5E5E5E;">%*%</span> sig <span class="sc" style="color: #5E5E5E;">%*%</span> D</span>
<span id="cb5-4">Z<span class="ot" style="color: #003B4F;">&lt;-</span> MASS<span class="sc" style="color: #5E5E5E;">::</span><span class="fu" style="color: #4758AB;">mvrnorm</span>((<span class="dv" style="color: #AD0000;">1</span><span class="sc" style="color: #5E5E5E;">/</span><span class="fl" style="color: #AD0000;">0.001</span>)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span>, <span class="fu" style="color: #4758AB;">rep</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">2</span>), cormat)</span>
<span id="cb5-5"></span>
<span id="cb5-6"></span>
<span id="cb5-7">z1 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(Z[, <span class="dv" style="color: #AD0000;">1</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="fl" style="color: #AD0000;">1.96</span></span>
<span id="cb5-8">z2 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(Z[, <span class="dv" style="color: #AD0000;">2</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span><span class="fl" style="color: #AD0000;">1.96</span></span>
<span id="cb5-9"></span>
<span id="cb5-10">fpr <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(z1<span class="sc" style="color: #5E5E5E;">|</span>z2)</span></code></pre></div>
</div>
<p>and we get something like 8.3%. But that doesn’t answer <em>why</em>, that just means I did my algebra correctly. As always, a visualization might help. take a look at Figure&nbsp;3. The shaded regions show the areas where the null would be rejected. These are the areas we would make a false positive. The dots indicate the standardized draws from the density of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Remember, this distribution <em>is</em> the null distribution for our GSD – these are draws from <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> when <img src="https://latex.codecogs.com/png.latex?H_0"> is true. And now here is the important part…</p>
<blockquote class="blockquote">
<p>The shaded region depends on critical values we use for each test in the sequence. If we naively use <img src="https://latex.codecogs.com/png.latex?Z_%7B1-%5Calpha/2%7D"> as the critical value for each group as in “peeking” conditions, then the shaded region is too big!</p>
</blockquote>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1">sims <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-2">  <span class="fu" style="color: #4758AB;">select</span>(sim, K, Z) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-3">  <span class="fu" style="color: #4758AB;">pivot_wider</span>(<span class="at" style="color: #657422;">names_from=</span><span class="st" style="color: #20794D;">'K'</span>, <span class="at" style="color: #657422;">values_from=</span><span class="st" style="color: #20794D;">'Z'</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-4">  <span class="fu" style="color: #4758AB;">rename</span>(<span class="at" style="color: #657422;">Z1 =</span> <span class="st" style="color: #20794D;">`</span><span class="at" style="color: #657422;">K=1</span><span class="st" style="color: #20794D;">`</span>, <span class="at" style="color: #657422;">Z2=</span><span class="st" style="color: #20794D;">`</span><span class="at" style="color: #657422;">K=2</span><span class="st" style="color: #20794D;">`</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-5">  <span class="fu" style="color: #4758AB;">select</span>(<span class="sc" style="color: #5E5E5E;">-</span>sim) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-6">  <span class="fu" style="color: #4758AB;">sample_n</span>(<span class="dv" style="color: #AD0000;">1000</span>) <span class="sc" style="color: #5E5E5E;">%&gt;%</span> </span>
<span id="cb6-7">  <span class="fu" style="color: #4758AB;">ggplot</span>(<span class="fu" style="color: #4758AB;">aes</span>(Z1, Z2))<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-8">  <span class="fu" style="color: #4758AB;">geom_point</span>(<span class="at" style="color: #657422;">color =</span> <span class="st" style="color: #20794D;">'dark gray'</span>, <span class="at" style="color: #657422;">fill=</span><span class="st" style="color: #20794D;">'gray'</span>, <span class="at" style="color: #657422;">alpha =</span> <span class="fl" style="color: #AD0000;">0.5</span>, <span class="at" style="color: #657422;">shape=</span><span class="dv" style="color: #AD0000;">21</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-9">  <span class="fu" style="color: #4758AB;">scale_x_continuous</span>(<span class="at" style="color: #657422;">limits =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>), <span class="at" style="color: #657422;">expand=</span><span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">0</span>))<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-10">  <span class="fu" style="color: #4758AB;">scale_y_continuous</span>(<span class="at" style="color: #657422;">limits =</span> <span class="fu" style="color: #4758AB;">c</span>(<span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="dv" style="color: #AD0000;">5</span>), <span class="at" style="color: #657422;">expand=</span><span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">0</span>))<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-11">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"rect"</span>, <span class="at" style="color: #657422;">xmin =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">ymin =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">alpha =</span> .<span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">fill=</span>my_blue)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-12">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"rect"</span>, <span class="at" style="color: #657422;">xmin =</span> <span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">ymin =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">alpha =</span> .<span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">fill=</span>my_blue)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-13">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"rect"</span>, <span class="at" style="color: #657422;">xmin =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">ymin =</span> <span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">alpha =</span> .<span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">fill=</span>my_blue)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-14">  <span class="fu" style="color: #4758AB;">annotate</span>(<span class="st" style="color: #20794D;">"rect"</span>, <span class="at" style="color: #657422;">xmin =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">xmax =</span> <span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">ymin =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.96</span>, <span class="at" style="color: #657422;">ymax =</span> <span class="sc" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">alpha =</span> .<span class="dv" style="color: #AD0000;">5</span>, <span class="at" style="color: #657422;">fill=</span>my_blue)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-15">  <span class="fu" style="color: #4758AB;">geom_hline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">yintercept=</span><span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.96</span>), <span class="at" style="color: #657422;">linetype=</span><span class="st" style="color: #20794D;">'dashed'</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-16">  <span class="fu" style="color: #4758AB;">geom_hline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">yintercept=</span><span class="fl" style="color: #AD0000;">1.96</span>), <span class="at" style="color: #657422;">linetype=</span><span class="st" style="color: #20794D;">'dashed'</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-17">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept=</span><span class="sc" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">1.96</span>), <span class="at" style="color: #657422;">linetype=</span><span class="st" style="color: #20794D;">'dashed'</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-18">  <span class="fu" style="color: #4758AB;">geom_vline</span>(<span class="fu" style="color: #4758AB;">aes</span>(<span class="at" style="color: #657422;">xintercept=</span><span class="fl" style="color: #AD0000;">1.96</span>), <span class="at" style="color: #657422;">linetype=</span><span class="st" style="color: #20794D;">'dashed'</span>)<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-19">  <span class="fu" style="color: #4758AB;">theme</span>(</span>
<span id="cb6-20">    <span class="at" style="color: #657422;">panel.grid.major =</span> <span class="fu" style="color: #4758AB;">element_line</span>(),</span>
<span id="cb6-21">    <span class="at" style="color: #657422;">aspect.ratio =</span> <span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb6-22">  )<span class="sc" style="color: #5E5E5E;">+</span></span>
<span id="cb6-23">  <span class="fu" style="color: #4758AB;">labs</span>(<span class="at" style="color: #657422;">x=</span><span class="fu" style="color: #4758AB;">expression</span>(Z<span class="sc" style="color: #5E5E5E;">^</span>(<span class="dv" style="color: #AD0000;">1</span>)),</span>
<span id="cb6-24">       <span class="at" style="color: #657422;">y=</span><span class="fu" style="color: #4758AB;">expression</span>(Z<span class="sc" style="color: #5E5E5E;">^</span>(<span class="dv" style="color: #AD0000;">2</span>)))</span>
<span id="cb6-25"></span>
<span id="cb6-26">find_region <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="cf" style="color: #003B4F;">function</span>(za){</span>
<span id="cb6-27">  z1 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(Z[, <span class="dv" style="color: #AD0000;">1</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span>za</span>
<span id="cb6-28">  z2 <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">abs</span>(Z[, <span class="dv" style="color: #AD0000;">2</span>])<span class="sc" style="color: #5E5E5E;">&gt;</span>za</span>
<span id="cb6-29"></span>
<span id="cb6-30">  fpr <span class="ot" style="color: #003B4F;">&lt;-</span> <span class="fu" style="color: #4758AB;">mean</span>(z1<span class="sc" style="color: #5E5E5E;">|</span>z2)</span>
<span id="cb6-31">  </span>
<span id="cb6-32">  (fpr <span class="sc" style="color: #5E5E5E;">-</span> <span class="fl" style="color: #AD0000;">0.05</span>)<span class="sc" style="color: #5E5E5E;">^</span><span class="dv" style="color: #AD0000;">2</span></span>
<span id="cb6-33">}</span>
<span id="cb6-34"></span>
<span id="cb6-35">result<span class="ot" style="color: #003B4F;">&lt;-</span><span class="fu" style="color: #4758AB;">optimize</span>(find_region, <span class="at" style="color: #657422;">interval=</span><span class="fu" style="color: #4758AB;">c</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">3</span>))</span></code></pre></div>
</details>
<div class="cell-output-display">
<div id="fig-regions" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Standardized draws from the density of <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. Shaded regions indicate where the null hypothesis would be rejected under “peeking” conditions. The shaded region has approximately 8.5% probability mass and represents the false positive rate. We need to select a different region so that the shaded region has probability mass closer to 5%.</figcaption><p></p>
<p><img src="https://dpananos.github.io/posts/2022-07-06-gsd/index_files/figure-html/fig-regions-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<div class="cell">

</div>
<p>That is the <em>why</em>! When we naively just run our test each time we peek, we are defining a region in <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> space which has too much probability. Peeking is fine, you just have to be careful in defining your rejection region in <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> space. Defining a better rejection region isn’t too hard, and we can do it using a numerical search. When we do so, we find that using a critical value of 2.18 results in a type one error closer to the desired 5%. However, we’re implicitly restricted ourselves to having the threshold be the same for each group. That doesn’t have to be the case as we will see eventually.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>We’ve done algebra, and it wasn’t for nothing. It have us insight into exactly what is going on and <em>why</em> the type one error increases under peeking. We also know that there is a way to fix it, we just need to define the shaded region a little more carefully. This will lead us to talk about alpha spending and various alpha spending functions.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<section id="sec-cov" class="level3">
<h3 class="anchored" data-anchor-id="sec-cov">Covariance Calculation</h3>
<p>The diagonals of the covariance matrix <img src="https://latex.codecogs.com/png.latex?%5CSigma"> are simply the variances of the marginal distributions.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CSigma_%7B1,%201%7D%20=%20%5Cdfrac%7Bp(1-p)%7D%7Bn_1%7D%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5CSigma_%7B2,%202%7D%20=%20%5Cdfrac%7Bp(1-p)%7D%7Bn_1%20+%20n_2%7D%20"></p>
<p>What remains is the covariance, which can be obtained with some covariance rules</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cbegin%7Balign%7D%20%5Coperatorname%7BCov%7D%5Cleft(%5Cbar%7BX%7D%5E%7B(1)%7D,%20%5Cbar%7BX%7D%5E%7B(2)%7D%5Cright)%20&amp;=%20%5Coperatorname%7BCov%7D%5Cleft(%5Cbar%7BX%7D_1,%20%5Cdfrac%7Bn_1%5Cbar%7BX%7D_1%20+%20n_2%5Cbar%7BX%7D_2%7D%7Bn_1%20+%20n_2%7D%5Cright)%5C%5C%0A&amp;=%5Cdfrac%7Bn_1%7D%7Bn_1%20+%20n_2%7D%5Coperatorname%7BVar%7D(%5Cbar%7BX_1%7D)%20+%20%5Cdfrac%7Bn_2%7D%7Bn_1+n_2%7D%5Coperatorname%7BCov%7D(%5Cbar%7BX%7D_1,%20%5Cbar%7BX%7D_2)%0A%5Cend%7Balign%7D"></p>
<p>Since the groups are independent, the sample means are also independent (but the cumlative means are not). Meaning <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7BCov%7D(%5Cbar%7BX%7D_1,%20%5Cbar%7BX%7D_2)=0"> so</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Coperatorname%7BCov%7D%5Cleft(%5Cbar%7BX%7D%5E%7B(1)%7D,%20%5Cbar%7BX%7D%5E%7B(2)%7D%5Cright)%20=%20%5Cdfrac%7Bp(1-p)%7D%7Bn_1%20+%20n_2%7D%20"></p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>See the Section&nbsp;6.1 for a calculation↩︎</p></li>
</ol>
</section></div> ]]></description>
  <category>Statistics</category>
  <category>AB Testing</category>
  <guid>https://dpananos.github.io/posts/2022-07-06-gsd/index.html</guid>
  <pubDate>Wed, 06 Jul 2022 04:00:00 GMT</pubDate>
  <media:content url="https://dpananos.github.io/posts/2022-07-06-gsd/joint_dist.png" medium="image" type="image/png" height="144" width="144"/>
</item>
<item>
  <title>This Is A Quarto Blog</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-06-22-new-blog/index.html</link>
  <description><![CDATA[ 




<p>This is a quarto blog.</p>
<p>That means I can write code in either R or python directly in the blog post and have it execute. So when you see something like</p>
<div class="cell" data-fig-dpi="240">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-3"></span>
<span id="cb1-4">t <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb1-5">plt.plot(t, np.sin(<span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>np.pi<span class="op" style="color: #5E5E5E;">*</span>t), color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'k'</span>)</span>
<span id="cb1-6">plt.plot(t, np.cos(<span class="dv" style="color: #AD0000;">2</span><span class="op" style="color: #5E5E5E;">*</span>np.pi<span class="op" style="color: #5E5E5E;">*</span>t), color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'red'</span>)</span>
<span id="cb1-7">plt.title(<span class="st" style="color: #20794D;">"Here is a plot"</span>)</span></code></pre></div>
<div class="cell-output-display">
<div id="fig-trig-py" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://dpananos.github.io/posts/2022-06-22-new-blog/index_files/figure-html/fig-trig-py-1.png" class="img-fluid figure-img" width="480"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Whoa, check it out!</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>That is the code that is actually executed. That means the blog is more reproducible and will have fewer errors. It also means you can go directly to <a href="https://github.com/Dpananos/dpananos.github.io">the repo for my blog</a> and clone the post to start tinkering. No more linking to other gitrepos, no more copying and pasting code with errors.</p>
<p>Did I mention I can write both R and python?</p>
<div class="cell" data-fig-dpi="240">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">t <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">seq</span>(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">0.01</span>)</span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;">plot</span>(t, <span class="fu" style="color: #4758AB;">sin</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>pi<span class="sc" style="color: #5E5E5E;">*</span>t), <span class="at" style="color: #657422;">main=</span><span class="st" style="color: #20794D;">'Here is another plot!'</span>, <span class="at" style="color: #657422;">xlab=</span><span class="st" style="color: #20794D;">''</span>, <span class="at" style="color: #657422;">ylab=</span><span class="st" style="color: #20794D;">''</span>, <span class="at" style="color: #657422;">type=</span><span class="st" style="color: #20794D;">'l'</span>)</span>
<span id="cb2-3"><span class="fu" style="color: #4758AB;">lines</span>(t, <span class="fu" style="color: #4758AB;">cos</span>(<span class="dv" style="color: #AD0000;">2</span><span class="sc" style="color: #5E5E5E;">*</span>pi<span class="sc" style="color: #5E5E5E;">*</span>t), <span class="at" style="color: #657422;">col=</span><span class="st" style="color: #20794D;">'red'</span>)</span></code></pre></div>
<div class="cell-output-display">
<div id="fig-trig-r" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://dpananos.github.io/posts/2022-06-22-new-blog/index_files/figure-html/fig-trig-r-3.png" class="img-fluid figure-img" width="480"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Whoa, check it out again!</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>I can also reference figures (like Figure&nbsp;2 and Figure&nbsp;1)</p>



 ]]></description>
  <category>News</category>
  <guid>https://dpananos.github.io/posts/2022-06-22-new-blog/index.html</guid>
  <pubDate>Wed, 22 Jun 2022 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Flippin’ Fun!</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2022-05-07-flippin/index.html</link>
  <description><![CDATA[ 




<p>I wrote an <a href="https://stats.stackexchange.com/a/574359/111259">answer</a> to a question about sequences of coin flips a couple days back that I was quite chuffed with. In short, the question asked for statistical ways to determine if a sequence of coin flips was from an unbiased coin or a human trying to appear random. The resulting model turned into a fun game <a href="https://twitter.com/PhDemetri/status/1523034006626856960">on twitter</a> centered around determining if people who follow me could simulate a sequence of coin flips that looked random (without using a real RNG, or some funny workaround. I have a lot of faith in my twitter followers…maybe too much).</p>
<p>Anyway, then I thought “I should fit a hierarchical model to this data”. So that’s what I’m doing</p>
<section id="initial-model" class="level2">
<h2 class="anchored" data-anchor-id="initial-model">Initial Model</h2>
<p>To understand the hierarchical model, we first need to understand the model I initially built. Let me give you a quick rundown on that.</p>
<p>Let <img src="https://latex.codecogs.com/png.latex?S"> be a sequence of bernoulli experiments so that <img src="https://latex.codecogs.com/png.latex?S_i"> is either 1 or a 0 (a heads or a tails if you wish). The question I answered concerns detecting if a given sequence <img src="https://latex.codecogs.com/png.latex?S"> could have been created by a human (or by a non-random process posing as random). I interpreted that as a call to estimate the lag-1 autocorrelation of the flips. The hypothesis being that humans probably perceive long streaks of one outcome or the other as signs of non-randomness and will intentionally switch the outcome if they feel the run is too long. I initially chose a Bayesian approach because I’m a glutton for punishment and someone else already gave a pretty good answer.</p>
<p>The model is quite straight forward to write down. Let <img src="https://latex.codecogs.com/png.latex?%5Crho"> be the correlation between <img src="https://latex.codecogs.com/png.latex?S_i"> and <img src="https://latex.codecogs.com/png.latex?S_%7Bi+1%7D">, and let <img src="https://latex.codecogs.com/png.latex?q"> be the expected number of heads in the sequence. We can write down the conditional probabilities that we see a 0/1 given the last element in the sequence was a 1/0. Those conditional probabilities are derived <a href="https://stats.stackexchange.com/questions/533217/interpretation-of-the-autocorrelation-of-a-binary-process">here</a> and they are…</p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(1%20%5Cvert%201)%20=%20q%20+%20%5Crho(1-q)%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(1%20%5Cvert%200)%20=%20q(1-%5Crho)%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(0%20%5Cvert%201)%20=%20(1-q)(1-%5Crho)%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20P(0%20%5Cvert%200)%20=%201%20-%20q%20+%20%5Crho%20%5Ccdot%20q%20"></p>
<p>The trick is to then count the subsequences of (1, 1), (1, 0), (0, 1), and (0, 0). Let <img src="https://latex.codecogs.com/png.latex?p_%7Bi%5Cvert%20j%7D%20=%20P(i%20%5Cvert%20j)">. We can then consider the count of each subsequence as multinomial</p>
<p><img src="https://latex.codecogs.com/png.latex?%20y%20%5Csim%20%5Cmbox%7BMultinomial%7D(%5Ctheta)%20%5C%3E.%20"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the multinomial parameter, wherein each element is <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5Bp_%7B1%20%5Cvert%201%7D,%20p_%7B1%5Cvert%200%7D,%20p_%7B0%20%5Cvert%201%7D,%20p_%7B0%5Cvert%200%7D%5D">. Equip this with a uniform prior on both <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?q"> and you’ve got yourself a model.</p>
</section>
<section id="the-stan-code" class="level2">
<h2 class="anchored" data-anchor-id="the-stan-code">The Stan Code</h2>
<p>The Stan code for this model is quite easy to understand. The <code>data</code> block will consist of the counts of each type of subsequence. We can then concatenate those counts into an <code>int</code> of length 4 via the <code>transformed data</code> block. The concatenated counts will be what we pass to the multinomial likelihood.</p>
<pre><code>data{
  int y_1_1; //number of concurrent 1s
  int y_0_1; //number of 0,1 occurences
  int y_1_0; //number of 1,0 occurences
  int y_0_0; //number of concurrent 0s
}
transformed data{
    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};
}</code></pre>
<p>The only parameters we are interested in estimating are the autocorrelation <code>rho</code> and the coin’s bias <code>q</code>.</p>
<pre><code>parameters{
  real&lt;lower=-1, upper=1&gt; rho;
  real&lt;lower=0, upper=1&gt; q;
}</code></pre>
<p>We can derive the probabilities we need via the equations above, and that is a job for the transformed parameters block. We can then concatenate the conditional probabilities into a <code>simplex</code> data type object, <code>theta</code> to pass to the multinomial likelihood. Be careful though, we need to multiply <code>theta</code> by 0.5 since we are working with conditional probabilities. Note <img src="https://latex.codecogs.com/png.latex?p_%7B1%5Cvert%201%7D%20+%20p_%7B0%20%5Cvert%201%20%7D%20+%20p_%7B1%5Cvert%200%7D%20+%20p_%7B0%20%5Cvert%200%20%7D%20=%202">, hence the scaling factor.</p>
<pre><code>transformed parameters{
  real&lt;lower=0, upper=1&gt; prob_1_1 = q + rho*(1-q);
  real&lt;lower=0, upper=1&gt; prob_0_1 = (1-q)*(1-rho);
  real&lt;lower=0, upper=1&gt; prob_1_0 = q*(1-rho);
  real&lt;lower=0, upper=1&gt; prob_0_0 = 1 - q + rho*q;
  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';
}</code></pre>
<p>The model call is then quite simple</p>
<pre><code>model{
  q ~ beta(1, 1);
  rho ~ uniform(-1, 1);
  y ~ multinomial(theta); 
}</code></pre>
<p>and we can even generate new sequences based off the estimated parameters as a sort of posterior predictive check.</p>
<pre><code>generated quantities{
    vector[300] yppc;
    
    yppc[1] = bernoulli_rng(q);
    
    for(i in 2:300){
        if(yppc[i-1]==1){
            yppc[i] = bernoulli_rng(prob_1_1);
        }
        else{
        yppc[i] = bernoulli_rng(prob_1_0);
        }
    }
}</code></pre>
<p>All in all the model is</p>
<pre><code>data{

  int y_1_1; //number of concurrent 1s
  int y_0_1; //number of 0,1 occurences
  int y_1_0; //number of 1,0 occurences
  int y_0_0; //number of concurrent 0s
  
}
transformed data{
    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};
}
parameters{
  real&lt;lower=-1, upper=1&gt; rho;
  real&lt;lower=0, upper=1&gt; q;
}
transformed parameters{
  real&lt;lower=0, upper=1&gt; prob_1_1 = q + rho*(1-q);
  real&lt;lower=0, upper=1&gt; prob_0_1 = (1-q)*(1-rho);
  real&lt;lower=0, upper=1&gt; prob_1_0 = q*(1-rho);
  real&lt;lower=0, upper=1&gt; prob_0_0 = 1 - q + rho*q;
  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';
}
model{
  q ~ beta(1, 1);
  rho ~ uniform(-1, 1);
  y ~ multinomial(theta);
  
}
generated quantities{
    vector[300] yppc;
    
    yppc[1] = bernoulli_rng(q);
    
    for(i in 2:300){
        if(yppc[i-1]==1){
            yppc[i] = bernoulli_rng(prob_1_1);
        }
        else{
        yppc[i] = bernoulli_rng(prob_1_0);
        }
    }
}</code></pre>
</section>
<section id="run-from-python" class="level2">
<h2 class="anchored" data-anchor-id="run-from-python">Run From Python</h2>
<p>With the model written down, all we need to do is add some python code to create the counts of each subsequence and then run the stan model. Here is teh python code I used to create the response tweets for that game I ran on twitter.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="im" style="color: #00769E;">import</span> cmdstanpy</span>
<span id="cb7-2"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb7-3"></span>
<span id="cb7-4">y_1_1 <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span> <span class="co" style="color: #5E5E5E;"># count of (1, 1)</span></span>
<span id="cb7-5">y_0_0 <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span> <span class="co" style="color: #5E5E5E;"># count of (0, 0)</span></span>
<span id="cb7-6">y_0_1 <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span> <span class="co" style="color: #5E5E5E;"># count of (0, 1)</span></span>
<span id="cb7-7">y_1_0 <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">0</span> <span class="co" style="color: #5E5E5E;"># count of (1, 0)</span></span>
<span id="cb7-8"></span>
<span id="cb7-9">sequence <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">list</span>(<span class="st" style="color: #20794D;">'TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT'</span>.upper())</span>
<span id="cb7-10"></span>
<span id="cb7-11"><span class="co" style="color: #5E5E5E;"># Do a rolling window trick I saw somewhere on twitter.</span></span>
<span id="cb7-12"><span class="co" style="color: #5E5E5E;"># This implements a rollowing window of 2</span></span>
<span id="cb7-13"><span class="co" style="color: #5E5E5E;"># In python 3.10, this would be a great use case for match</span></span>
<span id="cb7-14"><span class="cf" style="color: #003B4F;">for</span> pairs <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">zip</span>(sequence[:<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>], sequence[<span class="dv" style="color: #AD0000;">1</span>:]):</span>
<span id="cb7-15">    <span class="cf" style="color: #003B4F;">if</span> pairs <span class="op" style="color: #5E5E5E;">==</span> (<span class="st" style="color: #20794D;">'H'</span>,<span class="st" style="color: #20794D;">'H'</span>):</span>
<span id="cb7-16">        y_1_1 <span class="op" style="color: #5E5E5E;">+=</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-17">    <span class="cf" style="color: #003B4F;">elif</span> pairs <span class="op" style="color: #5E5E5E;">==</span> (<span class="st" style="color: #20794D;">'T'</span>,<span class="st" style="color: #20794D;">'H'</span>):</span>
<span id="cb7-18">        y_0_1 <span class="op" style="color: #5E5E5E;">+=</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-19">    <span class="cf" style="color: #003B4F;">elif</span> pairs <span class="op" style="color: #5E5E5E;">==</span> (<span class="st" style="color: #20794D;">'H'</span>, <span class="st" style="color: #20794D;">'T'</span>):</span>
<span id="cb7-20">        y_1_0 <span class="op" style="color: #5E5E5E;">+=</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-21">    <span class="cf" style="color: #003B4F;">else</span>:</span>
<span id="cb7-22">        y_0_0 <span class="op" style="color: #5E5E5E;">+=</span><span class="dv" style="color: #AD0000;">1</span></span>
<span id="cb7-23"></span>
<span id="cb7-24"><span class="co" style="color: #5E5E5E;"># Write the stan model as a string.  We will then write it to a file</span></span>
<span id="cb7-25">stan_code <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'''</span></span>
<span id="cb7-26"><span class="st" style="color: #20794D;">data{</span></span>
<span id="cb7-27"></span>
<span id="cb7-28"><span class="st" style="color: #20794D;">  int y_1_1; //number of concurrent 1s</span></span>
<span id="cb7-29"><span class="st" style="color: #20794D;">  int y_0_1; //number of 0,1 occurences</span></span>
<span id="cb7-30"><span class="st" style="color: #20794D;">  int y_1_0; //number of 1,0 occurences</span></span>
<span id="cb7-31"><span class="st" style="color: #20794D;">  int y_0_0; //number of concurrent 0s</span></span>
<span id="cb7-32"><span class="st" style="color: #20794D;">  </span></span>
<span id="cb7-33"><span class="st" style="color: #20794D;">}</span></span>
<span id="cb7-34"><span class="st" style="color: #20794D;">transformed data{</span></span>
<span id="cb7-35"><span class="st" style="color: #20794D;">    int y[4] = {y_1_1, y_0_1, y_1_0, y_0_0};</span></span>
<span id="cb7-36"><span class="st" style="color: #20794D;">}</span></span>
<span id="cb7-37"><span class="st" style="color: #20794D;">parameters{</span></span>
<span id="cb7-38"><span class="st" style="color: #20794D;">  real&lt;lower=-1, upper=1&gt; rho;</span></span>
<span id="cb7-39"><span class="st" style="color: #20794D;">  real&lt;lower=0, upper=1&gt; q;</span></span>
<span id="cb7-40"><span class="st" style="color: #20794D;">}</span></span>
<span id="cb7-41"><span class="st" style="color: #20794D;">transformed parameters{</span></span>
<span id="cb7-42"><span class="st" style="color: #20794D;">  real&lt;lower=0, upper=1&gt; prob_1_1 = q + rho*(1-q);</span></span>
<span id="cb7-43"><span class="st" style="color: #20794D;">  real&lt;lower=0, upper=1&gt; prob_0_1 = (1-q)*(1-rho);</span></span>
<span id="cb7-44"><span class="st" style="color: #20794D;">  real&lt;lower=0, upper=1&gt; prob_1_0 = q*(1-rho);</span></span>
<span id="cb7-45"><span class="st" style="color: #20794D;">  real&lt;lower=0, upper=1&gt; prob_0_0 = 1 - q + rho*q;</span></span>
<span id="cb7-46"><span class="st" style="color: #20794D;">  simplex[4] theta = 0.5*[prob_1_1, prob_0_1, prob_1_0, prob_0_0 ]';</span></span>
<span id="cb7-47"><span class="st" style="color: #20794D;">}</span></span>
<span id="cb7-48"><span class="st" style="color: #20794D;">model{</span></span>
<span id="cb7-49"><span class="st" style="color: #20794D;">  q ~ beta(1, 1);</span></span>
<span id="cb7-50"><span class="st" style="color: #20794D;">  rho ~ uniform(-1, 1);</span></span>
<span id="cb7-51"><span class="st" style="color: #20794D;">  y ~ multinomial(theta);</span></span>
<span id="cb7-52"><span class="st" style="color: #20794D;">  </span></span>
<span id="cb7-53"><span class="st" style="color: #20794D;">}</span></span>
<span id="cb7-54"><span class="st" style="color: #20794D;">generated quantities{</span></span>
<span id="cb7-55"><span class="st" style="color: #20794D;">    vector[300] yppc;</span></span>
<span id="cb7-56"><span class="st" style="color: #20794D;">    </span></span>
<span id="cb7-57"><span class="st" style="color: #20794D;">    yppc[1] = bernoulli_rng(q);</span></span>
<span id="cb7-58"><span class="st" style="color: #20794D;">    </span></span>
<span id="cb7-59"><span class="st" style="color: #20794D;">    for(i in 2:300){</span></span>
<span id="cb7-60"><span class="st" style="color: #20794D;">        if(yppc[i-1]==1){</span></span>
<span id="cb7-61"><span class="st" style="color: #20794D;">            yppc[i] = bernoulli_rng(prob_1_1);</span></span>
<span id="cb7-62"><span class="st" style="color: #20794D;">        }</span></span>
<span id="cb7-63"><span class="st" style="color: #20794D;">        else{</span></span>
<span id="cb7-64"><span class="st" style="color: #20794D;">        yppc[i] = bernoulli_rng(prob_1_0);</span></span>
<span id="cb7-65"><span class="st" style="color: #20794D;">        }</span></span>
<span id="cb7-66"><span class="st" style="color: #20794D;">    }</span></span>
<span id="cb7-67"><span class="st" style="color: #20794D;">}</span></span>
<span id="cb7-68"><span class="st" style="color: #20794D;">'''</span></span>
<span id="cb7-69"></span>
<span id="cb7-70"></span>
<span id="cb7-71"></span>
<span id="cb7-72"><span class="co" style="color: #5E5E5E;"># Write the model to a temp file</span></span>
<span id="cb7-73"><span class="cf" style="color: #003B4F;">with</span> <span class="bu" style="color: null;">open</span>(<span class="st" style="color: #20794D;">'model_file.stan'</span>, <span class="st" style="color: #20794D;">'w'</span>) <span class="im" style="color: #00769E;">as</span> model_file:</span>
<span id="cb7-74">    model_file.write(stan_code)</span>
<span id="cb7-75">    </span>
<span id="cb7-76"><span class="co" style="color: #5E5E5E;"># Compile the model</span></span>
<span id="cb7-77">model <span class="op" style="color: #5E5E5E;">=</span> cmdstanpy.CmdStanModel(stan_file<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'model_file.stan'</span>)</span>
<span id="cb7-78"></span>
<span id="cb7-79"><span class="co" style="color: #5E5E5E;"># data to pass to Stan</span></span>
<span id="cb7-80">data <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(y_1_1 <span class="op" style="color: #5E5E5E;">=</span> y_1_1, y_0_0 <span class="op" style="color: #5E5E5E;">=</span> y_0_0, y_0_1 <span class="op" style="color: #5E5E5E;">=</span> y_0_1, y_1_0 <span class="op" style="color: #5E5E5E;">=</span> y_1_0)</span>
<span id="cb7-81"></span>
<span id="cb7-82"><span class="co" style="color: #5E5E5E;"># Plotting stuff.</span></span>
<span id="cb7-83">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(dpi <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">120</span>, ncols<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>, figsize <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">15</span>, <span class="dv" style="color: #AD0000;">5</span>))</span>
<span id="cb7-84"></span>
<span id="cb7-85">ax[<span class="dv" style="color: #AD0000;">0</span>].set_title(<span class="st" style="color: #20794D;">'Auto-correlation'</span>)</span>
<span id="cb7-86">ax[<span class="dv" style="color: #AD0000;">1</span>].set_title(<span class="st" style="color: #20794D;">'Bias'</span>)</span>
<span id="cb7-87"></span>
<span id="cb7-88">ax[<span class="dv" style="color: #AD0000;">0</span>].set_xlim(<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb7-89">ax[<span class="dv" style="color: #AD0000;">1</span>].set_xlim(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb7-90"></span>
<span id="cb7-91">ax[<span class="dv" style="color: #AD0000;">0</span>].axvline(<span class="dv" style="color: #AD0000;">0</span>, color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'red'</span>)</span>
<span id="cb7-92">ax[<span class="dv" style="color: #AD0000;">1</span>].axvline(<span class="fl" style="color: #AD0000;">0.5</span>, color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'red'</span>)</span>
<span id="cb7-93"></span>
<span id="cb7-94">ax[<span class="dv" style="color: #AD0000;">0</span>].annotate(<span class="st" style="color: #20794D;">'Uncorrelated Flips'</span>, xy<span class="op" style="color: #5E5E5E;">=</span>(<span class="fl" style="color: #AD0000;">0.475</span>, <span class="fl" style="color: #AD0000;">0.5</span>), xycoords<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'axes fraction'</span>, rotation <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">90</span>)</span>
<span id="cb7-95">ax[<span class="dv" style="color: #AD0000;">1</span>].annotate(<span class="st" style="color: #20794D;">'Unbiased Flips'</span>, xy<span class="op" style="color: #5E5E5E;">=</span>(<span class="fl" style="color: #AD0000;">0.475</span>, <span class="fl" style="color: #AD0000;">0.5</span>), xycoords<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'axes fraction'</span>, rotation <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">90</span>)</span>
<span id="cb7-96"></span>
<span id="cb7-97"><span class="co" style="color: #5E5E5E;"># MCMC go brrrr</span></span>
<span id="cb7-98">fit <span class="op" style="color: #5E5E5E;">=</span> model.sample(data)</span>
<span id="cb7-99"></span>
<span id="cb7-100">ax[<span class="dv" style="color: #AD0000;">0</span>].hist(fit.stan_variable(<span class="st" style="color: #20794D;">'rho'</span>), edgecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'k'</span>, alpha <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb7-101">ax[<span class="dv" style="color: #AD0000;">1</span>].hist(fit.stan_variable(<span class="st" style="color: #20794D;">'q'</span>), edgecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'k'</span>, alpha <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb7-102"></span>
<span id="cb7-103">autocorr <span class="op" style="color: #5E5E5E;">=</span> fit.stan_variable(<span class="st" style="color: #20794D;">'rho'</span>).mean()</span>
<span id="cb7-104">bias <span class="op" style="color: #5E5E5E;">=</span> fit.stan_variable(<span class="st" style="color: #20794D;">'q'</span>).mean()</span>
<span id="cb7-105"></span>
<span id="cb7-106">tweet <span class="op" style="color: #5E5E5E;">=</span> <span class="ss" style="color: #20794D;">f"Your flips have an expected correlation of </span><span class="sc" style="color: #5E5E5E;">{</span>autocorr<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;"> and your coin's bias is about </span><span class="sc" style="color: #5E5E5E;">{</span>bias<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">"</span></span>
<span id="cb7-107"></span>
<span id="cb7-108"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Your sequence was </span><span class="sc" style="color: #5E5E5E;">{</span><span class="st" style="color: #20794D;">''</span><span class="sc" style="color: #5E5E5E;">.</span>join(sequence)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb7-109"><span class="bu" style="color: null;">print</span>(tweet)</span></code></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:cmdstanpy:compiling stan file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan to exe file /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:cmdstanpy:compiled model executable: /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:cmdstanpy:Stan compiler has produced 1 warnings:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>WARNING:cmdstanpy:
--- Translating Stan model to C++ code ---
bin/stanc  --o=/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan
Warning in '/Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.stan', line 11, column 4: Declaration
    of arrays by placing brackets after a variable name is deprecated and
    will be removed in Stan 2.32.0. Instead use the array keyword before the
    type. This can be changed automatically using the auto-format flag to
    stanc

--- Compiling, linking C++ code ---
clang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS          -c -include-pch stan/src/stan/model/model_header.hpp.gch -x c++ -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.hpp
clang++ -std=c++1y -Wno-unknown-warning-option -Wno-tautological-compare -Wno-sign-compare -D_REENTRANT -Wno-ignored-attributes      -I stan/lib/stan_math/lib/tbb_2020.3/include    -O3 -I src -I stan/src -I lib/rapidjson_1.1.0/ -I lib/CLI11-1.9.1/ -I stan/lib/stan_math/ -I stan/lib/stan_math/lib/eigen_3.3.9 -I stan/lib/stan_math/lib/boost_1.75.0 -I stan/lib/stan_math/lib/sundials_6.0.0/include -I stan/lib/stan_math/lib/sundials_6.0.0/src/sundials    -DBOOST_DISABLE_ASSERTS                -Wl,-L,"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb" -Wl,-rpath,"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb"      /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o src/cmdstan/main.o        -Wl,-L,"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb" -Wl,-rpath,"/Users/demetri/.cmdstan/cmdstan-2.29.2/stan/lib/stan_math/lib/tbb"   stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_nvecserial.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_cvodes.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_idas.a stan/lib/stan_math/lib/sundials_6.0.0/lib/libsundials_kinsol.a  stan/lib/stan_math/lib/tbb/libtbb.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc.dylib stan/lib/stan_math/lib/tbb/libtbbmalloc_proxy.dylib -o /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file
rm -f /Users/demetri/Documents/dpananos.github.io/posts/2022-05-07-flippin/model_file.o
</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:cmdstanpy:CmdStan start processing</code></pre>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"8b4197ff587543abbeaee330aeba92ed","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"97e61edfebdc429eb36afe8ac0f512b1","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"e78cdbeb72ba4ac2a7d8ddc90e504085","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-display">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id":"53c4fd9502654c479be23e9d7e0663e8","version_major":2,"version_minor":0}
</script>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                                </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                                </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                                </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                                </code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>INFO:cmdstanpy:CmdStan done processing.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Your sequence was TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHTTHHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT
Your flips have an expected correlation of -0.36 and your coin's bias is about 0.49</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://dpananos.github.io/posts/2022-05-07-flippin/index_files/figure-html/cell-2-output-17.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="stack-layerswait-thats-a-deep-learning-thing" class="level2">
<h2 class="anchored" data-anchor-id="stack-layerswait-thats-a-deep-learning-thing">Stack Layers…Wait, That’s a Deep Learning Thing</h2>
<p>Now it’s time to write a hierarchical model, and for that we need to be a little more careful. I initially thought I could just put priors on the population level autocorrelation and bias, but I quickly ran into a problem there, which I will illustrate below.</p>
<p>Suppose the coin’s bias is 0 (we always get tails). Could the autocorrelation be -1? No, it couldn’t be, because that would mean our next flip would have to be a 1, but the coin’s bias is 0! This illustrates some nuance to the problem I had failed to consider but luckily did not suffer from. The autocorrelation for two binary random variables, <img src="https://latex.codecogs.com/png.latex?X,%20Y">, is defined as</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Crho%20=%20%5Cdfrac%7B%5Calpha%20-%20q%7D%7Bq(1-q)%7D%20"></p>
<p>Where <img src="https://latex.codecogs.com/png.latex?E(XY)%20=%20%5Calpha">. You see, <img src="https://latex.codecogs.com/png.latex?%5Calpha"> can only be so big depending on the value of <img src="https://latex.codecogs.com/png.latex?q">, and if you place uniform priors on both <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?q"> you can quickly get conditional probabilities outside the unit interval. That’s exactly what happened, and I was banging my head against the wall for a night trying to figure out the bounds on <img src="https://latex.codecogs.com/png.latex?%5Crho"> given <img src="https://latex.codecogs.com/png.latex?q"> and it quickly became a mess.</p>
<p>There is another way. Rather than place priors on <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?q">, we could place priors on the multinomial parameter and then do algebra (two equations, two unknowns) to find out expressions for <img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?%5Crho"> in terms of <img src="https://latex.codecogs.com/png.latex?p_%7B1%5Cvert1%7D"> and <img src="https://latex.codecogs.com/png.latex?p_%7B1%5Cvert0%7D">. This isn’t ideal, because I have very good prior information on what <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?q"> should be, not on what <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> should be. Whatever, let’s proceed and see how our priors look like with a prior predictive check.</p>
<p>The model is actually simpler to write in Stan than the previous model. We will place a Dirichlet prior on the multinomial parameters (one for each person who responded with a sequence), and then each sequence is multinomial with that multinomial parameter.</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctheta_j%20%5Csim%20%5Cmbox%7BDirichlet%7D(%5Calpha)%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20y_j%20%5Csim%20%5Cmbox%7BMultinomial%7D(%5Ctheta_j)%20%20"></p>
<p>The quantities we care about can be expressed in terms of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Crho%20=%20p_%7B1%5Cvert%201%7D%20-%20p_%7B1%5Cvert%200%7D%20%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20q%20=%20%5Cdfrac%7B2p_%7B1%5Cvert%200%7D%7D%7B1%20-%20p_%7B1%5Cvert%201%7D%20+%20p_%7B1%5Cvert%200%7D%7D"></p>
<p>Here is the model in Stan</p>
<pre><code>data{

    int N; //How many sequences do we have
    int y[N, 4]; // matrix of counts of co-occurences of (1,1), (1,0), (0, 1), (0,0)
    int do_sample; // Flag to do a prior predictive check

}
parameters{
    vector&lt;lower=0&gt;[4] a;
    simplex[4] theta[N];
}
model{
  
  a ~ cauchy(0, 2.5);
  
  if(do_sample&gt;0){
      for(i in 1:N){
          theta[i] ~ dirichlet(a);
          y[i] ~ multinomial(theta[i]);
      }
    }
  
}
generated quantities{

    vector[4] theta_ppc = dirichlet_rng(a);
    real rho = theta_ppc[1] - theta_ppc[2];
    real q = 2* theta_ppc[2]/(1 - theta_ppc[1] + theta_ppc[2]); 
    
    real yppc[N, 4];
    
    for(i in 1:N){
        yppc[i] = multinomial_rng(theta[i], sum(y[i]));
    }
}</code></pre>
<p>Shown below are the priors for the autocorrelation and bias based on the priors I’ve used. They are a little too uncertain for my liking. Humans are pretty smart, and I don’t expect for the population average bias to be very far from 0.5. I would prefer that the prior for <img src="https://latex.codecogs.com/png.latex?q"> be very tightly centered around 0.5, and that the prior for <img src="https://latex.codecogs.com/png.latex?%5Crho"> be tightly centered on 0, but that’s life. The model runs the 76 sequences in about 12 seconds (4 chains, 1000 warmups, 1000 samples) and diagnostics don’t indicate any pathological behavior. Let’s look at the joint posterior.</p>
<div id="fig-distributions" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-prior" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://dpananos.github.io/posts/2022-05-07-flippin/priors.png" class="img-fluid figure-img" data-ref-parent="fig-distributions"></p>
<p></p><figcaption class="figure-caption">(a) Priors</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-hanno" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="https://dpananos.github.io/posts/2022-05-07-flippin/posterior.png" class="img-fluid figure-img" data-ref-parent="fig-distributions"></p>
<p></p><figcaption class="figure-caption">(b) Posteriors</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Prior and Posterior distributions for the model.</figcaption><p></p>
</figure>
</div>
<p>The take home here is that the sequences are largely consistent with an unbiased and uncorrelated coin. The expected correlation is negative (-0.05) meaning humans are more likely to switch from heads to tails, or tails to heads, and the expected bias is 0.53 meaning people seem to favor heads for some reason. The results are largely unsurprising, and I really wish I could place priors on <img src="https://latex.codecogs.com/png.latex?%5Crho"> and <img src="https://latex.codecogs.com/png.latex?q"> directly so that my model really does reflect the state of my knowledge, but this is good enough for now.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>I’ve been thinking about this problem for a while after I watched a talk by Gelman where he mentioned estimating the autocorrelation between bernoulli experiments in passing (he was talking about model criticism and offering examples of other things to check our model with). I’m moderately happy with the hierarchical model, and the results make a lot of sense. Humans are pretty smart, and we have an intuitive sense for what random looks like. I’m willing to bet that if people submitted longer sequences, we would have a more precise estimate of the bias/correlation.</p>
<p>One thing I’ve shirked is really detailed model criticism, though I have an idea of how I would do that. In that answer on cross validated, COOLSerdash posts a REALLY COOL way to visualize the sequence data. They plot the number of runs (sequences of consecutive heads or tails) against the longest run in the sequence. I think this would make for an excellent way to check that our model has learned the correct autocorrelation for each individual who participated in the game (though I think the sequences were too short to have a precise estimate).</p>


</section>

 ]]></description>
  <category>Bayes</category>
  <category>Stan</category>
  <category>Python</category>
  <guid>https://dpananos.github.io/posts/2022-05-07-flippin/index.html</guid>
  <pubDate>Sat, 07 May 2022 04:00:00 GMT</pubDate>
  <media:content url="https://dpananos.github.io/posts/2022-05-07-flippin/posterior.png" medium="image" type="image/png" height="96" width="144"/>
</item>
<item>
  <title>Hacking Sklearn To Do The Optimism Corrected Bootstrap</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2021-11-23-bootstrap/index.html</link>
  <description><![CDATA[ 




<p>Its late, I can’t sleep, so I’m writing a blog post about the optimism corrected bootstrap.</p>
<p>In case you don’t know, epidemiology/biostatistics people working on prediction models like to validate their models in a slightly different way than your run-in-the-mill data scientist. Now, it should be unsurprising that <a href="https://twitter.com/GaelVaroquaux/status/1293818409197731840">this has generated some discussion</a> between ML people and epi/biostats people, but I’m going to ignore this for now. I’m going to assume you have good reason for wanting to do the optimism corrected bootstrap in python, especially with sklearn, and if you don’t and want to discuss the pros and cons fo the method instead then lalalalalala I can’t hear you.</p>
<section id="the-optimism-corrected-bootstrap-in-7-steps" class="level2">
<h2 class="anchored" data-anchor-id="the-optimism-corrected-bootstrap-in-7-steps">The Optimism Corrected Bootstrap in 7 Steps</h2>
<p>As a primer, you might want to tread Alex Hayes’ <a href="https://www.alexpghayes.com/blog/predictive-performance-via-bootstrap-variants/">pretty good blog post about variants of the bootstrap</a> for predictive performance. It is more mathy than I care to be right now and in R should that be your thing.</p>
<p>To do the optimism corrected bootstrap, follow these 7 steps as found in <a href="https://link.springer.com/book/10.1007/978-0-387-77244-8">Ewout W. Steyerberg’s <em>Clinical Prediction Models</em></a>.</p>
<ol type="1">
<li><p>Construct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model.</p></li>
<li><p>Draw a bootstrap sample (Sample*) with replacement from the original sample.</p></li>
<li><p>Construct a model (Model<em>) in Sample</em>, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample.</p></li>
<li><p>Apply Model* to the original sample without any modification to determine the test performance.</p></li>
<li><p>Calculate the optimism as the difference between bootstrap performance and test performance.</p></li>
<li><p>Repeat steps 1–4 many times, at least 200, to obtain a stable mean estimate of the optimism.</p></li>
<li><p>Subtract the mean optimism estimate (step 6) from the apparent performance (step 1) to obtain the optimism-corrected performance estimate.</p></li>
</ol>
<p>This procedure is very straight forward, and could easily be coded up from scratch, but I want to use as much existing code as I can and put sklearn on my resume, so let’s talk about what tools exist in sklearn to do cross validation and how we could use them to perform these steps.</p>
</section>
<section id="cross-validation-in-sklearn" class="level2">
<h2 class="anchored" data-anchor-id="cross-validation-in-sklearn">Cross Validation in Sklearn</h2>
<p>When you pass arguments like <code>cv=5</code> in sklearn’s many functions, what you’re really doing is passing <code>5</code> to <code>sklearn.model_selection.KFold</code>. See <a href="https://github.com/scikit-learn/scikit-learn/blob/0d378913b/sklearn/model_selection/_validation.py#L48"><code>sklearn.model_selection.cross_validate</code></a> which calls a function called <a href="https://github.com/scikit-learn/scikit-learn/blob/0d378913be6d7e485b792ea36e9268be31ed52d0/sklearn/model_selection/_split.py#L2262">‘check_cv’</a> to verify this. <code>KFold.split</code> returns a generator, which when passed to <code>next</code> yields a pair of train and test indicides. The inner workings of <code>KFold</code> might look something like</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(number_folds):</span>
<span id="cb1-2">    train_ix <span class="op" style="color: #5E5E5E;">=</span> make_train_ix()</span>
<span id="cb1-3">    test_ix <span class="op" style="color: #5E5E5E;">=</span> make_test_ix()</span>
<span id="cb1-4">    <span class="cf" style="color: #003B4F;">yield</span> (trian_ix, test_ix)</span></code></pre></div>
<p>Those incidies are used to slice <code>X</code> and <code>y</code> to do the cross validation. So, if we are going to hack sklearn to do the optimisim corrected bootstrap for us, we really just need to write a generator to give me a bunch of indicies. According to step 2 and 3 above, the train indicies need to be resamples of <code>np.arange(len(X))</code> (ask yourself “why?”). According to step 4, the test indicies need to be <code>np.arnge(len(X))</code> (again….”why?“).</p>
<p>Once we have a generator to do give us our indicies, we can use <code>sklearn.model_selection.cross_validate</code> to fit models on the resampled data and predict on the original sample (step 4). If we pass <code>return_train_score=True</code> to <code>cross_validate</code> we can get the bootstrap performances as well as the test performances (step 5). All we need to do then is calculate the average difference between the two (step 6) and then add this quantity to the apparent performance we got from step 1.</p>
<p>That all sounds very complex, but the code is decieptively simple.</p>
</section>
<section id="the-code-i-know-you-skipped-here-dont-lie" class="level2">
<h2 class="anchored" data-anchor-id="the-code-i-know-you-skipped-here-dont-lie">The Code (I Know You Skipped Here, Don’t Lie)</h2>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb2-2"><span class="im" style="color: #00769E;">from</span> numpy.core.fromnumeric <span class="im" style="color: #00769E;">import</span> mean</span>
<span id="cb2-3"><span class="im" style="color: #00769E;">from</span> sklearn.model_selection <span class="im" style="color: #00769E;">import</span> cross_validate, RepeatedKFold</span>
<span id="cb2-4"><span class="im" style="color: #00769E;">from</span> sklearn.metrics <span class="im" style="color: #00769E;">import</span> mean_squared_error, make_scorer</span>
<span id="cb2-5"><span class="im" style="color: #00769E;">from</span> sklearn.linear_model <span class="im" style="color: #00769E;">import</span> LinearRegression</span>
<span id="cb2-6"><span class="im" style="color: #00769E;">from</span> sklearn.datasets <span class="im" style="color: #00769E;">import</span> load_diabetes</span>
<span id="cb2-7"><span class="im" style="color: #00769E;">from</span> sklearn.utils <span class="im" style="color: #00769E;">import</span> resample</span>
<span id="cb2-8"></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;"># Need some data to predict with</span></span>
<span id="cb2-10">data <span class="op" style="color: #5E5E5E;">=</span> load_diabetes()</span>
<span id="cb2-11">X, y <span class="op" style="color: #5E5E5E;">=</span> data[<span class="st" style="color: #20794D;">'data'</span>], data[<span class="st" style="color: #20794D;">'target'</span>]</span>
<span id="cb2-12"></span>
<span id="cb2-13"><span class="kw" style="color: #003B4F;">class</span> OptimisimBootstrap():</span>
<span id="cb2-14"></span>
<span id="cb2-15">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, n_bootstraps):</span>
<span id="cb2-16"></span>
<span id="cb2-17">        <span class="va" style="color: #111111;">self</span>.n_bootstraps <span class="op" style="color: #5E5E5E;">=</span> n_bootstraps</span>
<span id="cb2-18"></span>
<span id="cb2-19">    <span class="kw" style="color: #003B4F;">def</span> split(<span class="va" style="color: #111111;">self</span>, X, y,<span class="op" style="color: #5E5E5E;">*</span>_):</span>
<span id="cb2-20"></span>
<span id="cb2-21">        n <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">len</span>(X)</span>
<span id="cb2-22">        test_ix <span class="op" style="color: #5E5E5E;">=</span> np.arange(n)</span>
<span id="cb2-23"></span>
<span id="cb2-24">        <span class="cf" style="color: #003B4F;">for</span> _ <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="va" style="color: #111111;">self</span>.n_bootstraps):</span>
<span id="cb2-25">            train_ix <span class="op" style="color: #5E5E5E;">=</span> resample(test_ix)</span>
<span id="cb2-26">            <span class="cf" style="color: #003B4F;">yield</span> (train_ix, test_ix)</span>
<span id="cb2-27"></span>
<span id="cb2-28"><span class="co" style="color: #5E5E5E;"># Optimism Corrected</span></span>
<span id="cb2-29">model <span class="op" style="color: #5E5E5E;">=</span> LinearRegression()</span>
<span id="cb2-30">model.fit(X, y)</span>
<span id="cb2-31">apparent_performance <span class="op" style="color: #5E5E5E;">=</span> mean_squared_error(y, model.predict(X))</span>
<span id="cb2-32"></span>
<span id="cb2-33">opt_cv <span class="op" style="color: #5E5E5E;">=</span> OptimisimBootstrap(n_bootstraps<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">250</span>)</span>
<span id="cb2-34">mse <span class="op" style="color: #5E5E5E;">=</span> make_scorer(mean_squared_error)</span>
<span id="cb2-35">cv <span class="op" style="color: #5E5E5E;">=</span> cross_validate(model, X, y, cv<span class="op" style="color: #5E5E5E;">=</span>opt_cv, scoring<span class="op" style="color: #5E5E5E;">=</span>mse, return_train_score<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb2-36">optimism <span class="op" style="color: #5E5E5E;">=</span> cv[<span class="st" style="color: #20794D;">'test_score'</span>] <span class="op" style="color: #5E5E5E;">-</span> cv[<span class="st" style="color: #20794D;">'train_score'</span>]</span>
<span id="cb2-37">optimism_corrected <span class="op" style="color: #5E5E5E;">=</span> apparent_performance <span class="op" style="color: #5E5E5E;">+</span> optimism.mean()</span>
<span id="cb2-38"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'Optimism Corrected: </span><span class="sc" style="color: #5E5E5E;">{</span>optimism_corrected<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb2-39"></span>
<span id="cb2-40"><span class="co" style="color: #5E5E5E;"># Compare against regular cv</span></span>
<span id="cb2-41">cv <span class="op" style="color: #5E5E5E;">=</span> cross_validate(model, X, y, cv <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">10</span>, scoring<span class="op" style="color: #5E5E5E;">=</span>mse)[<span class="st" style="color: #20794D;">'test_score'</span>].mean()</span>
<span id="cb2-42"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'regular cv: </span><span class="sc" style="color: #5E5E5E;">{</span>cv<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb2-43"></span>
<span id="cb2-44"><span class="co" style="color: #5E5E5E;"># Compare against repeated cv</span></span>
<span id="cb2-45">cv <span class="op" style="color: #5E5E5E;">=</span> cross_validate(model, X, y, cv <span class="op" style="color: #5E5E5E;">=</span> RepeatedKFold(n_splits<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10</span>, n_repeats<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100</span>), scoring<span class="op" style="color: #5E5E5E;">=</span>mse)[<span class="st" style="color: #20794D;">'test_score'</span>].mean()</span>
<span id="cb2-46"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f'repeated cv: </span><span class="sc" style="color: #5E5E5E;">{</span>cv<span class="sc" style="color: #5E5E5E;">:.2f}</span><span class="ss" style="color: #20794D;">'</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Optimism Corrected: 2999.04
regular cv: 3000.38</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>repeated cv: 3009.02</code></pre>
</div>
</div>
<p>The three estimates (optimism corrected, 10 fold, and repeated 10 fold) should be reasonably close together, but uh don’t run this code multiple times. You might see that the optimism corrected estimate is quite noisy meaning I’m either wrong or that twitter thread I linked to might have some merit.</p>


</section>

 ]]></description>
  <category>Statistics</category>
  <category>Machine Learning</category>
  <category>Python</category>
  <category>Scikit-Learn</category>
  <guid>https://dpananos.github.io/posts/2021-11-23-bootstrap/index.html</guid>
  <pubDate>Tue, 23 Nov 2021 05:00:00 GMT</pubDate>
</item>
<item>
  <title>On Interpretations of Confidence Intervals</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2021-04-03-confidence-intervals/index.html</link>
  <description><![CDATA[ 




<p>The 95% in 95% confidence interval refers not to the probability that any one interval contains the estimand, but rather to the long term relative frequency of the estimator containing the estimand in an infinite sequence of replicated experiments under ideal conditions.</p>
<p>Now, if this were twitter I would get ratioed so hard I might have to take a break and walk it off. Luckily, this is my blog and not yours so I can say whatever I want with impunity. But, rather than shout my opinions and demand people listen, I thought it might be a good exercise to explain to you why I think this and perhaps why people might disagree. Let’s for a moment ignore the fact that the interpretation I use above is the <em>de jure</em> definition of a confidence interval and instead start where a good proportion of statistical learning starts; with a deck of shuffled cards.</p>
<p>I present to you a shuffled deck. Its a regular deck of cards, no funny business with the cards or the shuffling. What is the probability the top card of <em>this</em> deck an ace? I’d wager a good portion of people would say 4/52. If you, dear reader, said 4/52 then I believe you have made a benign mistake, but a mistake all the same. And I suspect the reason you’ve made this mistake is because you’ve swapped a hard question (the question about <em>this</em> deck) for an easier question (a question about the long term relative frequencies of coming to shuffled decks with no funny business and finding aces).</p>
<p>Swapping hard questions for easy questions is not a new observation. Daniel Khaneman writes about it in <em>Thinking Fast and Slow</em> and provides numerous examples. I’ll repeat some examples from the book here. We might swap the question:</p>
<ul>
<li>“How much would you contribute to save dolphins?” for “how much emotion do I feel when I think of dying dolphins?”</li>
<li>“How happy are you with your life?” for “What is my mood right now?”, and poignantly</li>
<li>“This woman is running for the primary. How far will she go in politics?” for “Does this woman look like a political winner”.</li>
</ul>
<p>The book <em>Thinking Fast and Slow</em> explains why we do this, or better yet why we have no control over this. I won’t explain it here. But it is important to know that this is something we do, mostly unconsciously.</p>
<p>So back to the deck of cards. Questions about the deck in front of you are hard. Its either an ace or not, but you can’t tell! The card is face down and there is no other information you could use to make the decision. So, you answer an easier one using information that you do know, namely the number of aces in the deck, the number of cards in the deck, the information that each card is equally likely to be on top given the fact there is no funny business with the cards or the shuffling, and the basic rules of probability you might have learned in high school if not elsewhere. But the answer you give is for a fundamentally different question, namely “If I were to observe a long sequence of well shuffled decks with no funny business, what fraction of them have an ace on top?”. Your answer is about that long sequence of shuffled decks. It isn’t about any one particular deck, and certainly not the one in front of you.</p>
<p>I think the same thing happens with confidence intervals. The estimator has the property that 95% of the time it is constructed (under ideal circumstances) it will contain the estimand. But any one interval does or does not contain the estimand. And unlike the deck of cards which can easily be examined, we can’t ever know for certain if the interval successfully captured the estimand. There is no moment where we get to verify the estimand is in the confidence interval, and so we are sort of left guessing thus prompting us to offer a probability that we are right.</p>
<p>The mistake is benign. It hurts no one to think about confidence intervals as having a 95% probability of containing the estimand. Your company will not lose money, your paper will (hopefully) not be rejected, and the world will not end. That being said, it is unfortunately incorrect if not by appealing to the definition, then perhaps for other reasons.</p>
<p>I’ll start with an appeal to authority. Sander Greenland and coauthors (who include notable epidemiologist Ken Rothman and motherfucking Doug Altman) include interpretation of a confidence interval as having 95% probability of containing the true effect as misconception 19 in <a href="https://link.springer.com/content/pdf/10.1007/s10654-016-0149-3.pdf">this amazing paper</a>. They note ” It is possible to compute an interval that can be interpreted as having 95% probability of containing the true value” but go on to say that this results in us doing a Bayesian analysis and computing a credible interval. If these guys are wrong, I don’t want to be right.</p>
<p>Additionally, when I say “The probability of a coin being flipped heads is 0.5” that references a long term frequency. I could, in principle, demonstrate that frequency by flipping a coin a lot and computing the empirical frequency of heads, which assuming the coin is fair and the number of flips large enough, will be within an acceptable range 0.5. To those people who say “This interval contains the estimand with 95% probability” I say “prove it”. Demonstrate to me via simulation or otherwise this long term relative frequency. I can’t imagine how this could be demonstrated because any fixed dataset will yield same answer over and over. Perhaps what supporters of this perspective mean is something closer to the Bayesian interpretation of probability (where probability is akin to strength in a belief). If so, the debate is decidedly answered because probability in frequentism is not about belief strength but about frequencies. Additionally, what is the random component in this probability? The data from the experiment are fixed, to allow these to vary is to appeal to my interpretation of the interval. If the estimand is random, then we are in another realm all together as frequentism assumes fixed parameters and random data. Maybe they mean something else which I just can’t think of. If there is something else, please let me know.</p>
<p>I’ve gotten flack about confidence intervals on twitter.</p>
<section id="flack-1-framing-it-as-a-bet" class="level2">
<h2 class="anchored" data-anchor-id="flack-1-framing-it-as-a-bet">Flack 1: Framing It As A Bet</h2>
<p>You present to me a shuffled deck with no funny business and offer me a bet in which I win X0,000 dollars if the card is an ace and lose X0 dollars if the card is not. “Aha Demetri! If you think the probability of the card on top being an ace is 0 or 1 you are either a fool for not taking the bet or are a fool for being so over confident! Your position is indefensible!” one person on twitter said to me (ok, they didn’t say it verbatim like this, but that was the intent).</p>
<p>Well, not so fast. Nothing about my interpretation precludes me from using the answer to a simpler question to make decisions (I would argue statistics is the practice of doing jus that, but I digress). The top card is still an ace or not, but I can still think about an infinite sequence of shuffled decks anyway. In most of those scenarios, the card on top is an ace. Thus, I take the bet and hope the top card is an ace (much like I hope the confidence interval captures the true estimand, even though I know it either does or does not).</p>
</section>
<section id="flack-2-my-next-interval-has-95-probability" class="level2">
<h2 class="anchored" data-anchor-id="flack-2-my-next-interval-has-95-probability">Flack 2: My Next Interval Has 95% Probability</h2>
<p>“But Demetri, if 95% refers to the frequency of intervals containing the estimand, then surely my next interval has 95% probability of capturing the estimand prior to seeing data. Hence, individual intervals <em>do</em> have 95% probability of containing the estimand”.</p>
<p>I get this sometimes, but don’t fully understand how it is supposed to be convincing. I see no problem with saying “the next interval has 95% probability” just like I have no problem with saying “If you shuffle those cards, the probability an ace is on top is 4/52” or “My next <a href="https://en.wikipedia.org/wiki/Tim_Hortons#Roll_Up_the_Rim_to_Win_campaign">Roll Up The Rim</a> cup has a 1 in 6 chance of winning”. This is starting to get more philosophical than I care it to, but those all reference non-existent things. Once they are brought into existence, it would be silly to think that they retain these properties. My cup is either winner or loser, even if I don’t roll it.</p>
</section>
<section id="flack-3-but-schrödingers-cat" class="level2">
<h2 class="anchored" data-anchor-id="flack-3-but-schrödingers-cat">Flack 3: But Schrödinger’s Cat…</h2>
<p>No.&nbsp;Stop. This is not relevant in the least. I’m talking about cards and coins, not quarks or electrons. The Wikipedia article even says “Schrödinger did not wish to promote the idea of dead-and-live cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics”. Cards can’t be and not-be aces until flipped. Get out of here.</p>
</section>
<section id="wrapping-up-dont-me" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up-dont-me">Wrapping Up, Don’t @ Me</h2>
<p>To be completely fair, I think the question about the cards I’ve presented to you is unfair. The question asks for a probability, and while 0 and 1 are valid probabilities, the question is phrased in a way so that you are prompted for a number between 0 and 1. Likewise, the name “95% confidence interval” begs for the wrong interpretation. That is the problem we face when we use language, which is naturally imprecise and full of shortcuts and ambiguity, to talk about things as precise as mathematics. It is a seminal case study in what I like to call the precision-usefulness trade off; precise statements are not useful. It is by, interpreting them and communicating them in common language that they become useful and that usefulness comes at the cost of precision (note, this explanation of the trade off is <em>itself</em> susceptible to the trade off). The important part is that we use confidence intervals to convey uncertainty in the estimate for which they are derived from. It isn’t important what you or I think about it, as the confidence interval is merely a means to an end.</p>
<p>AS I noted, the mistake is benign, and these arguments are mostly a mental exercise than a fight against a method which may induce harm. Were it not for COVID19, I would encourage us all to go out for a beer and have these conversations rather than do it over twitter. Anyway, if you promise not to @ me anymore about this and I promise not to tweet about it anymore.</p>


</section>

 ]]></description>
  <category>Statistics</category>
  <guid>https://dpananos.github.io/posts/2021-04-03-confidence-intervals/index.html</guid>
  <pubDate>Sat, 03 Apr 2021 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Log Link vs. Log(y)</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2020-10-06-links/index.html</link>
  <description><![CDATA[ 




<p>You wanna see a little gotcha in statistics? Take the following data</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><span class="fu" style="color: #4758AB;">set.seed</span>(<span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb1-2">N <span class="ot" style="color: #003B4F;">=</span> <span class="dv" style="color: #AD0000;">1000</span></span>
<span id="cb1-3">y <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">rlnorm</span>(N, <span class="fl" style="color: #AD0000;">0.5</span>, <span class="fl" style="color: #AD0000;">0.5</span>)</span></code></pre></div>
</div>
<p>and explain why <code>glm(y ~ 1, family = gaussian(link=log)</code> and <code>lm(log(y)~1)</code> produce different estimates of the coefficients. In case you don’t have an R terminal, here are the outputs</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1">log_lm <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">lm</span>(<span class="fu" style="color: #4758AB;">log</span>(y) <span class="sc" style="color: #5E5E5E;">~</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-2"><span class="fu" style="color: #4758AB;">summary</span>(log_lm)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = log(y) ~ 1)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.61028 -0.34631 -0.02152  0.35173  1.64112 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.49209    0.01578   31.18   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.499 on 999 degrees of freedom</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1">glm_mod <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">glm</span>(y <span class="sc" style="color: #5E5E5E;">~</span> <span class="dv" style="color: #AD0000;">1</span> , <span class="at" style="color: #657422;">family =</span> <span class="fu" style="color: #4758AB;">gaussian</span>(<span class="at" style="color: #657422;">link=</span>log))</span>
<span id="cb4-2"><span class="fu" style="color: #4758AB;">summary</span>(glm_mod)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = y ~ 1, family = gaussian(link = log))

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5282  -0.6981  -0.2541   0.4702   6.5869  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.61791    0.01698    36.4   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 0.9918425)

    Null deviance: 990.85  on 999  degrees of freedom
Residual deviance: 990.85  on 999  degrees of freedom
AIC: 2832.7

Number of Fisher Scoring iterations: 5</code></pre>
</div>
</div>
<p>Answer is the same as the difference between <img src="https://latex.codecogs.com/png.latex?E(g(X))"> and <img src="https://latex.codecogs.com/png.latex?g(E(X))"> which are not always the same. Let me explain.</p>
<p>First, let’s start with the lognormal random variable. <img src="https://latex.codecogs.com/png.latex?y%20%5Csim%20%5Coperatorname%7BLognormal%7D(%5Cmu,%20%5Csigma)"> means <img src="https://latex.codecogs.com/png.latex?%5Clog(y)%20%5Csim%20%5Coperatorname%7BNormal%7D(%5Cmu,%20%5Csigma)">. So <img src="https://latex.codecogs.com/png.latex?%5Cmu,%20%5Csigma"> are the parameters of the underlying normal distribution. When we do <code>lm(log(y) ~ 1)</code>, we are modelling <img src="https://latex.codecogs.com/png.latex?E(%5Clog(y))%20=%20%5Cbeta_0">. So <img src="https://latex.codecogs.com/png.latex?%5Cbeta_0"> is an estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmu"> and <img src="https://latex.codecogs.com/png.latex?%5Cexp(%5Cmu)"> is an estimate of the median of the lognormal. That is an easy check</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><span class="fu" style="color: #4758AB;">median</span>(y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.600898</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><span class="co" style="color: #5E5E5E;">#Meh, close enough</span></span>
<span id="cb8-2"><span class="fu" style="color: #4758AB;">exp</span>(<span class="fu" style="color: #4758AB;">coef</span>(log_lm))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept) 
   1.635723 </code></pre>
</div>
</div>
<p>If I wanted an estimate of the mean of the lognormal, I would need to add <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2/2"> to my estimate of <img src="https://latex.codecogs.com/png.latex?%5Cmu">.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><span class="fu" style="color: #4758AB;">mean</span>(y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.855038</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><span class="co" style="color: #5E5E5E;">#Meh, close enough</span></span>
<span id="cb12-2">sigma <span class="ot" style="color: #003B4F;">=</span> <span class="fu" style="color: #4758AB;">var</span>(log_lm<span class="sc" style="color: #5E5E5E;">$</span>residuals)</span>
<span id="cb12-3"><span class="fu" style="color: #4758AB;">exp</span>(<span class="fu" style="color: #4758AB;">coef</span>(log_lm) <span class="sc" style="color: #5E5E5E;">+</span> sigma<span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept) 
   1.852594 </code></pre>
</div>
</div>
<p>Ok, onto the glm now. When we use the glm, we model <img src="https://latex.codecogs.com/png.latex?%5Clog(E(y))%20=%20%5Cbeta_0">, so we model the mean of the lognormal directly. Case in point</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><span class="fu" style="color: #4758AB;">mean</span>(y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.855038</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><span class="fu" style="color: #4758AB;">exp</span>(<span class="fu" style="color: #4758AB;">coef</span>(glm_mod))</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept) 
   1.855038 </code></pre>
</div>
</div>
<p>and if I wanted the median, I would need to consider the extra factor of <img src="https://latex.codecogs.com/png.latex?%5Cexp(%5Csigma%5E2/2)"></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><span class="fu" style="color: #4758AB;">median</span>(y)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.600898</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><span class="fu" style="color: #4758AB;">exp</span>(<span class="fu" style="color: #4758AB;">coef</span>(glm_mod) <span class="sc" style="color: #5E5E5E;">-</span> sigma<span class="sc" style="color: #5E5E5E;">/</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept) 
   1.637881 </code></pre>
</div>
</div>
<p>Log link vs.&nbsp;log outcome can be tricky. Just be sure to know what you’re modelling when you use either.</p>



 ]]></description>
  <category>R</category>
  <category>Statistics</category>
  <guid>https://dpananos.github.io/posts/2020-10-06-links/index.html</guid>
  <pubDate>Tue, 06 Oct 2020 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Gradient Descent with ODEs</title>
  <dc:creator>Demetri Pananos</dc:creator>
  <link>https://dpananos.github.io/posts/2019-05-21-odes/index.html</link>
  <description><![CDATA[ 




<p>Gradient descent usually isn’t used to fit Ordinary Differential Equations (ODEs) to data (at least, that isn’t how the Applied Mathematics departments to which I have been a part have done it). Nevertheless, that doesn’t mean that it can’t be done. For some of my recent GSoC work, I’ve been investigating how to compute gradients of solutions to ODEs without access to the solution’s analytical form. In this blog post, I describe how these gradients can be computed and how they can be used to fit ODEs to synchronous data with gradient descent.</p>
<section id="up-to-speed-with-odes" class="level2">
<h2 class="anchored" data-anchor-id="up-to-speed-with-odes">Up To Speed With ODEs</h2>
<p>I realize not everyone might have studied ODEs. Here is everything you need to know:</p>
<p>A differential equation relates an unknown function <img src="https://latex.codecogs.com/png.latex?y%20%5Cin%20%5Cmathbb%7BR%7D%5En"> to it’s own derivative through a function <img src="https://latex.codecogs.com/png.latex?f:%20%5Cmathbb%7BR%7D%5En%20%5Ctimes%20%5Cmathbb%7BR%7D%20%5Ctimes%20%5Cmathbb%7BR%7D%5Em%20%5Crightarrow%20%5Cmathbb%7BR%7D%5En">, which also depends on time <img src="https://latex.codecogs.com/png.latex?t%20%5Cin%20%5Cmathbb%7BR%7D"> and possibly a set of parameters <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20%5Cin%20%5Cmathbb%7BR%7D%5Em">. We usually write ODEs as</p>
<p><img src="https://latex.codecogs.com/png.latex?y'%20=%20f(y,t,%5Ctheta)%20%5Cquad%20y(t_0)%20=%20y_0"></p>
<p>Here, we refer to the vector <img src="https://latex.codecogs.com/png.latex?y"> as “the system”, since the ODE above really defines a system of equations. The problem is usually equipped with an initial state of the system <img src="https://latex.codecogs.com/png.latex?y(t_0)%20=%20y_0"> from which the system evolves forward in <img src="https://latex.codecogs.com/png.latex?t">. Solutions to ODEs in analytic form are often <em>very hard</em> if not impossible, so most of the time we just numerically approximate the solution. It doesn’t matter how this is done because numerical integration is not the point of this post. If you’re interested, look up the class of <em>Runge-Kutta</em> methods.</p>
</section>
<section id="computing-gradients-for-odes" class="level2">
<h2 class="anchored" data-anchor-id="computing-gradients-for-odes">Computing Gradients for ODEs</h2>
<p>In this section, I’m going to be using derivative notation rather than <img src="https://latex.codecogs.com/png.latex?%5Cnabla"> for gradients. I think it is less ambiguous.</p>
<p>If we want to fit an ODE model to data by minimizing some loss function <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D">, then gradient descent looks like</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Ctheta_%7Bn+1%7D%20=%20%5Ctheta_n%20-%20%5Calpha%20%5Cdfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20%5Ctheta%7D%20"></p>
<p>In order to compute the gradient of the loss, we need the gradient of the solution, <img src="https://latex.codecogs.com/png.latex?y">, with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta">. The gradient of the solution is the hard part here because it can not be computed (a) analytically (because analytic solutions are hard AF), or (b) through automatic differentiation without differentiating through the numerical integration of our ODE (which seems computationally wasteful).</p>
<p>Thankfully, years of research into ODEs yields a way to do this (that is not the adjoint method. Surprise! You thought I was going to say the adjoint method didn’t you?). Forward mode sensitivity analysis calculates gradients by extending the ODE system to include the following equations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7Bd%7D%7Bdt%7D%5Cleft(%20%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20%5Ctheta%7D%20%5Cright)%20=%20%5Cmathcal%7BJ%7D_f%20%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20%5Ctheta%7D%20+%5Cdfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20"></p>
<p>Here, <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BJ%7D"> is the Jacobian of <img src="https://latex.codecogs.com/png.latex?f"> with respect to <img src="https://latex.codecogs.com/png.latex?y">. The forward sensitivity analysis is <em>just another differential equation</em> (see how it relates the derivative of the unknown <img src="https://latex.codecogs.com/png.latex?%5Cpartial%20y%20/%20%5Cpartial%20%5Ctheta"> to itself?)! In order to compute the gradient of <img src="https://latex.codecogs.com/png.latex?y"> with respect to <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> at time <img src="https://latex.codecogs.com/png.latex?t_i">, we compute</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20%5Ctheta%7D%20=%20%5Cint_%7Bt_0%7D%5E%7Bt_i%7D%20%5Cmathcal%7BJ%7D_f%20%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20%5Ctheta%7D%20+%20%5Cdfrac%7B%5Cpartial%20f%7D%7B%5Cpartial%20%5Ctheta%7D%20%5C,%20dt%20"></p>
<p>I know this looks scary, but since forward mode sensitivities are just ODEs, we actually just get this from what we can consider to be a black box</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20%5Ctheta%7D%20=%20%5Coperatorname%7BBlackBox%7D(f(y,t,%5Ctheta),%20t_0,%20y_0,%20%5Ctheta)"></p>
<p>So now that we have our gradient in hand, we can use the chain rule to write</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20%5Ctheta%7D%20=%5Cdfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20y%7D%20%5Cdfrac%7B%5Cpartial%20y%7D%7B%5Cpartial%20%5Ctheta%7D%20"></p>
<p>We can use automatic differentiation to compute <img src="https://latex.codecogs.com/png.latex?%5Cdfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20y%7D">.</p>
<p>OK, so that is some math (interesting to me, maybe not so much to you). Let’s actually implement this in python.</p>
</section>
<section id="gradient-descent-for-the-sir-model" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-for-the-sir-model">Gradient Descent for the SIR Model</h2>
<p>The SIR model is a set of differential equations which govern how a disease spreads through a homogeneously mixed closed populations. I could write an entire thesis on this model and its various extensions (in fact, I have), so I’ll let you read about those on your free time.</p>
<p>The system, shown below, is parameterized by a single parameter:</p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7BdS%7D%7Bdt%7D%20=%20-%5Ctheta%20SI%20%5Cquad%20S(0)%20=%200.99%20"></p>
<p><img src="https://latex.codecogs.com/png.latex?%20%5Cdfrac%7BdI%7D%7Bdt%7D%20=%20%5Ctheta%20SI%20-%20I%20%5Cquad%20I(0)%20=%200.01%20"></p>
<p>Let’s define the system, the appropriate derivatives, generate some observations and fit <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> using gradient descent. Here si what you’ll need to get started:</p>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> autograd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> autograd.builtins <span class="im" style="color: #00769E;">import</span> <span class="bu" style="color: null;">tuple</span></span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> autograd.numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-4"></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;">#Import ode solver and rename as BlackBox for consistency with blog</span></span>
<span id="cb1-6"><span class="im" style="color: #00769E;">from</span> scipy.integrate <span class="im" style="color: #00769E;">import</span> odeint <span class="im" style="color: #00769E;">as</span> BlackBox</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span></code></pre></div>
<p>Let’s then define the ODE system</p>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="kw" style="color: #003B4F;">def</span> f(y,t,theta):</span>
<span id="cb2-2">    <span class="co" style="color: #5E5E5E;">'''Function describing dynamics of the system'''</span></span>
<span id="cb2-3">    S,I <span class="op" style="color: #5E5E5E;">=</span> y</span>
<span id="cb2-4">    ds <span class="op" style="color: #5E5E5E;">=</span> <span class="op" style="color: #5E5E5E;">-</span>theta<span class="op" style="color: #5E5E5E;">*</span>S<span class="op" style="color: #5E5E5E;">*</span>I</span>
<span id="cb2-5">    di <span class="op" style="color: #5E5E5E;">=</span> theta<span class="op" style="color: #5E5E5E;">*</span>S<span class="op" style="color: #5E5E5E;">*</span>I <span class="op" style="color: #5E5E5E;">-</span> I</span>
<span id="cb2-6"></span>
<span id="cb2-7">    <span class="cf" style="color: #003B4F;">return</span> np.array([ds,di])</span></code></pre></div>
<p>and take appropriate derivatives</p>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="co" style="color: #5E5E5E;">#Jacobian wrt y</span></span>
<span id="cb3-2">J <span class="op" style="color: #5E5E5E;">=</span> autograd.jacobian(f,argnum<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">0</span>)</span>
<span id="cb3-3"><span class="co" style="color: #5E5E5E;">#Gradient wrt theta</span></span>
<span id="cb3-4">grad_f_theta <span class="op" style="color: #5E5E5E;">=</span> autograd.jacobian(f,argnum<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<p>Next, we’ll define the augmented system (that is, the ODE plus the sensitivities).</p>
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="kw" style="color: #003B4F;">def</span> ODESYS(Y,t,theta):</span>
<span id="cb4-2"></span>
<span id="cb4-3">    <span class="co" style="color: #5E5E5E;">#Y will be length 4.</span></span>
<span id="cb4-4">    <span class="co" style="color: #5E5E5E;">#Y[0], Y[1] are the ODEs</span></span>
<span id="cb4-5">    <span class="co" style="color: #5E5E5E;">#Y[2], Y[3] are the sensitivities</span></span>
<span id="cb4-6"></span>
<span id="cb4-7">    <span class="co" style="color: #5E5E5E;">#ODE</span></span>
<span id="cb4-8">    dy_dt <span class="op" style="color: #5E5E5E;">=</span> f(Y[<span class="dv" style="color: #AD0000;">0</span>:<span class="dv" style="color: #AD0000;">2</span>],t,theta)</span>
<span id="cb4-9">    <span class="co" style="color: #5E5E5E;">#Sensitivities</span></span>
<span id="cb4-10">    grad_y_theta <span class="op" style="color: #5E5E5E;">=</span> J(Y[:<span class="dv" style="color: #AD0000;">2</span>],t,theta)<span class="op" style="color: #5E5E5E;">@</span>Y[<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>::] <span class="op" style="color: #5E5E5E;">+</span> grad_f_theta(Y[:<span class="dv" style="color: #AD0000;">2</span>],t,theta)</span>
<span id="cb4-11"></span>
<span id="cb4-12">    <span class="cf" style="color: #003B4F;">return</span> np.concatenate([dy_dt,grad_y_theta])</span></code></pre></div>
<p>We’ll optimize the <img src="https://latex.codecogs.com/png.latex?L_2"> norm of the error</p>
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="kw" style="color: #003B4F;">def</span> Cost(y_obs):</span>
<span id="cb5-2">    <span class="kw" style="color: #003B4F;">def</span> cost(Y):</span>
<span id="cb5-3">        <span class="co" style="color: #5E5E5E;">'''Squared Error Loss'''</span></span>
<span id="cb5-4">        n <span class="op" style="color: #5E5E5E;">=</span> y_obs.shape[<span class="dv" style="color: #AD0000;">0</span>]</span>
<span id="cb5-5">        err <span class="op" style="color: #5E5E5E;">=</span> np.linalg.norm(y_obs <span class="op" style="color: #5E5E5E;">-</span> Y, <span class="dv" style="color: #AD0000;">2</span>, axis <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb5-6"></span>
<span id="cb5-7">        <span class="cf" style="color: #003B4F;">return</span> np.<span class="bu" style="color: null;">sum</span>(err)<span class="op" style="color: #5E5E5E;">/</span>n</span>
<span id="cb5-8"></span>
<span id="cb5-9">    <span class="cf" style="color: #003B4F;">return</span> cost</span></code></pre></div>
<p>Create some observations from which to fit</p>
<div class="cell" data-fig-height="5" data-fig-width="5" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">np.random.seed(<span class="dv" style="color: #AD0000;">19920908</span>)</span>
<span id="cb6-2"><span class="co" style="color: #5E5E5E;">## Generate Data</span></span>
<span id="cb6-3"><span class="co" style="color: #5E5E5E;">#Initial Condition</span></span>
<span id="cb6-4">Y0 <span class="op" style="color: #5E5E5E;">=</span> np.array([<span class="fl" style="color: #AD0000;">0.99</span>,<span class="fl" style="color: #AD0000;">0.01</span>, <span class="fl" style="color: #AD0000;">0.0</span>, <span class="fl" style="color: #AD0000;">0.0</span>])</span>
<span id="cb6-5"><span class="co" style="color: #5E5E5E;">#Space to compute solutions</span></span>
<span id="cb6-6">t <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">5</span>,<span class="dv" style="color: #AD0000;">101</span>)</span>
<span id="cb6-7"><span class="co" style="color: #5E5E5E;">#True param value</span></span>
<span id="cb6-8">theta <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">5.5</span></span>
<span id="cb6-9"></span>
<span id="cb6-10">sol <span class="op" style="color: #5E5E5E;">=</span> BlackBox(ODESYS, y0 <span class="op" style="color: #5E5E5E;">=</span> Y0, t <span class="op" style="color: #5E5E5E;">=</span> t, args <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">tuple</span>([theta]))</span>
<span id="cb6-11"></span>
<span id="cb6-12"><span class="co" style="color: #5E5E5E;">#Corupt the observations with noise</span></span>
<span id="cb6-13">y_obs <span class="op" style="color: #5E5E5E;">=</span> sol[:,:<span class="dv" style="color: #AD0000;">2</span>] <span class="op" style="color: #5E5E5E;">+</span> np.random.normal(<span class="dv" style="color: #AD0000;">0</span>,<span class="fl" style="color: #AD0000;">0.05</span>,size <span class="op" style="color: #5E5E5E;">=</span> sol[:,:<span class="dv" style="color: #AD0000;">2</span>].shape)</span>
<span id="cb6-14"></span>
<span id="cb6-15">plt.scatter(t,y_obs[:,<span class="dv" style="color: #AD0000;">0</span>], marker <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'.'</span>, alpha <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>, label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'S'</span>)</span>
<span id="cb6-16">plt.scatter(t,y_obs[:,<span class="dv" style="color: #AD0000;">1</span>], marker <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'.'</span>, alpha <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>, label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'I'</span>)</span>
<span id="cb6-17"></span>
<span id="cb6-18"></span>
<span id="cb6-19">plt.legend()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="6">
<pre><code>&lt;matplotlib.legend.Legend at 0x7f8e9d0b3d30&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://dpananos.github.io/posts/2019-05-21-odes/index_files/figure-html/cell-7-output-2.png" width="571" height="404"></p>
</div>
</div>
<p>Perform Gradient Descent</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">theta_iter <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb8-2">cost <span class="op" style="color: #5E5E5E;">=</span> Cost(y_obs[:,:<span class="dv" style="color: #AD0000;">2</span>])</span>
<span id="cb8-3">grad_C <span class="op" style="color: #5E5E5E;">=</span> autograd.grad(cost)</span>
<span id="cb8-4"></span>
<span id="cb8-5">maxiter <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">100</span></span>
<span id="cb8-6">learning_rate <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="co" style="color: #5E5E5E;">#Big steps</span></span>
<span id="cb8-7"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(maxiter):</span>
<span id="cb8-8"></span>
<span id="cb8-9">    sol <span class="op" style="color: #5E5E5E;">=</span> BlackBox(ODESYS,y0 <span class="op" style="color: #5E5E5E;">=</span> Y0, t <span class="op" style="color: #5E5E5E;">=</span> t, args <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">tuple</span>([theta_iter]))</span>
<span id="cb8-10"></span>
<span id="cb8-11">    Y <span class="op" style="color: #5E5E5E;">=</span> sol[:,:<span class="dv" style="color: #AD0000;">2</span>]</span>
<span id="cb8-12"></span>
<span id="cb8-13">    theta_iter <span class="op" style="color: #5E5E5E;">-=</span>learning_rate<span class="op" style="color: #5E5E5E;">*</span>(grad_C(Y)<span class="op" style="color: #5E5E5E;">*</span>sol[:,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">2</span>:]).<span class="bu" style="color: null;">sum</span>()</span>
<span id="cb8-14"></span>
<span id="cb8-15">    <span class="cf" style="color: #003B4F;">if</span> i<span class="op" style="color: #5E5E5E;">%</span><span class="dv" style="color: #AD0000;">10</span><span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>:</span>
<span id="cb8-16">        <span class="bu" style="color: null;">print</span>(<span class="st" style="color: #20794D;">"Theta estimate: "</span>, theta_iter)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  1.697027594337629</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  3.9189060278370365</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  4.810038385538704</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.251499985105974</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.427206219478129</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.46957706068474</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.47744643541383</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.4792194685272095</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.479636817124458</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta estimate:  5.47973599525063</code></pre>
</div>
</div>
<p>And lastly, compare our fitted curves to the true curves</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">sol <span class="op" style="color: #5E5E5E;">=</span> BlackBox(ODESYS, y0 <span class="op" style="color: #5E5E5E;">=</span> Y0, t <span class="op" style="color: #5E5E5E;">=</span> t, args <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">tuple</span>([theta_iter]))</span>
<span id="cb19-2">true_sol <span class="op" style="color: #5E5E5E;">=</span> BlackBox(ODESYS, y0 <span class="op" style="color: #5E5E5E;">=</span> Y0, t <span class="op" style="color: #5E5E5E;">=</span> t, args <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">tuple</span>([theta]))</span>
<span id="cb19-3"></span>
<span id="cb19-4"></span>
<span id="cb19-5">plt.plot(t,sol[:,<span class="dv" style="color: #AD0000;">0</span>], label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'S'</span>, color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'C0'</span>, linewidth <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb19-6">plt.plot(t,sol[:,<span class="dv" style="color: #AD0000;">1</span>], label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'I'</span>, color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'C1'</span>, linewidth <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb19-7"></span>
<span id="cb19-8">plt.scatter(t,y_obs[:,<span class="dv" style="color: #AD0000;">0</span>], marker <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'.'</span>, alpha <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb19-9">plt.scatter(t,y_obs[:,<span class="dv" style="color: #AD0000;">1</span>], marker <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'.'</span>, alpha <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.5</span>)</span>
<span id="cb19-10"></span>
<span id="cb19-11"></span>
<span id="cb19-12">plt.plot(t,true_sol[:,<span class="dv" style="color: #AD0000;">0</span>], label <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'Estimated '</span>, color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'k'</span>)</span>
<span id="cb19-13">plt.plot(t,true_sol[:,<span class="dv" style="color: #AD0000;">1</span>], color <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">'k'</span>)</span>
<span id="cb19-14"></span>
<span id="cb19-15">plt.legend()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>&lt;matplotlib.legend.Legend at 0x7f8e9de4e5e0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://dpananos.github.io/posts/2019-05-21-odes/index_files/figure-html/cell-9-output-2.png" width="571" height="404"></p>
</div>
</div>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>Fitting ODEs via gradient descent is possible, and not as complicated as I had initially thought. There are still some relaxations to be explored. Namely: what happens if we have observations at time <img src="https://latex.codecogs.com/png.latex?t_i"> for one part of the system but not the other? How does this scale as we add more parameters to the model? Can we speed up gradient descent some how (because it takes too long to converge as it is, hence the <code>maxiter</code> variable). In any case, this was an interesting, yet related, divergence from my GSoC work. I hope you learned something.</p>


</section>

 ]]></description>
  <category>Python</category>
  <category>Machine Learning</category>
  <category>Statistics</category>
  <guid>https://dpananos.github.io/posts/2019-05-21-odes/index.html</guid>
  <pubDate>Tue, 21 May 2019 04:00:00 GMT</pubDate>
  <media:content url="https://dpananos.github.io/posts/2019-05-21-odes/sir_curves.png" medium="image" type="image/png" height="96" width="144"/>
</item>
</channel>
</rss>
